{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceitos fundamentais\n",
    "\n",
    "---\n",
    "\n",
    "#### Tipos de Dados\n",
    "\n",
    "- numéricos\n",
    "\n",
    " - contagem de ocorrências\n",
    " \n",
    " - intensidade de uma propriedade (são sempre **analógicos** e apresentam uma precisão em casas decimais - e às vezes uma margem de erro)\n",
    " \n",
    "- categóricos (são sempre **discretos** - podem provir de um tipo analógico, discretizado em bandas)\n",
    "\n",
    " - quando são **nominais**, representam uma classe, uma categoria, uma subcategoria\n",
    " \n",
    " - quando são **ordinais**, podem ser ordenados (como notas em uma turma de alunos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Statistics\n",
    "\n",
    " - medidas de **centralidade** → dão uma ideia do elemento típico\n",
    " \n",
    " - medidas de **dispersão** → dão uma ideia da variabilidade\n",
    " \n",
    " *a ideia básica é formar os conceitos de **variabilidade** e **especificidade**, que são centrais na Estatística!*\n",
    " \n",
    " #### Centralidade (Center)\n",
    " \n",
    "  - média → é a mais precisa (mas corta dados inteiros, como animais, o que em alguns casos não é legal)\n",
    "  \n",
    "  - mediana → divide o grupo em dois e pega o dado central (funciona mais como uma roleta, com uma seta que aponta um elemento)\n",
    "  \n",
    "  *isso não fica tão bom quando meu conjunto tem um número par de elementos. Normalmente se pegam os dois centrais e se entrega a média deles*\n",
    "  \n",
    "  - moda → o valor mais frequente às vezes é muito útil, para sabermos por exemplo, qual a reação a tomar na maior parte dos casos\n",
    "  \n",
    "  *alguns conjuntos de dados apresentam mais de uma moda, é bom plotar e visualizar como nossos dados se comportam*\n",
    " \n",
    "Mais para frente:\n",
    " \n",
    " - **dispersão (spread)**\n",
    " \n",
    " - **formato (shape)**\n",
    " \n",
    " - **aberrantes, atípicos (outliers)**\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notação\n",
    "\n",
    "Gradient boosting [aqui](https://en.wikipedia.org/wiki/Gradient_boosting)\n",
    "\n",
    "- X para representar uma coluna\n",
    "\n",
    "- x1..n para representar os valores de X, para um determinado registro, ou linha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dispersão (Spread)\n",
    " \n",
    "   - Range (total)\n",
    "   \n",
    "   - Interquartile Range (IQR - entre quartis)\n",
    "   \n",
    "   - Standard Deviation\n",
    "\n",
    "   - Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representações visuais:\n",
    "\n",
    " - histogramas para uma visualização mais detalhada\n",
    " \n",
    " - box-plot para comparar diversas séries de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-Number summary:\n",
    "\n",
    " - ponto central - mediana (segundo quartil - marca dos 50%)\n",
    " \n",
    " - máximo e mínimo (amplitude)\n",
    " \n",
    " - pontos (primeiro - marca dos 25% e terceiro quartil - marca dos 75%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um número apenas - Desvio Padrão:\n",
    "\n",
    " - na média, quanto as os valores variam em relação à média\n",
    " \n",
    " - quanto maior o desvio padrão, maior o risco da operação!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,5,10,3,8,12,4]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = sum(a) / len(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for num in a:\n",
    "    c.append((num-b)**2)\n",
    "d = sum(c) / len(c)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "e = math.sqrt(d)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [12, -2, 10, 0, 7, 3]\n",
    "b = sum(a) / len(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nova lista de exercícios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [15, 4, 3, 8, 15, 22, 7, 9, 2, 3, 3, 12, 6]\n",
    "print ('média: ', sum(a) / len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "b = numpy.sort(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (-1*(b[0] - b[len(b)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (13.5-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape\n",
    "\n",
    " - Inclinação\n",
    " \n",
    "  - Symmetric - Bell Curve ou Distribuição Normal)\n",
    "   \n",
    "    - Média-Mediana-Moda iguais\n",
    "    \n",
    "    - Boxplot também fica simétrico\n",
    "    \n",
    "    - são casos comuns:\n",
    "    \n",
    "      - distribuição de altura e peso\n",
    "      \n",
    "      - testes padronizados (como QI)\n",
    "      \n",
    "      - precipitação (isso não é mais verdade com a mudança climática!)\n",
    "      \n",
    "      - a média de uma distribuição\n",
    "      \n",
    "      - erros em manufatura (quando não há aprendizagem)\n",
    " \n",
    "  - Skew para esquerda\n",
    "  \n",
    "    - Moda → Mediana → Média\n",
    "    \n",
    "    - Boxplot fica distorcido para a esquerda\n",
    "    \n",
    "    - são casos comuns:\n",
    "    \n",
    "      - quantidade de droga no seu sangue (o corpo tenta expulsar a supersaturação)\n",
    "      \n",
    "      - distribuição de riquezas (tendem a se concentrar em cada vez menos mãos)\n",
    "      \n",
    "      - habilidades humanas para o atletismo (a vida no campo criava pessoas mais robustas)\n",
    "\n",
    "  - Skew para a direita\n",
    "  \n",
    "    - são casos comuns:\n",
    "    \n",
    "      - GPA (que vai se tornando mais difícil há cada ano)\n",
    "      \n",
    "      - expectativa de vida\n",
    "      \n",
    "      - preços de tarifas públicas\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers (detecção de anomalias)\n",
    "\n",
    " - em alguns casos, eles são **erros** → **remover**\n",
    " \n",
    " - em outros eles são **pontos extraordinários** → **isolar** e **tratar separadamente**\n",
    " \n",
    "Um roteiro:\n",
    "\n",
    "1. plote seus dados\n",
    "\n",
    "2. se houver outliers, determine o que irá fazer com eles\n",
    "\n",
    "3. se a distribuição for simétrica, use e abuse da distribuição normal (use média e desvio padrão) [normal quantile plots](https://data.library.virginia.edu/understanding-q-q-plots/)\n",
    " \n",
    "*se não for, prefira trabalhar com os boxplots de 5 pontos*\n",
    "\n",
    "Um teste de [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) pode ser útil para detectar distorções, se elas existirem\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive statistics\n",
    "\n",
    "Descrever todo o conjunto de dados coletados\n",
    "\n",
    "\n",
    "#### Inferential statistics\n",
    "\n",
    "Tirar conclusões a partir de uma população analisada, a partir de uma amostra\n",
    "\n",
    " - fazer um curso na Udacity faz você tomar mais café?\n",
    " \n",
    "  - Population - our entire group of interest\n",
    "  \n",
    "  - Parameter - numeric summary about a population\n",
    "\n",
    "  - Sample - subset of the population\n",
    "\n",
    "  - Statistic - numeric summary about a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admissions case study\n",
    "\n",
    "---\n",
    "\n",
    "**Paradoxo de Simpson** (o quão estatísticas podem ser perigosas)\n",
    "\n",
    "Universidade de Berkeley\n",
    "\n",
    "- Exame 1\n",
    "\n",
    " - 450 dos 900 rapazes que prestaram o concurso foram aprovados (50%)\n",
    " \n",
    " - 80 das 100 moças foram aprovadas (80%)\n",
    "\n",
    "- Exame 2\n",
    "\n",
    " - 10 dos 100 rapazes que prestaram o concurso foram aprovados (10%)\n",
    " \n",
    " - 180 das 900 moças foram aprovadas (20%)\n",
    " \n",
    "Primeiramente, parece que as **moças** são em média, **favorecidas** no ingresso\n",
    "\n",
    "Mas se somarmos as populações:\n",
    "\n",
    " - nos rapazes, 460 dos 1.000 candidatos foram aprovados (46%)\n",
    "\n",
    " - nas moças no entanto, apenas 260 das 1.000 candidatas foram aprovadas (26%)\n",
    " \n",
    "E agora parece o contrário, que em média são os **rapazes** que são **favorecidos** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código todo das filtragens para replicar o Paradoxo de Simpson é um pouco maçante. Copio apenas alguns pontos essenciais, mostrando como fazer filtragens e contagens seletivas no Pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Admission rate for males\n",
    "x3 = dfadm.loc[dfadm['admitted'] == True].loc[dfadm['gender'] == 'male']\n",
    "rm = x2.count()[0]/totm\n",
    "rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfphy = dfadm.loc[dfadm['major'] == 'Physics']\n",
    "totphy = dfphy.count()[0]\n",
    "totphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Admission rate for female physics majors\n",
    "tdadmfemphy = dffemphy.loc[dffemphy['admitted'] == True].count()[0]\n",
    "tdadmfemphy/tdfemphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Separo doses inicial e final em colunas separadas (depois eliminar os prefixos u ao final de cada dosagem):\n",
    "\n",
    "*simplesmente uso o método **.split()** e depois elimino a coluna original da dosagem com **.drop()** e o problema está resolvido!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments_clean = pd.melt(treatments_clean, id_vars=['given_name', 'surname', 'hba1c_start', 'hba1c_end', 'hba1c_change'],\n",
    "                           var_name='treatment', value_name='dose')\n",
    "treatments_clean = treatments_clean[treatments_clean.dose != \"-\"]\n",
    "treatments_clean['dose_start'], treatments_clean['dose_end'] = treatments_clean['dose'].str.split(' - ', 1).str\n",
    "treatments_clean = treatments_clean.drop('dose', axis=1)\n",
    "\n",
    "Aqui eu junto as colunas de nome e sobrenome numa única coluna:\n",
    "\n",
    "*O método **.merge()** faz isso para mim*\n",
    "\n",
    "treatments_clean = pd.merge(treatments_clean, adverse_reactions_clean,\n",
    "                            on=['given_name', 'surname'], how='left')\n",
    "\n",
    "id_names = patients_clean[['patient_id', 'given_name', 'surname']]\n",
    "id_names.given_name = id_names.given_name.str.lower()\n",
    "id_names.surname = id_names.surname.str.lower()\n",
    "treatments_clean = pd.merge(treatments_clean, id_names, on=['given_name', 'surname'])\n",
    "treatments_clean = treatments_clean.drop(['given_name', 'surname'], axis=1)\n",
    "\n",
    "# Patient ID should be the only duplicate column\n",
    "all_columns = pd.Series(list(patients_clean) + list(treatments_clean))\n",
    "all_columns[all_columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability\n",
    "\n",
    "---\n",
    "\n",
    "Probabilidade serve para fazer **predições**, baseadas em modelos (causas possíveis) e dados (eventos passados)\n",
    "\n",
    " - predigo **dado** (um evento futuro)\n",
    "\n",
    "Estatística serve para analisar eventos passados para **inferir** quais modelos explicam melhor o que aconteceu\n",
    "\n",
    " - predigo **modelo** (a validade ou a conformação básica de um)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fair coin: P(Tail) = P(Head) = 0.5\n",
    "\n",
    "*diferente de uma Loaded coin!*\n",
    "\n",
    "P(1) + P(2) + P(n) = 1\n",
    "\n",
    "#### Two flips, fair coin\n",
    "\n",
    "P(H,H) = ? -> 0.25\n",
    "\n",
    "Truth table: P(H,H) + P(H,T) + P(T,H) + P(T,T) = 1 (todas as possibilidades possíveis)\n",
    "\n",
    "Outro caminho, multiplicando as probabilidades de cada evento independente: P(H,H) = 0.5 x 0.5\n",
    "\n",
    "#### Two flips, loaded coin\n",
    "\n",
    "P(H,H) = ? -> 0.36\n",
    "\n",
    "Truth table:\n",
    "\n",
    "    P(H,H) 0.36\n",
    "\n",
    "    P(H,T), P(T,H) 0.24\n",
    "    \n",
    "    P(T,T) = 0.16\n",
    "    \n",
    "#### Two flips, exactly one Heads\n",
    "\n",
    " P(exactly one H) = ? -> 0.5\n",
    " \n",
    "    P(H,H) 0.25\n",
    "\n",
    "    P(H,T), P(T,H) 0.25\n",
    "    \n",
    "    P(T,T) = 0.25\n",
    "\n",
    "#### Three flips, exactly one Heads\n",
    "\n",
    " P(exactly one H) = ? -> 3/8\n",
    " \n",
    "    P(H,H,H) 1/8 cada\n",
    "\n",
    "    P(T,H,H)\n",
    "    \n",
    "    P(H,T,H)\n",
    "    \n",
    "    P(T,T,H) *\n",
    "    \n",
    "    P(H,H,T)\n",
    "    \n",
    "    P(T,H,T) *\n",
    "    \n",
    "    P(H,T,T) *\n",
    "    \n",
    "    P(T,T,T)\n",
    "    \n",
    "#### Three flips, exactly one Heads, loaded coin\n",
    "\n",
    " P(H) = 0.6\n",
    "\n",
    " P(exactly one H) = ? -> 0.288\n",
    " \n",
    "    P(H,H,H) 0.216\n",
    "\n",
    "    P(T,H,H) P(H,T,H) P(H,H,T) 0.144\n",
    "    \n",
    "    P(T,T,H) P(T,H,T) P(H,T,T) 0.096\n",
    "    \n",
    "    P(T,T,T) 0.064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.4*0.4*0.6*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial Distribution\n",
    "\n",
    "---\n",
    "\n",
    "Número H=T\n",
    "\n",
    "    2 moedas -> 2 combinações\n",
    "\n",
    "    4 moedas -> 6 combinações\n",
    "    \n",
    "    5 moedas -> 0 combinações\n",
    "    \n",
    "Apenas 1H\n",
    "\n",
    "    5 moedas -> 5 combinações\n",
    "    \n",
    "2H    \n",
    "    \n",
    "    5 moedas -> 10 combinações\n",
    "    \n",
    "    Padrão:\n",
    "    \n",
    "    H . . . . (4x)\n",
    "    \n",
    "    T H . . . (3x)\n",
    "    \n",
    "    T T H . . (2x)\n",
    "    \n",
    "    T T T H H (1x)\n",
    "    \n",
    "    5 x 4 / 2 (para não contar as duplicidades, eu divido primeira moeda, 2 posições possíveis; segunda apenas 1) = 10\n",
    "    \n",
    "3H\n",
    "\n",
    "    5 moedas -> 10 combinações\n",
    "    \n",
    "    5 x 4 x 3 / 3 x 2 x 1 (para a primeira moeda, eu tenho 3 posições; segunda, 2 posições; terceira, apenas 1)\n",
    "    \n",
    "    (primeira moeda, 5 posições possíveis; segunda moeda, 4 posições; terceira moeda, 3 posições)\n",
    "    \n",
    "*eu divido o total de **combinações** possíveis, pelas **permutações** de moedas premiadas possíveis (pois para mim tanto faz se é a primeira ou a segunda moeda que me deu a primeira cara!)*\n",
    "\n",
    "5H\n",
    "\n",
    "    10 moedas -> 210 combinações\n",
    "    \n",
    "Fórmula:\n",
    "\n",
    "    n! / k! x (n-k)!\n",
    "    \n",
    "    sendo n o número total de lançamentos\n",
    "    \n",
    "    e k o número de moedas lançadas\n",
    "    \n",
    "3H\n",
    "\n",
    "    125 moedas -> 317.750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(10*9*8*7*6)/(5*4*3*2*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(125*124*123)/(3*2*1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binomial\n",
    "\n",
    "Moeda justa P(H) = 0.5 -> 5 / 32\n",
    "    \n",
    "Lanço 5 vezes\n",
    "    \n",
    "    Probabilidade de cair exatamente 1 Heads\n",
    "    \n",
    "    Tamanho da minha Truth Table: 2^5 = 32 (total de combinações possíveis)\n",
    "    \n",
    "    Combinações de H: 5! / 4! x 1! = 5\n",
    "    \n",
    "Lanço 5 vezes\n",
    "\n",
    "    Probabilidade de cair exatamente 3 Heads -> 10/32\n",
    "    \n",
    "    Tamanho da minha Truth Table: 2^5 = 32\n",
    "    \n",
    "    Combinações de 3H: 5! / 3! x 2! = 10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(5*4*3)/(3*2*1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora eu tenho uma moeda viciada: P(H) = 0.8\n",
    "    \n",
    "    \n",
    "Lanço 3 vezes\n",
    "    \n",
    "    P(apenas 1 H) -> 0.096\n",
    "    \n",
    "- primeira parte: qual a probabilidade indicada?\n",
    "\n",
    "    P(1H, 2T) = 0.8*0.2*0.2 = 0.032\n",
    "    \n",
    "- segunda parte: combinações possíveis?\n",
    "\n",
    "    3! / 2! 1! = 3\n",
    "    \n",
    "Lanço 5 vezes\n",
    "\n",
    "    P(exatas 4 H) ->\n",
    "    \n",
    "- primeira parte: qual a probabilidade indicada?\n",
    "\n",
    "    P(1H, 2T) = 0.8*0.8*0.8*0.8*0.2 = 0.4096\n",
    "    \n",
    "    p^k * (1-p)^(n-k)\n",
    "    \n",
    "- segunda parte: combinações possíveis?\n",
    "\n",
    "    3! / 2! 1! = 3\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(X=x)=  x!/ (n−x)!n! x p^x x (1−p)^(n−x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8*0.8*0.8*0.8*0.2*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(5*4*3*2*1) / (4*3*2*1 * 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "moeda viciada: P(H) = 0.8\n",
    "\n",
    "Lanço 5 vezes\n",
    "\n",
    "    P(exatas 3 H) -> 0.4096\n",
    "\n",
    "- primeira parte: qual a probabilidade indicada?\n",
    "\n",
    "    P(1H, 2T) = 0.8*0.8*0.8*0.2*0.2\n",
    "\n",
    "- segunda parte: combinações possíveis?\n",
    "\n",
    "    5! / 3! 2! = 10\n",
    "    \n",
    "Lanço 12 vezes\n",
    "\n",
    "    P(exatas 9 H) ->\n",
    "\n",
    "- primeira parte: qual a probabilidade indicada?\n",
    "\n",
    "    P(1H, 2T) = 0.8*0.8*0.8*0.8*0.8*0.8*0.8*0.8*0.8*0.2*0.2*0.2\n",
    "\n",
    "- segunda parte: combinações possíveis?\n",
    "\n",
    "    5! / 3! 2! = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8*0.8*0.8*0.2*0.2*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(5*4*3*2*1) / ((3*2*1) * (2*1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8*0.8*0.8*0.8*0.8*0.8*0.8*0.8*0.8*0.2*0.2*0.2*220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(12*11*10) / (3*2*1) #(12*11*10*9...) / (9*8*7...) * (3*2*1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional probability\n",
    "\n",
    "---\n",
    "\n",
    "A probabilidade da ave que eu avistar na minha corrida é\n",
    "\n",
    "- 50% coruja\n",
    "\n",
    "- 50% sabiá\n",
    "\n",
    "Agora, seu eu for:\n",
    "\n",
    "- um caminhante noturno\n",
    "\n",
    " - 100% coruja de chifres\n",
    " \n",
    " - 2% sabiá\n",
    "\n",
    "- um caminhante matutino\n",
    "\n",
    " - 0% coruja de chifres\n",
    " \n",
    " - 98% sabiá\n",
    " \n",
    "Então: a probabilidade de um evento é afetada pela condição de um outro evento!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso do paciente no hospital\n",
    "\n",
    "1. Sobre o **paciente**:\n",
    "\n",
    "    P(cancer) = 0.1\n",
    "\n",
    "    P(^cancer) = 0.9\n",
    "    \n",
    "\n",
    "2. Sobre o **teste**:\n",
    "\n",
    "    P(positive|cancer) = 0.9\n",
    "\n",
    "    P(negative|cancer) = 0.9\n",
    "    \n",
    "    P(positive|^cancer) = 0.2\n",
    "    \n",
    "    P(negative|^cancer) = 0.8\n",
    "    \n",
    "Com isso eu crio a **Truth table**, com todos os casos possíveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancer e Positivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.1*0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancer e Negativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.1*0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^Cancer e Positivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9*0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^Cancer e Negativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9*0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste positivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.18+0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total probabilities\n",
    "\n",
    "P(positivo) = P(positivo|cancer) x P(cancer) + P(positivo|^cancer) x P(^cancer)\n",
    "\n",
    "P(negativo) = 1 - P(positivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problema das moedas\n",
    "\n",
    "Saco com duas moedas:\n",
    "\n",
    "- a primeira é uma moeda justa P(H) = 0.5\n",
    "\n",
    "- a segunda é uma moeda viciada P(H) = 0.9\n",
    "\n",
    "Eu tenho **igual chance** de pegar qualquer uma das moedas\n",
    "\n",
    "- Eu pego uma delas e lanço. Qual a chance de saírem Heads? Resposta: 0.7\n",
    "\n",
    " - Caso1: 0.5 x 0.5\n",
    "\n",
    " - Caso2: 0.5 x 0.9\n",
    "\n",
    "Eu pego uma delas e lanço duas vezes. Qual a chance de saírem Heads e Tails? Resposta: 0.7\n",
    "\n",
    "- 50% de chegar ao Caso1\n",
    "\n",
    " - Caso1: 0.5 x 0.5 x 0.5\n",
    "\n",
    "- 50% de chegar ao caso2\n",
    "\n",
    " - Caso2: 0.5 x 0.9 x 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.5 * 0.5) + (0.5 * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.5 * 0.5) + (0.5 * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.5 * 0.5 * 0.5) + (0.5 * 0.9 * 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(test) = P(test|disease) x P(disease) + P(test|^disease) x P(^disease)\n",
    "\n",
    "P(A|B) = P(A intersecção B) / P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problema das moedas\n",
    "\n",
    "Saco com duas moedas:\n",
    "\n",
    "- a primeira é uma moeda viciada P(H) = 1\n",
    "\n",
    "- a segunda é uma moeda viciada P(H) = 0.6\n",
    "\n",
    "Eu tenho **igual chance** de pegar qualquer uma das moedas\n",
    "\n",
    "1. Eu pego uma delas e lanço duas vezes. Qual a chance de saírem Tails e Tails? Resposta:\n",
    "\n",
    "50% de chegar ao Caso1\n",
    "\n",
    "- Inviável\n",
    "\n",
    "50% de chegar ao caso2\n",
    "\n",
    "- Caso2: 0.5 x 0.4 x 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5 * 0.4 * 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes rule\n",
    "\n",
    "---\n",
    "\n",
    "Olhe em Machine Learning, o conteúdo está todo lá!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.01*0.9\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0.99*0.1\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a / c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b / c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test: neg\n",
    "\n",
    "P(cancer, neg):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.01 * 0.1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0.99 * 0.9\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a / c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b / c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nova doença\n",
    "\n",
    "P(c|neg):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.01/0.46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(^c|neg):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.45/0.46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(c|pos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.1*0.9\n",
    "a / c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(^c|pos:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0.9*0.5\n",
    "c = a + b\n",
    "b / c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em **Bayes Rule** eu normalmente tenho uma variável que eu não consigo testar diretamente (**prior**). E então eu desenvolvo testes indiretos, que me acendem uma luz, se a variável desconhecida for positiva e a luz não se acende, se a variável desconhecida for negativa (por exemplo, um ladrão tentando pular a minha cerca)\n",
    "\n",
    "Então eu multiplico essa variável por uma **medição**, que me dá um certo valor para a luz acesa e outro para a luz apagada e depois eu renormalizo isso para somar 1. Tudo isso é nossa **melhor estimativa**, no mundo dos mortais, por exemplo da presença de Deus (pelo menos aparentemente era assim que Bayes tentou inicialmente provar a existência de Deus!)\n",
    "\n",
    "De qualquer maneira, esse processo reproduz bastante fielmente o funcionamento tanto **físico** tanto dos sensores de um robô ou de um alarme, como que através de diversos exames laboratoriais, nós temos **evidência científica** de que algo está, ou não ocorrendo (por exemplo, que aquele meu novo creme dermatológico esteja eliminando fungos, ou que uma pessoa submetida a exame de dengue realmente esteja ou não com a doença!)\n",
    "\n",
    "Até hoje, tanto em robôs como em medicamentos, os dois parâmetros básicos aqui envolvidos, a **variabilidade** e **especificidade** são de suma importância!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Robot sensing\n",
    "\n",
    "Eu tenho um robô que vê apenas duas cores: R e G. A chance delas aparecerem é de 50% para cada uma, assim:\n",
    "\n",
    "    P(R) = P(G) = 0.5\n",
    "\n",
    "O sensor do meu robô não é muito preciso. A chance dele identificar corretamente é igual para cada uma delas, de 80%:\n",
    "\n",
    "    P(see R|at R) = 0.8\n",
    "    P(see G|at G) = 0.8\n",
    "    \n",
    "O sinal do meu robô foi: R\n",
    "\n",
    "Pode-se\n",
    "\n",
    "    P(at R|see R)\n",
    "    P(at G|see R)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint probabilities:\n",
    "\n",
    "    P(see R, at R) = P(R) x P(see R|at R) = 0.4\n",
    "    P(see G, at R) = P(R) x P(see G|at G) = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.5 * 0.8\n",
    "b = 0.5 * 0.2\n",
    "c = a + b # normallizer\n",
    "print(\"J1 :{} J2 :{}\".format(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior probabilities:\n",
    "    P(at R|see R) = 0.8\n",
    "    P(at G|see R) = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = a / c\n",
    "e = b / c\n",
    "print(\"Po1 :{} Po2 :{}\".format(d,e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora: P(G) = 1 (não existem pisos vermelhos!)\n",
    "\n",
    "    P(see R|at R) = 0.8\n",
    "\n",
    "    P(see G|at G) = 0.8\n",
    "\n",
    "Pode-se:\n",
    "\n",
    "    P(at R|see R) = 0\n",
    "\n",
    "    P(at G|see R) = 1 (é o defeito do sensor, normalizado!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    P(G) = 0.5\n",
    "\n",
    "    P(see R|at R) = 0.8\n",
    "\n",
    "Agora:\n",
    "\n",
    "    P(see G|at G) = P(see R|at G) = 0.5\n",
    "\n",
    "Pode-se:\n",
    "\n",
    "    P(at R|see R) =\n",
    "    \n",
    "    P(at R, see R) = 0.5 * 0.8\n",
    "\n",
    "    P(at G|see R) = \n",
    "    \n",
    "    P(at G, see R) = 0.5 * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.5 * 0.8\n",
    "b = 0.5 * 0.5\n",
    "c = a + b # essa conta não está correta, mas como as duas são 0.5, passa!\n",
    "d = a / c\n",
    "e = b / c\n",
    "print(\"Joint1 :{} Joint2 :{} SeeR :{} Posterior1 :{} Posterior2 :{}\".format(a,b,c,d,e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora meu robô tem um tablado com a pedra A vermelha, a pedra B verde e a pedra B verde:\n",
    "\n",
    "    P(A) = P(B) = P(C) = 1/3\n",
    "    \n",
    "E a leitura do sensor do robô:\n",
    "\n",
    "    P(lê R | A) = 0.9\n",
    "    \n",
    "    P(lê G | B) = 0.9\n",
    "    \n",
    "    P(lê G | C) = 0.9\n",
    "    \n",
    "Pede-se:\n",
    "\n",
    "- joints\n",
    "\n",
    "    P(A, lê R) = 1/3 * 0.9\n",
    "    \n",
    "    P(B, lê R) = 1/3 * 0.1\n",
    "    \n",
    "    P(C, lê R) = 1/3 * 0.1\n",
    "    \n",
    "- normallizer\n",
    "    \n",
    "    P(R) = 0.3667\n",
    "    \n",
    "- posterior\n",
    "\n",
    "    P(A | lê R)\n",
    "    \n",
    "    P(B | lê R) \n",
    "    \n",
    "    P(C | lê R) \n",
    "    \n",
    "*moral da história, eu posso ter mais de um fato **causador** independente e da mesma maneira, eu também posso ter mais de um **teste**. A Regra de Bayes é bastante generalizada!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (1/3) * 0.9\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (1/3) * 0.1\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + 2*b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a / c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b / c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sebastian em casa\n",
    "\n",
    "    P(gone) = 0.6\n",
    "    \n",
    "    P(home) = 0.4\n",
    "\n",
    "    P(rain | home) = 0.01\n",
    "    \n",
    "    P(rain | gone) = 0.3\n",
    "\n",
    "Abre a janela e está chovendo! Qual a chance de estar em casa?\n",
    "\n",
    "    P(home | rain) ?\n",
    "    \n",
    "    P(home, rain) = 0.04\n",
    "    \n",
    "    P(gone, rain) = 0.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.4 * 0.01\n",
    "b = 0.6 * 0.3\n",
    "c = a + b\n",
    "d = a / c\n",
    "e = b / c\n",
    "print(\"Po1 :{} Po2 :{}\".format(d,e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisão\n",
    "\n",
    "Eventos dependentes:\n",
    "\n",
    "    P(A) = 0.2\n",
    "    \n",
    "    P(B) = 0.1\n",
    "    \n",
    "    P(B|A) = 0.3 \n",
    "    \n",
    "Qual a probabilidade de P(A|B)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.2 * 0.3) / 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisão 2\n",
    "\n",
    "Coin 1 is fair. When flipped it has a probability of 0.5 for heads and 0.5 for tails\n",
    "\n",
    "Coin 2 is biased. When flipped it has a probability of 0.9 for heads and 0.1 for tails\n",
    "\n",
    "You grab one of these two coins at random (equally likely that you grabbed coin 1 or 2) and you flip it\n",
    "\n",
    "What's the probability it comes up heads?\n",
    "\n",
    "Joints:\n",
    "\n",
    "- P(c1) * P(H, c1) = 0.5 * 0.5\n",
    "\n",
    "- P(c2) * P(H, c1) = 0.5 * 0.9\n",
    "\n",
    "P(H) é a soma dos dois joints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.5 * 0.5\n",
    "b = 0.5 * 0.9\n",
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegar uma das moedas e lançá-la duas vezes\n",
    "\n",
    "    P(T, T) ?\n",
    "    \n",
    "- caminho 1 - peguei a c1:\n",
    "\n",
    "    P(c1) * P(T, c1) * P(T, c1) = 0.5 * 0.5 * 0.5\n",
    "    \n",
    "- caminho 2 - peguei a c2:\n",
    "\n",
    "    P(c2) * P(T, c2) * P(T, c2) = 0.5 * 0.1 * 0.1\n",
    "    \n",
    "P(T, T) é a soma dos dois caminhos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.5 * 0.5 * 0.5\n",
    "b = 0.5 * 0.1 * 0.1\n",
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Problema do carro ao ver o amarelho\n",
    "\n",
    "P(S) e P(Y) são **prior probabilities**\n",
    "\n",
    "Existe um carro automático. Em determinado cruzamento movimentado, a chance de que o carro pare é de 40%\n",
    "\n",
    "    P(S) = 0.4\n",
    "    P(^S) = 0.6\n",
    "\n",
    "A chance de que o farol esteja no amarelo é de apenas 10% (é uma luz de transição)\n",
    "\n",
    "    P(Y) = 0.1\n",
    "    P(^Y) = 0.9\n",
    "    \n",
    "Se o carro está parado, a chance de ser amarelo é de 12%\n",
    "\n",
    "    P(Y|S) = 0.12\n",
    "    \n",
    "Se o farol está em amarelo, qual a chance do carro estar parado?\n",
    "\n",
    "    P(S|Y) = P(S) x P(Y|S) / P(Y) = 0.48\n",
    "    \n",
    "*a resposta de 48% é meio intuitiva, pois o amarelo meio que atrai o motorista a parar o carro (que mais ou menos na metade das vezes ele consegue e na outra metade, um bando de corruptos arrecadam uma boa multa nesse cassa níqueis eletrônico)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.4*0.12)/0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Problema da freeway\n",
    "\n",
    "A faixa da esquerda é para quem quer correr\n",
    "\n",
    "- At any given time, 20% of cars are in the left-most lane\n",
    "\n",
    "- Overall, 40% of cars on the highway are classified as going fast\n",
    "\n",
    "- Out of all the cars in the leftmost lane, 90% are going fast\n",
    "\n",
    "Se um carro está veloz, qual a chance de que ele esteja na faixa da esquerda?\n",
    "\n",
    "    P(esquerda) = 0.2\n",
    "\n",
    "    P(veloz) = 0.4\n",
    "    \n",
    "    P(veloz | esquerda) = 0.9\n",
    "    \n",
    "Pede-se:\n",
    "\n",
    "    P(esquerda | veloz)\n",
    "    \n",
    "    P(esquerda) * P(veloz | esquerda) / P(veloz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.2*0.9) / 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python programming Bayes Rule\n",
    "\n",
    "*há vários notebooks Jupyter sobre isso!*\n",
    "\n",
    "---\n",
    "\n",
    "#### Flipping coins\n",
    "\n",
    "Random Sampling no NumPy\n",
    "\n",
    "- uma moeda ou milhares de moedas em uma lista -> método **.randint()**\n",
    "\n",
    "Observe que o método exclui o zero e o 1, então em um dado ficaria mais ou menos assim:\n",
    "\n",
    "    tests = np.random.choice(np.arange(1, 7), size=int(1e6))\n",
    "\n",
    "- moeda viciada -> **.random.choice()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome of one coin flip\n",
    "np.random.randint(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcomes of ten thousand coin flips\n",
    "np.random.randint(2, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean outcome of ten thousand coin flips\n",
    "np.random.randint(2, size=10000).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome of one coin flip\n",
    "np.random.choice([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome of ten thousand coin flips\n",
    "np.random.choice([0, 1], size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean outcome of ten thousand coin flips\n",
    "np.random.choice([0, 1], size=10000).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcomes of ten thousand biased coin flips\n",
    "np.random.choice([0, 1], size=10000, p=[0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean outcome of ten thousand biased coin flips\n",
    "np.random.choice([0, 1], size=10000, p=[0.8, 0.2]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "E se eu aumentar o número de amostras, eu tenho algum ganho na qualidade dos meus dados? Eles se tornam mais **precisos**?\n",
    "\n",
    "#### Teste de muitas moedas\n",
    "\n",
    "*obs: existe um notebook Jupyter à parte, mais completo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of heads from 10 fair coin flips\n",
    "np.random.binomial(10, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results from 20 tests with 10 coin flips\n",
    "np.random.binomial(10, 0.5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean number of heads from the 20 tests\n",
    "np.random.binomial(10, 0.5, 20).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reflects the fairness of the coin more closely as # tests increases\n",
    "np.random.binomial(10, 0.5, 1000000).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.random.binomial(10, 0.5, 1000000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets more narrow as number of flips increase per test\n",
    "plt.hist(np.random.binomial(100, 0.5, 1000000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Teste de distribuição binomial\n",
    "\n",
    "*obs: existe um notebook Jupyter à parte, mais completo!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 1 million tests of ten fair coin flips\n",
    "tests = np.random.binomial(10, 0.5, int(1e6))\n",
    "print(tests)\n",
    "\n",
    "# proportion of tests that produced 4 heads\n",
    "(tests == 4).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A operação desse negócio é muito simples!\n",
    "\n",
    "Trata-se de um produtor de distribuições binomiais (0 ou 1)\n",
    "\n",
    "Quando eu digo que eu quero lançar 10 moedas, ele gera números inteiros entre 0 e 10 e vai gravando numa lista. A ideia é a segunte:\n",
    "\n",
    "- 0 são Heads\n",
    "\n",
    "- 1 são Tails\n",
    "\n",
    "Se no **primeiro** parâmetro eu alimentei como 10 e ele gravou o resultado 0, isso quer dizer que todas as 10 moedas deram cara!\n",
    "\n",
    "O **segundo** parâmetro é a probabilidade de cada evento. Então 0.5 para uma moeda justa e 0.8 por exemplo, para uma moeda viciada. O 0.8 alimenta o primeiro parâmetro (Heads). Basta usar o bom senso e o resultado sai perfeito!\n",
    "\n",
    "E o **terceiro** parâmetro é o tamanho da minha amostra. No caso eu quero 1e6, ou seja 1.000.000 de lançamentos!\n",
    "\n",
    "Finalmente, eu tiro a média de todos os meus lançamentos. Para medidas de Cara exatamente igual a 4, **tests == 4** será minha comparação. Isso poderia ser de até 4, e ficaria **tests <= 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Cancer data\n",
    "\n",
    "*há um notebook Jupyter com tudo detalhado!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A busca por True num campo booleano é super simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.has_cancer.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.has_cancer == False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tirar uma média por True também é simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.has_cancer.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método Pandas **.query()** é uma mão na roda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.query('has_cancer')['test_result'] == 'Negative').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilidade condicional e a Regra de Bayes\n",
    "\n",
    "---\n",
    "\n",
    "*há um notebook Jupyter só para este cara aqui!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.query('has_cancer == False')['test_result'] == 'Negative').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*observe que a Regra de Bayes é tão usual, que o próprio Pandas calcula o **posterior** automaticamente para nós, a partir da média de comparação entre dois campos. O maior problema não é **fazer** as coisas, é **testar** para vermos se não comemos bola em algum ponto!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal distribution\n",
    "\n",
    "---\n",
    "\n",
    "#### Binomial distribution -> Central Limit Theorem -> Normal distribution\n",
    "\n",
    "A fórmula para a distribuição Binomial é toda quadradinha... o que o Teorema do Limite Central Faz é demonstrar que para uma quantidade realmente grande de indivíduos, isso se aproxima de uma curva suave, chamada curva de distribuição Normal\n",
    "\n",
    "O primeiro termo da distribuição Binomial (número de combinações) é maximizado para a metade do valor de k\n",
    "\n",
    "Eu tenho a distribuição com média e variância (Mu, Sigma^2)\n",
    "\n",
    "e eu tenho um bom comportamento da curva quadrática positiva:\n",
    "\n",
    "    f(x) = (X-Mu)^2\n",
    "    \n",
    "agora eu divido isso por Sigma^2, criando uma função mais alongada ou encurtada:\n",
    "\n",
    "    f(x) = (X-Mu)^2 / Sigma^2\n",
    "\n",
    "*pequenas variâncias tornam a curva mais encurtada e grandes variâncias tornam a curva mais alongada, o que faz sentido!*\n",
    "\n",
    "e agora eu multiplico por -1/2, o que a coloca de ponta-cabeça e a torna ainda mais achatada e negativa:\n",
    "    \n",
    "    f(x) = -1/2 * (X-Mu)^2 / Sigma^2\n",
    "    \n",
    "e por fim, eu elevo tudo a e, tornando-a uma curva de características exponenciais:\n",
    "\n",
    "    f(x) = e^(-1/2 * (X-Mu)^2 / Sigma^2)\n",
    "    \n",
    "*observe que por causa do expoente ser todo ele negativo, eu estou tomando apenas o lado **esquerdo** da exponencial, parando em X=0. Neste ponto, a função atinge seu ápice, já que nada é tirado de Mu*\n",
    "\n",
    "*e neste ponto exato, quando f(x) é igual a Mu, os valores do esponencial se cancelam, resultando e^0 = 1!*\n",
    "\n",
    "*mais uma coisa, para f(x) muito grandes, ou muito pequenos, o valor do expoente fica muito elevado e e^infinito -> zero. É o que ocorre no final das duas caudas da função!*\n",
    "\n",
    "O único problema restante é que a área sob a curva soma (2 x Pi x Sigma^2)^1/2. Eu posso normalizar isso dividindo por esta constante:\n",
    "\n",
    "    ND(Xi, Mui, Sigma^2) = 1/(2 x Pi x Sigma^2)^1/2 * e^(-1/2 * (X-Mu)^2 / Sigma^2)\n",
    "\n",
    "*Obs: esta fórmula é uma **aproximação** e para alguns casos, como média entre zero e 1, o resultado pode ser negativo!*\n",
    "\n",
    "*Uso: para grandes números (tipo 10.000)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teorema do Limite Central\n",
    "\n",
    "*há vários notebooks Jupyter sobre isso!*\n",
    "\n",
    "---\n",
    "\n",
    "Estaremos pensando agora mais sobre **estatísticas**:\n",
    "\n",
    "- **dados** -> **conclusões**\n",
    "\n",
    "- e não **probabilidades** -> **conclusões**\n",
    "\n",
    "Trabalhamos até agora com **descriptive statistics** -> descrever estatisticamente os dados coletados\n",
    "\n",
    "Agora, **inferential statistics** -> tirar conclusão sobre uma determinada população, a partir de uma amostra\n",
    "\n",
    "---\n",
    "\n",
    "#### Sampling distribution\n",
    "\n",
    "1 amostra:\n",
    "\n",
    "- População: 21 estudantes\n",
    "\n",
    "- Parâmetro: 71% dos estudantes bebem café\n",
    "\n",
    "- Amostra: 5 estudantes\n",
    "\n",
    "- Estatística: 20% dos estudantes amostrados referiram beber café\n",
    "\n",
    "Agora eu tiro x amostras de n (x é um número grande, como 10.000) e faço a estatística de cada uma dessas amostras\n",
    "\n",
    "Depois eu ploto isso em um gráfico de barras, que me mostra algo parecido com uma curva de distribuição normal. Essa curva é a distribuição amostral!\n",
    "\n",
    "Eu posso observar também além de tirar a variância da minha população:\n",
    "    \n",
    "    p*(1-p), sendo p a minha média da população\n",
    "    \n",
    "Eu posso tirar a variância das minhas 10.000 amostras:\n",
    "\n",
    "    p*(1-p)/5 sendo n=5 o número de elementos de cada amostra\n",
    "    \n",
    "E mudando n, eu posso tirar uma nova variância de outras 10.000 amostras aleatórias:\n",
    "\n",
    "    p*(1-p)/20 sendo agora n=20\n",
    "    \n",
    "Ao observar todas essas variâncias, eu percebo que as minhas novas amostras com 20 elementos possuem uma variância muito **menor**. E de fato, a minha curva de distribuição amostral agora têm realmente a forma da curva de **distribuição normal**!\n",
    "\n",
    "*parece uma doideira fazer tudo isso, jogar um esforço computacional imenso fora para calcular médias e variâncias de 10.000 grupos aleatórios de 20 indivíduos. Por que eu simplesmente não faço isso com toda minha população? Por uma razão simples, **não é assim que a natureza se comporta!**. Eu não consigo simplesmente amostrar todos os brasileiros em busca de resultados positivos para HIV. Eu consigo obter amostras dentro de **determinados grupos** e com isso eu vou criando aquilo que se chama de minha **inferência estatística**. O processo geral é bem mais complexo do que o apresentado aqui (eu tenho dados de bancos de sangue, que recusaram um número x de doadores cujo tesde de HIV deu positivo, eu tenho testes realizados em presídios, eu tenho testes realizados em campanhas de servidores públicos e outras coisas assim, todas elas têm seus vícios e um dos piores é o **tempo** - em alguns casos aqueles resultados foram colhidos há 5 anos!), mas é assim que a coisa funciona de um modo geral*\n",
    "\n",
    "*existe um notebook Jupyter para simular tudo isso!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Sampling Distributions Notes\n",
    "\n",
    "We have already learned some really valuable ideas about sampling distributions:\n",
    "\n",
    "First, we have defined sampling distributions as the distribution of a statistic.\n",
    "\n",
    "This is fundamental - I cannot stress the importance of this idea. We simulated the creation of sampling distributions in the previous ipython notebook for samples of size 5 and size 20, which is something you will do more than once in the upcoming concepts and lessons.\n",
    "\n",
    "Second, we found out some interesting ideas about sampling distributions that will be iterated later in this lesson as well. We found that for proportions (and also means, as proportions are just the mean of 1 and 0 values), the following characteristics hold.\n",
    "\n",
    "1. The sampling distribution is centered on the original parameter value.\n",
    "\n",
    "2. The sampling distribution decreases its variance depending on the sample size used. Specifically, the variance of the sampling distribution is equal to the variance of the original data divided by the sample size used. This is always true for the variance of a sample mean!\n",
    "\n",
    "In notation, we say if we have a random variable, X, with variance of Sigma^2, then the distribution of Xmed (the sampling distribution of the sample mean) has a variance of Sigma^2/n\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markdown parser included in the Jupyter Notebook is MathJax-aware. This means that you can freely mix in mathematical expressions using the MathJax subset of Tex and LaTeX. Some examples from the MathJax site are reproduced below, as well as the Markdown+TeX source.\n",
    "\n",
    "Motivating Examples\n",
    "The Lorenz Equations\n",
    "Source\n",
    "\\begin{align}\n",
    "\\dot{x} & = \\sigma(y-x) \\\\\n",
    "\\dot{y} & = \\rho x - y - xz \\\\\n",
    "\\dot{z} & = -\\beta z + xy\n",
    "\\end{align}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notação de parâmetros\n",
    "\n",
    "Parâmetro (da POPULAÇÃO):\n",
    "\n",
    "*isso aqui **nunca** muda!*\n",
    "\n",
    "- média: $\\mu$\n",
    "\n",
    "- desvio padrão $\\sigma -> desvio padrão da POPULAÇÂO\n",
    "\n",
    "- variância $\\sigma^2$ -> variância da POPULAÇÃO\n",
    "\n",
    "- proporção $\\pi$\n",
    "\n",
    "- coeficiente de regressão $\\beta$\n",
    "\n",
    "Estatística (da AMOSTRA):\n",
    "\n",
    "*isso aqui varia em cada **amostragem** e pelo **número de elementos** de cada amostra!*\n",
    "\n",
    "*e no final, o resultado é influenciado pela **quantidade de amostras** que tiramos!*\n",
    "\n",
    "- média: $\\hat{\\mu}$ $\\bar{x}$ -> média da AMOSTRA\n",
    "\n",
    "- desvio padrão $\\hat{\\sigma}$ $S$ -> desvio padrão da AMOSTRA\n",
    "\n",
    "- variância $\\hat{\\sigma}^2$ $S^2$ -> variância da AMOSTRA\n",
    "\n",
    "- proporção $p$ $\\hat{\\pi}$\n",
    "\n",
    "- coeficiente de regressão $b$ $\\hat\\beta$\n",
    "\n",
    "Dois teoremas importantes:\n",
    "\n",
    "- Teorema do Limite Central\n",
    "\n",
    "- Le dos Grandes Números\n",
    "\n",
    "*eles são **fundamentais** para isso dar certo!*\n",
    "\n",
    "editor [aqui](https://latex.codecogs.com/eqneditor/editor.php) \n",
    "\n",
    "R [aqui](https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Usos do Teorema do Limite Central:\n",
    "\n",
    "Se aplica em:\n",
    "\n",
    "- média $\\bar{x}$ proporção $p$\n",
    " \n",
    "- diferenças em médias $\\bar{x}1 - \\bar{x}2$ e proporções $p1 - p2$\n",
    "\n",
    "Não se aplica em:\n",
    "\n",
    "- variância $S^2$ ou coeficiente de correlação $r$\n",
    "\n",
    "- valor máximo de um dataset $x(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Bootstrapping\n",
    "\n",
    "Além de trabalhar com distribuições amostrais, eu também posso amostrar aleatoriamente com reposição dados de um depósito, para realizar uma técnica chamada **bootstrapping**\n",
    "\n",
    "#### Inferential statisticals\n",
    "\n",
    "- use statisticals to infer something about a population\n",
    "\n",
    "Usado em:\n",
    "\n",
    "- intervalos de confiança\n",
    "\n",
    "- teste de hipóteses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Truque para puxar de volta o último valor da memória:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bootstrapping** é usado em:\n",
    "\n",
    "- Leading Machine Algorithms\n",
    "\n",
    "- Random Forests\n",
    "\n",
    "- Stochastic Gradient Boosting\n",
    "\n",
    "Bradley Effron (1979) [aqui](https://en.wikipedia.org/wiki/Bradley_Efron)\n",
    "\n",
    "Mais sobre o método [aqui](https://stats.stackexchange.com/questions/26088/explaining-to-laypeople-why-bootstrapping-works)\n",
    "\n",
    "*no more data needed to gain understanding of the parameter*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parâmetro** -> um sumário numérico de uma **população**\n",
    "\n",
    "**estatística** -> um sumário numérico de uma **amostra**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervalos de confiança\n",
    "\n",
    "*há vários notebooks Jupyter sobre isso!\n",
    "\n",
    "---\n",
    "\n",
    "Antes estávamos pescando com anzol (minha amostra tinha um resultado preciso, mas único)\n",
    "\n",
    "Agora pescamos com rede (a malha da minha rede determina os tamanhos dos peixes que eu vou pescar)\n",
    "\n",
    "Como usar **distribuições amostrais** para inferir onde um **parâmetro** está localizado?\n",
    "\n",
    "*no mundo real, eu normalmente **não sei** um parâmetro da minha população!*\n",
    "\n",
    "---\n",
    "\n",
    "Eu posso **cortar** minha distribuição amostral em um ponto qualquer, segundo uma margem de confiança que eu desejo. Por exemplo, cortar minha curva normal da amostra para **95% da área** ser aproveitada! Com duas caudas, eu estarei desprezando apenas 2.5% em cada uma delas!\n",
    "\n",
    "*é tipo, eu jogo fora os **outliers** e tento ajustar o resto dos dados. Parece bastante razoável!*\n",
    "\n",
    "Em outras palavras, eu posso afirmar, com **95% de confiança**, de que os critérios da inferência estão corretos!\n",
    "\n",
    "Eu também posso dizer assim: eu tenho **95% de confiança** de que o uso algum meio de transporte automotivo é necessário para pessoas que trabalham entre 4 e 42km de suas residências!\n",
    "\n",
    "*neste último caso, como a distância é crítica, eu poderia tentar cortar em **apenas uma** das caudas, a da menor distância!*\n",
    "\n",
    "---\n",
    "\n",
    "Da mesma maneira que eu posso usar o bootstrap para inferir em um intervalo de confiança a média de altura dos bebedores de café, eu posso também usar isso para comparar dois parâmetros: o quanto as alturas diferem, entre bebedores e não bebedores de café?\n",
    "\n",
    "Agora em cada iteração de bootstrapping, eu pego a **média** para cada grupo e também a **diferença**\n",
    "\n",
    "*Pro tip: the difference in means in a single iteration of a bootstrap sample is stored in diff*\n",
    "\n",
    "O que ocorreu é que quando eu tirei o meu **intervalo de confiança** das diferenças das médias das alturas entre bebedores e não bebedores de café para cada um dos meus 10.000 botstrappings, a perna da esquerda da curva de distribuição normal ficou deslocada do zero. Isso indica que existe **sim** diferença de altura na média dos bebedores de café (eles seriam ligeiramente mais altos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Statistical vs Practical Significance\n",
    "\n",
    "- Statistical - evidence from hypothesis tests and confidence intervals that H1 is true\n",
    "\n",
    "- Practical - considers real world aspects, not just numbers, in making final conclusions\n",
    "\n",
    " - algumas coisas vão mal com a H1:\n",
    "\n",
    "   - custa muito caro para implementar\n",
    " \n",
    "   - chama mais donos de cachorro do que meu negócio consegue absorver\n",
    "\n",
    "Intervalos de confiança **tradicionais**:\n",
    "   \n",
    " - bootstrapping\n",
    " \n",
    "   - cria uma melhor representação do que o parâmetro deve ser, mas...\n",
    " \n",
    "     - não porta **intervalos de confiança**\n",
    " \n",
    "     - apenas **presume** que a amostra é realmente representativa da população!\n",
    "     \n",
    "   - com amostras grandes, este método irá prover bons resultados\n",
    " \n",
    " - sampling distributions\n",
    " \n",
    "#### Métodos tradicionais para obter intervalos de confiança vs bootstrapping\n",
    " \n",
    "Existem centenas, talvez milhares de métodos tradicionais de teste de hipóteses. Muitos deles, com os avanços da computação, se tornaram obsoletos. Alguns deles aparecem em Statistical Inference, Hypotheses Testing [aqui](https://stattrek.com/hypothesis-test/how-to-test-hypothesis.aspx):\n",
    " \n",
    " - Proportions\n",
    " \n",
    " - Diff between props\n",
    " \n",
    " - Mean\n",
    " \n",
    " - Diff between means\n",
    " \n",
    " - Diff between pairs\n",
    " \n",
    " - Goodness of fit test\n",
    " \n",
    " - Homogeneity\n",
    " \n",
    " - Independence\n",
    " \n",
    " - Regression slope\n",
    " \n",
    "*todos eles estão implementados no iPython. Se eu quiser testar qualquer um deles para ver como ele se comporta (normalmente um teste especializado bem aplicado é computacionalmente mais rápido que o bootstrap), é só googlar e ver como escrever o código!* \n",
    " \n",
    "*Each of these hypothesis tests is linked to a corresponding confidence interval, but again the bootstrapping approach can be used in place of any of these! Simply by understanding what you would like to estimate, and simulating the sampling distribution for the statistic that best estimates that value* \n",
    "\n",
    "*com bootstrapping, você pode simular os resultados de **qualquer** intervalo de confiança que você quiser construir!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Termos associados a Intervalos de Confiança\n",
    "\n",
    "- sample size\n",
    "\n",
    "- confidence level\n",
    "\n",
    "  - a pesquisa apresenta uma margem de confiança de 95%\n",
    "\n",
    "- margin of error (MOE)\n",
    "\n",
    "  - a peça possui de largura: 100 (+/- 3) milímetros de largura\n",
    "\n",
    "- confidence interval width - decreasing Chi width:\n",
    "\n",
    "  - increase sample size **n**\n",
    " \n",
    "  - decrease confidence level\n",
    "  \n",
    "**Increase** sample size -> confidence intervall will **narrow**\n",
    "\n",
    "**Increase** confidence level -> donfidence intervall will **widen**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estatística prática - coisas que não devo fazer:\n",
    "\n",
    "- fazer declaração sobre um dos **indivíduos** amostrados\n",
    "\n",
    " - eu não posso dizer por exemplo, que \"um indivíduo estará, em uma confiança de 95%, mais tentando a clicar no cachorrinho que no gatinho\". Comerciais antigamente faziam esta distorção. Isso é antiético e por isso hoje em dia a publicidade de um modo geral tem que passar por uma aprovação\n",
    "\n",
    " - agora se for **vital** para mim saber se um determinado indivíduo está propenso a **realmente** clicar no cachorrinho da nossa webpage? Bom para isso existem técnicas de **machine learning**, que envolvem análise bayesiana, criação de perceptrons de múltiplas camadas, etc..\n",
    " \n",
    "Um intervalo de confiança nos diz coisas como:\n",
    "\n",
    "- \"Estamos 95% confiantes de que em média a droga Novofox irá ter bons resultados em membros deste grupo populacional testado\"\n",
    "\n",
    "- \"Temos 95% de confiança que a propoção de usuários que irão clicar na página cachorro será maior do que a proporção de usuários que irão clicar na página do gato\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis test\n",
    "\n",
    "*há vários notebooks Jupyter sobre isso!\n",
    "\n",
    "---\n",
    "\n",
    "Pressuposto básico:\n",
    "\n",
    "- quando estou lidando com teste de hipóteses, não tenho como testar toda a população! Então eu tenho que trabalhar por todo o tempo não com o **parâmetro** e sim com dados amostrais!\n",
    "\n",
    "- no entanto, tudo o que diz respeito a validação de hipóteses está ligado à **estatística**. É onde a coisa vai realmente se aplicar: eu faço toda minha modelagem, mas em algum momento terei que ir ao mundo real e colocar tudo isso em prática!\n",
    "\n",
    "- outra coisa, falando em **parâmetros**, quais deles podem ser testados por hipóteses? Qualquer um! O que não posso mesmo é elaborar uma hipotese sobre um **dado amostral**!\n",
    "\n",
    "Eu me baseio sempre em duas hipóteses, excludentes:\n",
    "\n",
    "- H0 (minha **null hypothesis**) de que por exemplo, o remédio para dor de cabeça não produz **nenhum** efeito. Observe que a HO deve apresentar **sempre** o sinal de igual (=). É possível que isso também seja < ou = a alguma coisa, tudo bem\n",
    "\n",
    "- H1 (minha **alternative hypothesis**) que caso H0 seja falsificada, é ela que entrará em vigor. Por exemplo, que neste caso, meu remédio é **efetivo** no combate de cefaléia\n",
    "\n",
    "Condição inicial:\n",
    "\n",
    "H0 = True\n",
    "\n",
    "Durante os testes, tentarei falsificar esta condição\n",
    "\n",
    "Esta é a maneira que a ciência têm lidado (validação de hipóteses) em praticamente todos os campos. Isso se provou melhor do que a pergunta simples: \"Cafeína cura dor de cabeça?\". Para o médico 1, sim, e ele recomenda para todos os pacientes. Para o médico 2, em alguns casos sim e outros não. Já para o médico 3... e então, quem está com a razão?\n",
    "\n",
    "*Lavoisier elaborou este sistema de pesquisa sistemática em sua prática de laboratório. Vale a pena consultar sua vida e obra...*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipos de erro\n",
    "\n",
    "Fato:\n",
    "\n",
    "- O soldado comparece à corte marcial e é fuzilado\n",
    "\n",
    "Do ponto de vista da **verdade**\n",
    "\n",
    "- ele era inocente\n",
    "\n",
    "- ele era culpado\n",
    "\n",
    "Do ponto de vista da **sentença**\n",
    "\n",
    "- ele foi absolvido\n",
    "\n",
    "- ele foi condenado\n",
    "\n",
    "Duas situações podem ser excluídas de cara:\n",
    "\n",
    "- o soldado era inocente e foi absolvido\n",
    "\n",
    "- o soldado era culpado e foi condenado\n",
    "\n",
    "---\n",
    "\n",
    "Sistema inicialmente setado em:\n",
    "\n",
    "#### H0 - todo réu é inocente até que se prove o contrário\n",
    "\n",
    "#### Erro do Tipo 1:\n",
    "\n",
    "- o soldado era inocente e foi condenado - validou erradamente a H1 (o alarme deu um falso disparo)\n",
    "\n",
    "É o $\\alpha$, ou **falso positivo**\n",
    "\n",
    "*memorize que erros do Tipo 1 são o **pior tipo de erro** que pode acontecer!*\n",
    "\n",
    "#### Erro do Tipo 2:\n",
    "\n",
    "- o soldado era culpado e foi absolvido (o alarme não disparou)\n",
    "\n",
    "Na prática:\n",
    "\n",
    "- eu defino um **limiar** (threshold) denominado $\\alpha$, para erros do tipo 1 e o seto o mais baixo possível para **evitar ao máximo** erros do tipo 1\n",
    "\n",
    " - medicina: 1%\n",
    " \n",
    " - negócios, pesquisas e outras aplicações: 5%\n",
    " \n",
    "*em alguns casos isso pode ficar bem abaixo disso. Por exemplo, bons alarmes possuem mais de um mecanismo de sensoriamento. Um disparo é analisado pela central de acordo com os padrões que surgiram (algo passou pelo muro - um gato?) + (alguém tentou forçar a caixa de telefonia - um mendigo?) + (uma sombra atravessou a câmera de vigilância do beco...)*\n",
    "\n",
    "*em medicina isso também é usado: o médico examina outros sinais e pede outros exames antes de fechar um diagnóstico. O médico também entrevista o paciente*\n",
    "\n",
    "*por que é perigoso usar aeronaves, ou automóveis que ainda são **protótipos**? Por essa mesma razão, os equipamentos não foram submetidos a todas as hipóteses de stress, erros fatais podem ter sido cometidos durante o seu projeto, ou a sua construção!*\n",
    "\n",
    "*Frequently people will talk about the \"power\" of a statistical test as 1 - Beta (or 1 minus the type two error rate). This is the ability of an individual to correctly choose the alternative hypothesis*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Como escolhemos hipóteses?\n",
    "\n",
    "Eu poderia fazer toda a minha inferência apenas me baseando em intervalos de confiança. Mas a maneira mais elegante de fazer isso e escolhendo uma H0 e verificando se minha inferência estatística cabe dentro dela. Se não couber, estaremos então validando H1\n",
    "\n",
    "*por todo momento você está testando **parâmetros**! A estatística da sua amostra é algo já conhecido!*\n",
    "\n",
    "*segundo o Teorema do Limite Central, com uma amostra de **150**, a média terá a conformação de uma curva normal* \n",
    "\n",
    "*em teste de hipóteses, nós primeiro simulamos a partir do **primeiro** valor da hipótese alternativa que ainda estiver no intervalo da hipótese nula** \n",
    "\n",
    "*a distribuição amostral para a média também é igual a **(CODECOG) \\FRAC{SIGMA}{SQRT{N}}**. Observe isto*\n",
    "\n",
    "*comparando a média amostral de fato com esta distribuição, nos dirá o quão provável é que nossa estatística tenha vindo da hipótese nula*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais sobre estatística [aqui](http://www.statsmodels.org/stable/glm.html) e [aqui](http://onlinestatbook.com/2/sampling_distributions/samp_dist_mean.html)\n",
    "\n",
    "Em um teste prático:\n",
    "\n",
    "- nós devemos esperar que o desvio padrão da distribuição amostral por diferenças de H0 seja essencialmente a mesma que nós observamos nos dados\n",
    "\n",
    "- se H0 for verdadeira, nós devemos esperar que a diferença entre as médias dos bebedores e dos não bebedores de café seja zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### $Valor-P$\n",
    "\n",
    "*se no gráfico de diferenças para H0, o valor-p for menor do que 0.01, já nos parece improvável que a estatística provenha de H0!*\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "H0: There was no change in the average morale of the company\n",
    "\n",
    "H1: The new program on average increased morale\n",
    "\n",
    "*The probability of the observed change in average morale occurring or an average change even more in favor of an increase in morale given there was actually no change in morale*\n",
    "\n",
    "Nós podemos integrar a distribuição amostral até encontrar valor(es)-p para a(s) cauda(s), de maneira a que a área excluída seja a que buscamos (ex: 5%, em duas caudas)\n",
    "\n",
    "Uma alternativa é irmos simulando até encontrar valor(es)-p adequados!\n",
    "\n",
    "- Simulate the values of your statistic that are possible from the null\n",
    "\n",
    "- Calculate the value of the statistic you actually obtained in your data\n",
    "\n",
    "- Compare your statistic to the values from the null\n",
    "\n",
    "- Calculate the proportion of null values that are considered extreme based on your alternative\n",
    "\n",
    "---\n",
    "\n",
    "É testado um produto novo\n",
    "\n",
    "A estatística no caso é a média amostral do moral da equipe de vendas depois, menos o moral antes do novo produto \n",
    "\n",
    "H0: o antigo era **igual** ou **melhor** ao novo\n",
    "\n",
    "- sem diferença:\n",
    "\n",
    " - a área sombreada é **maior** do que a positiva\n",
    " \n",
    " - e menor do que a **negativa** da minha estatística observada para obter o valor-p\n",
    " \n",
    "- o moral médio da minha equipe de vendas é menor do que antes:\n",
    "\n",
    " - a área sombreada é **menor** do que a da minha estatística observada para obter o valor-p\n",
    " \n",
    "- o moral médio da equipe de vendas é maior do que antes\n",
    "\n",
    " - a área sombreada é **maior** do que a da minha estatística observada para obter o valor-p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É uma ferramenta bastante útil para saber se minha H1 estaria à **direita** ou à **esquerda** da minha distribuição, a média de respostas False/True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(null_vals > sample_mean).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra coisa que eu posso fazer é plotar o histograma da minha H1 e ver se as linhas de limite da minha H0 cruzam meu histograma. Caso elas fiquem distantes, o mais plausível é que H0 realmente não esteja representando bem meu experimento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = sample_mean\n",
    "high = null_mean + (null_mean - sample_mean)\n",
    "\n",
    "plt.hist(null_vals);\n",
    "plt.axvline(x=low, color='r', linewidth=2)\n",
    "plt.axvline(x=high, color='r' linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se:\n",
    "    \n",
    "$valor-p$ <= $\\alpha$\n",
    "\n",
    "Então **rejeite** Ho!\n",
    "\n",
    "---\n",
    "\n",
    "Evite **cringeworthy conclusions**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuidados\n",
    "\n",
    "- minha amostra é representativa da população?\n",
    "\n",
    "- qual o impacto do tamanho da minha amostra em relação aos meus resultados?\n",
    "\n",
    " - tamanhos amostrais **muito grandes** geram validação da H1!\n",
    " \n",
    "   - nestes casos, hypothesis testing não é a ferramenta adequada\n",
    "   \n",
    "   - eu poderia por exemplo criar uma H2, H3, produtos personalizados...\n",
    "   \n",
    "   - com **machine learning** eu posso aproximar até uma customização a nível de **indivíduo**!\n",
    "   \n",
    "- se muitas pesquisas do mundo estão fazendo o mesmo teste, eu devo aplicar a **Correção de Bonferroni**:\n",
    "\n",
    " - $\\alpha$ / m onde m é o número de testes\n",
    " \n",
    " - dez testes simultâneos a 5% na verdade geram um threshold de 0,5%!\n",
    " \n",
    " - outros:\n",
    " \n",
    "  - Tukey\n",
    "  \n",
    "  - Q-Values\n",
    "  \n",
    "- Effect Size é um problema e existe um curso que trata disso [aqui](https://www.youtube.com/watch?v=z98xODInLCQ)\n",
    "\n",
    "- \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Pontos para guardar:\n",
    "\n",
    "- $\\alpha$ é o gatilho para o percentual de erros do **Tipo I** que eu aceito cometer (ele tem que estar setado **antes** do experimento ser conduzido. Não vale um \"ah, bom... me parece que este Alfa é aceitável neste caso...)\n",
    "\n",
    " - a primeira coisa que eu faço é setar meu Alfa. Para isso eu tenho que saber se é uma inferência **bicaudal** ou **monocaudal**. Daí, tendo o meu parâmetro de qualidade que é:\n",
    " \n",
    "   - curiosidade geral / ciência -> 5%\n",
    " \n",
    "   - medicina -> 1%\n",
    "   \n",
    " - então eu descubro o Alfa, baseado no fato de deixar um área de cauda compatível com esta situação. O Alfa será meu gatilho!\n",
    " \n",
    " - depois eu seto H0 como sendo **verdadeira**. Eu não posso escolher setar qual será verdadeira. Sempre será H0. E o tipo de erro a ser evitado é o erro **Tipo I**, que é mudar, sem precisar de mudança\n",
    " \n",
    " - daí colho minha amostra. Já nesse ponto eu tenho uma boa intuição se H0 será validada ou não. Quando os dados são numéricos,  plotar o histograma da amostra já me mostra qual o ponto central. Se estiver perto do valor predito por H0, é praticamente certo que ela validará\n",
    " \n",
    " - agora eu faço o bootstrapping da minha amostra setada para H0. E agora eu vou determinar o meu valor-p. Ele tem a ver com o quanto eu me afastei da minha previsão. Um segundo momento, se os meus dados forem booleanos, é no meu bootstrapping. Ocorre a mesma coisa: eu observo o ponto central e terei uma forte intuição de quem irá vencer\n",
    " \n",
    " - o $valor-P$ é a probabilidadede de observar nossa estatística, ou uma estatística mais extremada, na H0 \n",
    " \n",
    " - se eu estiver trabalhando com hipóteses do tipo H0:a>=5, então eu terei uma validação **monocaudal**. Agora se minha hipótese zero for do tipo H0:a=5, então eu não sei de qual lado o dado poderá falsificar. E portanto, eu terei uma validação **bicaudal**. Isso é muito importante na hora de distribuir minha confiança (ex: 5%) em dois extremos de 2.5% cada um, ou em um de 5%!\n",
    " \n",
    " - finalmente, eu faço a comparação: se Alfa for **maior** do que meu valor-p, o gatinho disparou e eu devo **rejeitar a hipótese nula**\n",
    " \n",
    " - caso contário, nada aconteceu e eu devo **aceitar a hipótese nula**\n",
    "\n",
    "Mais um ponto a se considerar:\n",
    "\n",
    "- correção de Bonfferroni (no caso de múltiplos parâmetros, como um experimento sendo conduzido simultaneamente em várias partes do mundo)\n",
    "\n",
    "Texto: o que é o valor p [aqui](https://rebeccaebarnes.github.io/2018/05/01/what-is-a-p-value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes A/B\n",
    "\n",
    "*há vários notebooks Jupyter sobre isso!*\n",
    "\n",
    "---\n",
    "\n",
    "- para alguns infelizes, chamado **grupo de controle**, é dado um placebo, um conhecimento todo errôneo, uma arma que não atira ou uma webpage totalmente obsoleta\n",
    "\n",
    "- para outros é dado o produto que desejamos testar. Este é o **grupo experimental**\n",
    "\n",
    "Depois verificamos o que aconteceu em média em cada grupo. Se apenas os pacientes do placebo morreram, se os estudantes continuaram desempregados, se o policial morreu em combate, se os usuários se sentiram realmente sonolentos com a sua \"novíssima\" webpage...\n",
    "\n",
    "O que isso evita: você meter no mercado um produto totalmente disfuncional. Uma coisa **enganadora**... remédio que é remédio bom mesmo tem que matar ou curar e não ficar naquela frescurazinha de que \"está curando, mas é como a canja da vovó, leva tempo para se acostumar...\"\n",
    "\n",
    "E o que acontece com o grupo de controle depois que terminou o experimento? Bem, por acaso você já se perguntou com carne de quê salsicha é feita?\n",
    "\n",
    "---\n",
    "\n",
    "É **imprescindível** observar também se não houve interferência de um dos desses dois fatores:\n",
    "\n",
    "- change version - o cara não gostou da mudança do visual do Windows 10, apenas porque ele teve que aprender tudo de novo, do zero, como mexer no seu computador... (a depreciação foi porque o seu usuário é um bebê mimado)\n",
    "\n",
    "- novelty effect - o cara achou \"uau\" a nova versão do tênis de corrida, porque você meteu dourado nos detalhes e agora ele pensa se o próprio Homem de Ferro... (a escolha não teve nada a ver com o sistema de suspensão)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Caso da provedora de cursos online **Audacity**\n",
    "\n",
    "Audacity possui um **fluxograma** das atividades dos seus visitantes. Alguns veem buscar quizzes, outros por curiosidade... e alguns chegam a criar um login para pegar algum curso ou material gratuito... e outros entram na parte que sustenta o sistema e contratam um curso pago... do que contratam o curso pago, alguns terminam o curso e tiram o certificado... e acabam voltando para mais cursos! (a forma geral deste fluxo é a de um **garrafão invertido**\n",
    "\n",
    "A fim de melhorar a performance, duas iniciativas:\n",
    "\n",
    "- Projeto A: reformular página Web\n",
    "\n",
    " - Explore courses now! button\n",
    "\n",
    "Como minha página experimental é vista por apenas uma **fração** dos usuários, isso também tem que ser ponderado para se tirar a **eficiência** do novo botão:\n",
    "\n",
    "- CTR (Click Through Rate - #visitantes únicos que clicaram / #visitantes únicos que visitaram)\n",
    "\n",
    "- $H1: \\pi{new} > \\pi{old}$\n",
    "\n",
    "- Assim $H0: \\pi{new} \\leq \\pi{old}$\n",
    "\n",
    "Reagrupando os termos:\n",
    "\n",
    "- $H0: \\pi{new} - \\pi{old} \\leq 0$\n",
    "\n",
    "- $H1: \\pi{new} - \\pi{old} > 0$\n",
    "\n",
    "*escritor de equações LaTex [aqui](https://latex.codecogs.com/eqneditor/editor.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Considerações finais:\n",
    "\n",
    "- o método de Bonferroni é **conservativo**, dependendo do caso, pode-se usar outro mais adequado\n",
    "\n",
    "- **jamais** viole os princípios da Estatística! Quando você tenta \"cozinhar\" resultados, você está pondo reputações, capital e vidas em risco!\n",
    "\n",
    "---\n",
    "\n",
    "Desafios a serem consdiderados ao aplicar um teste A/B:\n",
    "\n",
    "- Novelty effect and change aversion \n",
    "\n",
    " - when existing users first experience a change\n",
    "\n",
    "- Sufficient traffic and conversions\n",
    "\n",
    " - to have significant and repeatable results\n",
    "\n",
    "- Best metric choice for making the ultimate decision \n",
    "\n",
    " - eg. measuring revenue vs. clicks\n",
    "\n",
    "- Long enough run time for the experiment\n",
    "\n",
    " - to account for changes in behavior based on time of day/week or seasonal events\n",
    "\n",
    "- Practical significance of a conversion rate\n",
    "\n",
    " - the cost of launching a new feature vs. the gain from the increase in conversion\n",
    "\n",
    "- Consistency among test subjects in the control and experiment group\n",
    " \n",
    " - imbalance in the population represented in each group can lead to situations like Simpson's Paradox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Problemas de Comparação Múltipla [aqui](https://en.wikipedia.org/wiki/Multiple_comparisons_problem)\n",
    "\n",
    "- Correção de Bonferroni [aqui](https://en.wikipedia.org/wiki/Bonferroni_correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "*há vários notebooks Jupyter sobre isso!*\n",
    "\n",
    "---\n",
    "\n",
    "#### Machine learning\n",
    "\n",
    "- Supervised learning -> data to predict:\n",
    "\n",
    " - a value\n",
    " \n",
    " - a label\n",
    " \n",
    " - Qual o cachorro que você vai comprar?\n",
    " \n",
    " - Qual imóvel serviria para você?\n",
    " \n",
    "- Unsupervised learning -> clustering data based on common characteristics\n",
    "\n",
    " - agrupamento de clientes\n",
    " \n",
    " - agrupamento em tópicos\n",
    "\n",
    "\n",
    "*o que nos interessa aqui? É que em Supervised learning, **regressões** são extremamente úteis!*\n",
    "\n",
    "*aqui nós vamos comparar apenas duas variáveis quantitativas!*\n",
    "\n",
    "---\n",
    "\n",
    "Nós vamos precisar para começar:\n",
    "\n",
    "- de alguns dados X vs Y\n",
    "\n",
    "- visualizar isso através de um **scatter plot**\n",
    "\n",
    "Em seguida vamos tentar:\n",
    "\n",
    "- determinar um **coeficiente de correlação**\n",
    "\n",
    "- plotar nossa reta no gráfico\n",
    "\n",
    "- medir o quanto ela se ajustou aos nossos dados\n",
    "\n",
    "Meu objetivo será determinar:\n",
    "\n",
    "- a direção/inclinação da reta (positive, negative) (1... -1)\n",
    "\n",
    "- sua força em explicar os dados (strong, average, weak) (1... -1)\n",
    "\n",
    "- coeficiente de correlação **r** (correlação de Pearson)\n",
    "\n",
    "    - **negativo** para dados relacionados inversamente\n",
    "    \n",
    "    - **positivo** para dados relacionados diretamente\n",
    "    \n",
    "    - **forte** para valores entre 0.7 e 1 (inclui o 0.7)\n",
    "    \n",
    "    - **modelado** para valores a partir de 0.3 (inclui o 0.3)\n",
    "    \n",
    "    - **fraco** para menores de 0.3\n",
    "\n",
    "Outras coisas que depois podem me ajudar:\n",
    "\n",
    "- o ponto onde essa reta cruza as abscissas (em alguns casos é útil)\n",
    "\n",
    "- o **domímio** da função (se eu vou aceitar valores de X ou de Y negativos, a faixa de valores)\n",
    "\n",
    "- até onde a extrapolação é aceitável (normalmente eu posso extrapolar ligeiramente para cima ou para baixo de onde eu ainda tenho pontos... mas é uma imprudência seguir muito longe!)\n",
    "\n",
    "---\n",
    "\n",
    "A variável X é aquela que eu tenho controle:\n",
    "\n",
    "- variável independente\n",
    "\n",
    "- explanatory variable\n",
    "\n",
    "A variável Y é dependente de X: \n",
    "\n",
    "- y = f(x)\n",
    "\n",
    "- variável dependente\n",
    "\n",
    "- response variable\n",
    "\n",
    "*exemplo é preços x vendas. Eu consigo controlar as mentes dos meus clientes ara que eles continuem comprando? Normalmente não. Mas eu consigo controlar preços e ver como as vendas se comportam? Certamente que sim!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca boa do Python para lidar com isso é **statsmodels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A reta\n",
    "\n",
    "Dois elementos\n",
    "\n",
    "- intercept -> onde a reta cruza o eixo $x$ (explanatory), $b0$,  $\\beta0$\n",
    "\n",
    "- slope -> coeficiente de inclinação, $b1$, $\\beta1$\n",
    "\n",
    "- $\\hat{Y}1$ -> valores na reta $(X1, \\hat{Y}1)$\n",
    "\n",
    "- $Y1$ -> pontos de dados $(X1, Y1)$\n",
    "\n",
    "---\n",
    "\n",
    "Predicted:\n",
    "\n",
    "- $\\hat{Y}1$ - A predicted value of the response\n",
    "\n",
    "- $b0$ - The predicted value of the response when the explanatory variable is zero\n",
    "\n",
    "- $b1$ - The predicted change in the response for every one unit increase in the explanatory variable.\n",
    "\n",
    "Actual:\n",
    "\n",
    "- $Y1$ - An actual value of the response\n",
    "\n",
    "- $\\beta0$ - The actual average response value for the population when the explanatory variable is zero\n",
    "\n",
    "- $\\beta1$ - The actual average change in the response for the population with every one unit increase in the explanatory variable\n",
    "\n",
    "Others:\n",
    "\n",
    "- $n$ - The number of rows in the dataset\n",
    "\n",
    "---\n",
    "\n",
    "Para determinar o quanto a curva explica os pontos:\n",
    "\n",
    "- algoritmo do quadrado mínimo (Lei dos Quadrados Mínimos)\n",
    "\n",
    "A solução tradicional [aqui](https://www.youtube.com/watch?v=zPG4NjIkCjc) - não faz mais sentido usá-la já que tempos Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Intercept** - normalmente é necessário em nossos modelos lineares. Infelizmente, o statsmodels não o coloca automaticamente e é necessário fazer isso na mão. Uma descrição [aqui](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model)\n",
    "\n",
    "*ao que parece, se você não seta o intercept, o modelo acumula erro residual no seu ponto \"morto\". E depois as coisas não saem mais como deveriam*\n",
    "\n",
    "---\n",
    "\n",
    "**Summary** - do python é extremamente útil:\n",
    "\n",
    "- **area** -> $b1$\n",
    "\n",
    "- **intercept** -> $b0$\n",
    "\n",
    "---\n",
    "\n",
    "**Inserir dados**\n",
    "\n",
    "Verifique que todas as suas entradas estejam na mesma **unidade**!\n",
    "\n",
    "Observe se o valor do **intercept** parece consistente\n",
    "\n",
    "Verifique se o modelo faz sentido para o mundo **real** (isso não é apenas um exercício acadêmico!)\n",
    "\n",
    "---\n",
    "\n",
    "**P > |t|** - valor-P\n",
    "\n",
    "Area: 0.0 <- isso é um **forte** indicativo de que este parâmetro (área) é **essencial** para predizer o valor de um imóvel!\n",
    "\n",
    "- H0: $\\beta1$ = 0\n",
    "\n",
    "- H1: $\\beta1$ <> 0\n",
    "\n",
    "Intercept: 0.8231...\n",
    "\n",
    "- H0: $\\beta0$ = 0\n",
    "\n",
    "- H1: $\\beta0$ <> 0\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can perform hypothesis tests for the coefficients in our linear models using Python (and other software). These tests help us determine if there is a statistically significant linear relationship between a particular variable and the response. The hypothesis test for the intercept isn't useful in most cases\n",
    "\n",
    "However, the hypothesis test for each x-variable is a test of if that population slope is equal to zero vs. an alternative where the parameter differs from zero. Therefore, if the slope is different than zero (the alternative is true), we have evidence that the x-variable attached to that coefficient has a statistically significant linear relationship with the response. This in turn suggests that the x-variable should help us in predicting the response (or at least be better than not having it in the model)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como saber se um modelo é adequado?\n",
    "\n",
    "**R-squared** - a quantidade de variabilidade na resposta (Y) explicada pelo seu modelo\n",
    "\n",
    "- 1 é total\n",
    "\n",
    "- 0 é nenhuma\n",
    "\n",
    "*nenhuma não significa que a coisa não pode ser **modelizada**! Podemos tentar por exemplo, retirar outliers, definir um domínio de aplicação (ex: para valores de X entre 5 e 25) ou buscar outros tipos de modelos, como curvas log-exp, quadráticas, cúbicas, etc..*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The Rsquared value is the square of the correlation coefficient.\n",
    "\n",
    "A common definition for the Rsquared variable is that it is the amount of variability in the response variable that can be explained by the x-variable in our model. In general, the closer this value is to 1, the better our model fits the data\n",
    "\n",
    "Many feel that Rsquared isn't a great measure (which is possibly true), but I would argue that using cross-validation can assist us with validating any measure that helps us understand the fit of a model to our data. Here, you can find one such argument explaining why one individual doesn't care for Rsquared\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problema do valor das casas vs área\n",
    "\n",
    "The **p-value** associated with area... \n",
    "\n",
    "- is very ***small**, which suggests \n",
    "\n",
    " - there is statistical evidence that the population **slope** associated with area in relating to price is **non-zero**\n",
    " \n",
    "*é o valor $P>|t|$. Quando nele aparece um valor muito **pequeno**, de fato isso sugere que existe **forte correlação** entre as duas variáveis!* \n",
    "\n",
    "*isso representa a **significância** da relação entre a variável de entrada e a variável de resposta. Um valor-P abaixo de 0.05 é considerado **significante** (no entanto, outros números, como 0.16 podem ser considerados **relevantes**)\n",
    "\n",
    "*uma abordagem prática é criar um modelo o mais complexo possível e depois ir **simplificando** até conseguir descrever sem perda de qualidade, com o menor número possível de variáveis**\n",
    "\n",
    "---\n",
    "\n",
    "#### Problema do valor de um diamante vs quilates\n",
    "\n",
    "As assertivas mais complexas apareceram no problema \"casas vs criminalidade\". Observar a explicação para cada uma delas.\n",
    "\n",
    "---\n",
    "\n",
    "#### Problema do valor das casas vs criminalidade\n",
    "\n",
    "\n",
    "Explicações\n",
    "\n",
    "- \"Para cada vez que a criminalidade aumenta 100%, o valor mediano do imóvel diminui 412.8 dólares\"\n",
    "\n",
    " - isso está relacionado ao coeficiente b1\n",
    " \n",
    " - o fator de inclinação da reta é de -0.4128 (ou seja negativo e o resto são apenas ajustes de unidades)\n",
    " \n",
    "- \"Se não existem crimes, o valor mediano esperado para uma casa é de 24,016 dólares\"\n",
    "\n",
    " - crimes é meu eixo de variável x\n",
    " \n",
    " - para que crime seja zero, este será o ponto onde a reta intecepta x=0\n",
    " \n",
    " - o coeficiente inicial b0 é de 24.016 (o resto são ajustes de unidades)\n",
    "\n",
    "- \"14.9 da variabilidade do preço de um imóvel está relacionado à taxa de criminalidade por mil habitantes\"\n",
    "\n",
    " - o valor $R^2$ é de 0.149\n",
    "\n",
    "- \"O valor-p provido de 0.0000 de crimes per capita é estatisticamente relevante para prover informações na previsão do valor de imóveis\"\n",
    "\n",
    " - o $p >|t|$ de fato é este valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "*há vários notebooks Jupyter sobre isso!*\n",
    "\n",
    "---\n",
    "\n",
    "- multiple inputs\n",
    "\n",
    "- **variable** e **cathegorical** (damos um peso a cada elemento da categoria: casa moderna=3, vitoriana=5...) data\n",
    "\n",
    "- uso de álgebra linear para alimentar todos os parâmetros\n",
    "\n",
    "- Vetor de preços: Y\n",
    "\n",
    "- Matriz de dados: X\n",
    "\n",
    "Livro recomendado [aqui](https://www.amazon.com.br/Introduction-Statistical-Learning-Applications-Statistics-ebook/dp/B01IBM7790/ref=tmm_kin_swatch_0?_encoding=UTF8&qid=1549072697&sr=8-1)\n",
    "\n",
    "Curso Khan Academy gratuito em Álgebra Linear [aqui](https://www.khanacademy.org/math/linear-algebra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Passos:\n",
    "\n",
    "- primeiro eu identifico quais as variáveis **numéricas** (as **categóricas** eu lidarei mais tarde, usando um truque\n",
    "\n",
    "- depois eu faço regressões preço x cada uma das variáveis e verifico se cada uma delas é explicativa\n",
    "\n",
    "- por fim eu coloco todas em um modelo só\n",
    "\n",
    "*observe que no caso de **imóveis** a soma dos r-quadrados deu **maior do que um**. Por que isso? Por uma razão simples, essas variáveis não são totalmente **independentes**!*\n",
    "\n",
    "*assim em uma casa com maior área construída é de se esperar que ela apresente mais dormitórios, mais banheiros...*\n",
    "\n",
    "---\n",
    "\n",
    "Os **intercepts** vêm daqui:\n",
    "\n",
    "The takeaway for us is that we can find the optimal $\\beta$ estimates by calculating $(X'X)^{-1}X'y$\n",
    "\n",
    "    np.dot(np.dot(np.linalg.inv(np.dot(x.transpose(), x),x.transpose()), y)\n",
    "    \n",
    "Como isso funciona [aqui](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Nós podemos sempre continuar a **adcionar** variáveis a um modelo, e a interpretação dos coeficientes adquiridos contiunuará a mesma. Mas você pode continuar adicionando variáveis à parte **mantida constante** da interpretação\n",
    "\n",
    "É aqui que nós podemos acrescentar nossas **variáveis dummy**. Uma das maneiras é usar uma notação binária. O 1 denota **existência**\n",
    "\n",
    "Por exemplo, padrão do bairro:\n",
    "\n",
    "- A -> 1\n",
    "\n",
    "- B, C, D -> 0\n",
    "\n",
    "Você também pode criar com isso uma **tabela de setagem**:\n",
    "\n",
    "- Coluna A -> 0/1\n",
    "\n",
    "- Coluna B -> 0/1\n",
    "\n",
    "...\n",
    "\n",
    "- Coluna G- -> 0/1\n",
    "\n",
    "*e a última delas você pode jogar fora, pois você sabe pelas outras, quando ela será setada! E isso define um conjunto de vetores **linearmente independentes** e portanto, uma matriz [**full rank**](https://www.cds.caltech.edu/~murray/amwiki/index.php/FAQ:_What_does_it_mean_for_a_non-square_matrix_to_be_full_rank%3F)*\n",
    "\n",
    "*assim, no final das contas, suponto que eu tenha **cinco coeficientes** a adicionar no meu modelo, elas serão representadas por **quatro variáveis dummie** e mais o **intercept**. O que você joga fora é chamado de **baseline category**. É assim que funciona!*\n",
    "\n",
    "*uma das razões para eu eliminar uma das **variáveis dummie** é assegurar que o produto cartesiano $X X´$ seja invertível!*\n",
    "\n",
    "---\n",
    "\n",
    "O Pandas já possui um método para lidar com variáveis categóricas. Você pode usar **.get_dummies()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Notas gerais:\n",
    "\n",
    "- **multiple linear regression model** é similar a **simple linear regression**\n",
    "\n",
    "- encode **dummy variables** e interpretar seus coeficientes\n",
    "\n",
    "- fazem que a interpretação direta dos coeficientes não seja tão prioritária (o modelo tem que predizer **melhor** e não se trabalhar a interpretação):\n",
    "\n",
    " - **higher order terms** e suas implicações na hora de interpretar coeficientes\n",
    "\n",
    " - significado de **interação** para que um fator seja necessário no nosso modelo\n",
    "\n",
    "- **model assumptions**\n",
    "\n",
    "- **multicollinearity** e como ela impacta nos coeficientes e erros padrão do modelo\n",
    "\n",
    "- **variance inflation factors**\n",
    "\n",
    "*em produtos médicos é normal se usar um software chamado SAS. Seus critérios são um pouco diferentes dos vistos aqui*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qual a razão de estar modelando?\n",
    "\n",
    "- como as minhas **entradas** se relacionam à **resposta**?\n",
    "\n",
    "- busco a **melhor predição**\n",
    "\n",
    "- quais **entradas** melhor predizem a **resposta**?\n",
    "\n",
    "Problemas possíveis:\n",
    "\n",
    "- uma correlação linear não existe ou é frágil demais\n",
    "\n",
    "- tenho muitos erros/ruídos\n",
    "\n",
    "- minhas variáveis mudam constantemente\n",
    "\n",
    "- outliers\n",
    "\n",
    "- multicolinearidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Statistical Learning book notes\n",
    "\n",
    "#### Linearity\n",
    "The assumption of linearity is that a linear model is the relationship that truly exists between your response and predictor variables. If this isn't true, then your predictions will not be very accurate. Additionally, the linear relationships associated with your coefficients really aren't useful either.\n",
    "\n",
    "In order to assess if a linear relationship is reasonable, a plot of the residuals $(y -\\hat{y})(y−y^​)$ by the predicted values $(\\hat{y})(y^​)$ is often useful. If there are curvature patterns in this plot, it suggests that a linear model might not actually fit the data, and some other relationship exists between the predictor variables and response. There are many ways to create non-linear models (even using the linear model form), and you will be introduced to a few of these later in this lesson.\n",
    "\n",
    "In the image at the bottom of this page, these are considered the biased models. Ideally, we want to see a random scatter of points like the top left residual plot in the image.\n",
    "\n",
    "#### Correlated Errors\n",
    "Correlated errors frequently occur when our data are collected over time (like in forecasting stock prices or interest rates in the future) or data are spatially related (like predicting flood or drought regions). We can often improve our predictions by using information from the past data points (for time) or the points nearby (for space).\n",
    "\n",
    "The main problem with not accounting for correlated errors is that you can often use this correlation to your advantage to better predict future events or events spatially close to one another.\n",
    "\n",
    "One of the most common ways to identify if you have correlated errors is based on the domain from which the data where collected. If you are unsure, there is a test known as a Durbin-Watson test that is commonly used to assess whether correlation of the errors is an issue. Then ARIMA or ARMA models are commonly implemented to use this correlation to make better predictions.\n",
    "\n",
    "#### Non-constant Variance and Normally Distributed Errors\n",
    "Non-constant variance is when the spread of your predicted values differs depending on which value you are trying to predict. This isn't a huge problem in terms of predicting well. However, it does lead to confidence intervals and p-values that are inaccurate. Confidence intervals for the coefficients will be too wide for areas where the actual values are closer to the predicted values, but too narrow for areas where the actual values are more spread out from the predicted values.\n",
    "\n",
    "Commonly, a log (or some other transformation of the response variable is done) in order to \"get rid\" of the non-constant variance. In order to choose the transformation, a Box-Cox is commonly used.\n",
    "\n",
    "Non-constant variance can be assessed again using a plot of the residuals by the predicted values. In the image at the bottom of the page, non-constant variance is labeled as heteroscedastic. Ideally, we want an unbiased model with homoscedastic residuals (consistent across the range of values).\n",
    "\n",
    "Though the text does not discuss normality of the residuals, this is an important assumption of regression if you are interested in creating reliable confidence intervals.\n",
    "\n",
    "#### Outliers/Leverage Points\n",
    "Outliers and leverage points are points that lie far away from the regular trends of your data. These points can have a large influence on your solution. In practice, these points might even be typos. If you are aggregating data from multiple sources, it is possible that some of the data values were carried over incorrectly or aggregated incorrectly.\n",
    "\n",
    "Other times outliers are accurate and true data points, not necessarily measurement or data entry errors. In these cases, 'fixing' is more subjective. Often the strategy for working with these points is dependent on the goal of your analysis. Linear models using ordinary least squares, in particular, are not very robust. That is, large outliers may greatly change our results. There are techniques to combat this - largely known as regularization techniques. These are beyond the scope of this class, but they are quickly discussed in the free course on machine learning.\n",
    "\n",
    "#### Multi-collinearity\n",
    "Multicollinearity is when we have predictor variables that are correlated with one another. One of the main concerns of multicollinearity is that it can lead to coefficients being flipped from the direction we expect from simple linear regression.\n",
    "\n",
    "One of the most common ways to identify multicollinearity is with bivariate plots or with variance inflation factors (or VIFs). This is a topic we will dive into in the next concept, so we won't spend as much time on it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multicolinearidade\n",
    "\n",
    "*nossas variáveis-x devem estar correlacionadas com a **resposta** e não entre elas mesmas!*\n",
    "\n",
    "*um caso é o de potência de processadores de vídeo e tela com qualidade Retina, em modelos de notebook: ora se o fabricante se importou em gastar mais com uma placa de vídeo poderosa, é praticamente evidente que ele também irá se importar em botar uma tela com resolução mais cristalina! Então modelizar essas duas variáveis em um modelo linear de custo de notebook quase certamente será redundante e só me causará ruído!*\n",
    "\n",
    "VIF - Variance Inflation Factors [aqui](https://etav.github.io/python/vif_factor_python.html)\n",
    "\n",
    "Para identificar:\n",
    "\n",
    "- Scatterplot Matrix\n",
    "\n",
    "- VIF > 10 multicolinearidade VIFi = 1/1-R2i\n",
    "\n",
    "Estratégia:\n",
    "\n",
    "- remover as de menor interesse no meu modelo (ex: número de quartos)\n",
    "\n",
    "- comece removendo uma, verificando os impactos e quando os VIFs ficarem aceitáveis, poderá parar por aí (monitore se o r-square não se alterou significantemente para pior e se os STD-Err diminuíram!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higher order terms\n",
    "\n",
    "Em alguns casos, nós podemos simular resultados lineares, com dados que se comportam mais como se fossem não lineares:\n",
    "\n",
    " - interações $x1 x2$\n",
    " \n",
    " - $r^2$, $r^3$, $r^n$\n",
    " \n",
    "O problema é que interpretar os resultados torna-se bem mais **complexo** e afastando-se do eixo X, há considerações que devem ser seguidas\n",
    "\n",
    "*no caso de **interações**, eu entendo que existe um fator **sinérgico** ou **disnérgico** no meu modelo: adicionando mais uma pessoa à minha classe de teatro, as oficinas ficam mais interessantes e o amigo daquela pessoa tem 20% de também se agregar!*\n",
    "\n",
    "$\\hat{y} = b0 + b1x1 + b2x2 + b3x1x2$\n",
    "\n",
    "*conforme o preço das casas da vizinhança B aumenta com a área, o preço das casas da vizinhança A aumenta com a área multiplicado por um fato de correlação entre elas, $b3$*\n",
    "\n",
    "*para termos que **não estão** associados a correlações ou elementos não lineares, a lógica de inferência do quanto um aumenta... continua a mesma. Eles só se contaminam os envolvidos!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "*há vários notebooks Jupyter sobre isso!*\n",
    "\n",
    "---\n",
    "\n",
    "Extratifica os dados em dois grupos possíveis\n",
    "\n",
    "- é ótima para criar **categorias**, pois é exponencialmente divergente para as duas pontas!\n",
    "\n",
    "#### Interpretação mais complexa:\n",
    "\n",
    "- para interpretar os dados, você tem que **exponenciar** para ter o resultado correto (1.46 -> 4.32)\n",
    "\n",
    "        exp(coef)\n",
    "        \n",
    "- aumenta 1 unidade em $x1$, aumentam em uma chance mulpitilicativa $e^{b1}$ em A\n",
    "\n",
    "#### Variáveis dummie:\n",
    "\n",
    "- modo de tratar é exatamente **igual** à colineariedade\n",
    "\n",
    "- no entanto, devo lembrar que agora meu modelo é **exponencial**\n",
    "\n",
    "- quando eu obtenho valores **negativos**, normalmente é mais benéfico trabalhar com $1/x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnóstico - eficiência\n",
    "\n",
    "- accuracy -> proporção: rótulos corretos / linhas\n",
    "\n",
    "#### Confusion matrix (machine learning)\n",
    "\n",
    "- classe predita vs classe de fato\n",
    "\n",
    "- linhas: de fato\n",
    "\n",
    "- colunas: predito\n",
    "\n",
    "**Recall:**\n",
    "\n",
    "- isso é bom para sistemas de reconhecimento facial do Facebook! É um sistema para entretenimento e um falso positivo acaba produzindo apenas ridadas!\n",
    "\n",
    "- minha maior preocupação é identificar o maior número de gente\n",
    "\n",
    " - se eu erro um pouco, meu cliente não vai ficar tão chateado...\n",
    "    \n",
    " - mas se eu não reconheço o rosto da BFF na foto... aff! \n",
    "\n",
    "- probabilidade de um algoritmo identificar **Hugo Chávez**, dado que a pessoa de fato é Hugo Chávez\n",
    " \n",
    "- \"Quantos ítens relevantes foram selecionados?\" \n",
    " \n",
    "        Recall = Verdadeiros Positivos / Verdadeiros Positivos + Falsos Negativos\n",
    "        \n",
    "        Eram 52: identifiquei 20, acertei 12\n",
    "        \n",
    "        Recall = 12 / 12 + 32 = 3/8 \n",
    "\n",
    "*observe que este sistema não é muito **sensível**. Ele é um pouco lento para identificar rostos... ele está cometendo muitos erros do **Tipo II** - deveria identificar e não o faz*\n",
    "\n",
    "**Precision:**\n",
    "\n",
    "- isso é bom para um drone armado com um sniper - é melhor deixar passar do que pegar o cara errado!\n",
    "\n",
    "- dos caras que meu sistema identificou:\n",
    "\n",
    " - quantos eram realmente o cara que eu tinha como alvo?\n",
    "\n",
    "- supondo que o algoritmo reconheceu **Hugo Chávez**, qual a chance da pessoa ser de fato Hugo Chávez?\n",
    " \n",
    "- \"Quantos dos ítens selecionados são relevantes?\"\n",
    "\n",
    "        Precisão = Verdadeiros Positivos / Verdadeiros Positivos + Falsos Positivos\n",
    "\n",
    "        Eram 50: identifiquei 20, acertei 12\n",
    "\n",
    "        Precisão = 12 / 12 + 8 = 3/5 \n",
    "\n",
    "*observe que este sistema é razoavelmente **preciso**. Os rostos que ele pegou ele identificou a maioria... ele não está cometendo muitos erros do **Tipo I** - identifica quando não é*\n",
    "\n",
    "*bom, isso para um sistema de câmera de monitoramento... porque erros de **falso positivo** são considerados **gravíssimos**! Então não é bom brincar com eles! Sim, eu sempre vou querer um sistema bem preciso! Mas normalmente esses dois aspectos são conflitantes: ou é bastante sensível, ou é preciso do jeito que eu quero...*\n",
    "\n",
    "*mais um ponto, normalmente bons sistemas costumam ser **caros**. Envolvem ótica de precisão, sensores calibrados para produzir boas imagens, fontes de luz controladas... E na parte digital, algoritmos robustos e rápidos, grande quantidade de informação anterior coletada...*\n",
    "\n",
    "**Acuidade:**\n",
    "\n",
    "- minha maior preocupação é identificar corretamente o caso:\n",
    "\n",
    " - é ele, se o sujeito de fato é\n",
    "    \n",
    " - não é ele, se o sujeito de fato não é\n",
    " \n",
    "**Sensibilidade**\n",
    "\n",
    "- de um método reflete o quanto este é eficaz em identificar **corretamente**, dentre todos os indivíduos avaliados\n",
    "\n",
    "\n",
    "**Especificidade** \n",
    "\n",
    "- de um método reflete o quanto ele é eficaz em identificar corretamente os indivíduos que **não apresentam** a condição de interesse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ScikitLearn\n",
    "\n",
    "Sklearn:\n",
    "\n",
    "- Logistics [aqui](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "- Confusion Matrix [aqui](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "\n",
    "- Models Training [aqui](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "- ROC Curves [aqui](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py)\n",
    "\n",
    "A useful resource for exploring the rationale and process for splitting your data into train and test sets [aqui](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)\n",
    "\n",
    "Curvas ROC e AUC [aqui](https://community.alteryx.com/t5/Data-Science-Blog/ROC-Curves-in-Python-and-R/ba-p/138430)\n",
    "\n",
    "---\n",
    "\n",
    "In general, it is useful to split your data into:\n",
    "\n",
    "- training and\n",
    "\n",
    "- testing data \n",
    "\n",
    "to assure your model can predict well not only with the data it was fit to, but also on data that the model has never seen before\n",
    "\n",
    "Proving the model performs well on test data assures that you have a model that will do well in the future use cases - whether that be future students, future transactions, or any other future predictions you might want to make\n",
    "\n",
    "---\n",
    "\n",
    "Notas finais:\n",
    "\n",
    "- Estatística\n",
    "\n",
    " - determinar **relações** (relationships)\n",
    " \n",
    " - e entender os **mecanismos orientadores** (driving mechanisms)\n",
    " \n",
    " - as relações ocorreram por mero acaso?\n",
    " \n",
    "- Machine Learning\n",
    "\n",
    " - trabalha para predizer tão bem quanto o possível\n",
    " \n",
    " - algumas vezes sem ter atenção em por que as coisas foram bem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
