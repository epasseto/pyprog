{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "### Técnicas fundamentais\n",
    "\n",
    "- Dataset/questions\n",
    "\n",
    " - do you have enough data?\n",
    " \n",
    " - can I define a question?\n",
    " \n",
    " - enough/right features do answer questions?\n",
    "\n",
    "- Features\n",
    "\n",
    " - exploration\n",
    " \n",
    "   * inspect for interrelations\n",
    "  \n",
    "   * outlier removal\n",
    "  \n",
    "   * imputation\n",
    "  \n",
    "   * cleaning\n",
    "   \n",
    " - creation\n",
    " \n",
    "   * think about it like a human!\n",
    "   \n",
    " - representation\n",
    " \n",
    "   * text vectorization\n",
    "   \n",
    "   * discretization\n",
    "   \n",
    " - scaling\n",
    " \n",
    "   * meant substraction\n",
    "   \n",
    "   * minmay scalter\n",
    "   \n",
    "   * standard scalter\n",
    "   \n",
    " - selection\n",
    " \n",
    "   * kBest\n",
    "   \n",
    "   * percentile\n",
    "   \n",
    "   * recursive feature estim\n",
    "   \n",
    " - transforms\n",
    "  \n",
    "   * PCA\n",
    "    \n",
    "   * ICA\n",
    "    \n",
    "- Algorithms\n",
    "\n",
    " - tune your algorithm\n",
    " \n",
    "   * parameters of algorithm\n",
    "  \n",
    "   * visual inspection\n",
    "  \n",
    "   * performance on test data\n",
    "  \n",
    "   * GridSearch CV\n",
    "  \n",
    " - pick an algorithm - labeled data → supervised\n",
    " \n",
    "   * **non-ordered or discrete output**\n",
    "  \n",
    "     * decision tree\n",
    "   \n",
    "     * naive Bayes\n",
    "   \n",
    "     * erscranbles\n",
    "   \n",
    "     * k nearest heighness\n",
    "   \n",
    "     * LDA\n",
    "   \n",
    "     * Logistic regression\n",
    "   \n",
    "   * **ordered or continous output**\n",
    "  \n",
    "     * linear regression\n",
    "   \n",
    "     * lasso regression\n",
    "   \n",
    "     * decision tree regression\n",
    "   \n",
    "     * SV regression\n",
    "   \n",
    " - pick an algorithm - unlabeled data → unsupervised\n",
    "   \n",
    "   * k-means clustering\n",
    "  \n",
    "   * spectral clustering\n",
    "  \n",
    "   * PCA\n",
    "  \n",
    "   * mixture models/EM algorithm\n",
    "\n",
    "- Evaluation\n",
    "\n",
    " - validate\n",
    " \n",
    "   * train/test split\n",
    "  \n",
    "   * k-field\n",
    "  \n",
    "   * visualize\n",
    " \n",
    " - pick metric(s)\n",
    " \n",
    "   * SSE/r^2\n",
    "  \n",
    "   * precision\n",
    "  \n",
    "   * recall\n",
    "  \n",
    "   * F1 score\n",
    "  \n",
    "   * ROC curve\n",
    "  \n",
    "   * custom\n",
    "  \n",
    "   * wide/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes\n",
    "\n",
    "---\n",
    "\n",
    "Classificação de animais mochos (eu errei, pois a palavra usada não era mocho e achei que o cavalo ficaria muito bem no grupo da girafa, afinal de contas são parentes...). Com mil animais eu ficaria melhor para classificar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Classification\n",
    "\n",
    "- reconhecer uma pessoa em um álbum de fotografias\n",
    "\n",
    "- a partir da escolha musical, recomendar uma nova música a esta pessoa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feições (features) e rótulos (labels)\n",
    "\n",
    "Queremos escolher uma música para Katie\n",
    "\n",
    "Podemos criar algumas feições:\n",
    "\n",
    "- gênero\n",
    "\n",
    "- duração\n",
    "\n",
    "- estilo\n",
    "\n",
    "- voz\n",
    "\n",
    "Meu classificador irá coletar feições e processar como:\n",
    "\n",
    "- gostei\n",
    "\n",
    "- não gostei\n",
    "\n",
    "Eu jogo minhas feições num gráfico de pontos, duas a duas (gênero x duração). Para fazer isso, eu crio uma nota, tipo 0 a 10 para cada uma das minhas feições\n",
    "\n",
    "Eu repito esse processo para várias músicas\n",
    "\n",
    "E então para algumas pessoas, aparece um padrão claro, do tipo de feições que as pessoas apreciam\n",
    "\n",
    "É uma resposta ternária:\n",
    "\n",
    "- gosto\n",
    "\n",
    "- não gosto\n",
    "\n",
    "- ainda não está claro\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stanley terrain classification\n",
    "\n",
    "- o terreno é muito inclinado? \n",
    "\n",
    " - grau de 0 a 100\n",
    " \n",
    " - para cima ou para baixo\n",
    " \n",
    " - simétrico ou mais para esquerda/direita\n",
    "\n",
    "- o terreno é muito acidentado?\n",
    "\n",
    " - densidade dos acidentes (rugas por segundo)\n",
    " \n",
    " - grandes ou pequenas rugas\n",
    " \n",
    " - a suspensão ainda está ativa (lidando com a ruga anterior) quando surge a nova ou não\n",
    " \n",
    " - a direção do veículo está reta ou apontando para um lado (quantos graus)\n",
    " \n",
    "*eu preciso dizer para o Stanley, baseado nestas duas variáveis (e que podem ser realmente complexas) se ele deve pisar mais no acelerador, ou tentar freiar*\n",
    " \n",
    "Gráfico de pontos: inclinação x rugosidade\n",
    " \n",
    "O que o supervised learning vai criando é uma **superfície de decisão**, isolando os pontos mais familiares. Quanto mais pontos eu tiver, maior minha experiência e menor minha incerteza!\n",
    " \n",
    "Para aproveitar a capacidade de processamento vetorial atual de algumas máquinas, eu posso criar **superfície de decisão linear**, separando os pontos a partir de uma reta, ou mais de uma reta\n",
    "\n",
    "dados → superfície de decisão\n",
    "\n",
    "*pontos muito próximos à superfície de decisão podem apresentar problemas!*\n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naïve Bayes\n",
    "\n",
    "    sckikit-learn ou sk-learn (biblioteca)\n",
    "    \n",
    "    sklearn.naive_bates.GaussianNB\n",
    "    \n",
    "    GaussianNB() cria um classificador\n",
    "    \n",
    "    .fit(X, Y) (feições, rótulos) aqui o modelo aprende com os dados de entrada e cria seu padrão\n",
    "    \n",
    "    .predict([[0, -1]]) prevê qual seria o rótulo para este novo ponto dado\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "#import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Mini project - Naïve Bayes\n",
    "\n",
    "Use a Naive Bayes Classifier to identify emails by their authors\n",
    "    \n",
    "authors and labels:\n",
    "\n",
    "- Sara has label 0\n",
    "\n",
    "- Chris has label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-5e14bf133045>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-5e14bf133045>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    @@ -41,7 +41,7 @@\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print (\"Enron dataset should be last item on the list, along with its current size\")\n",
    "print (\"download will complete at about 423 MB\")\n",
    "import urllib\n",
    "url = \"https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tgz\"\n",
    "urllib.urlretrieve(url, filename=\"../enron_mail_20150507.tgz\") \n",
    "url = \"https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tar.gz\"\n",
    "urllib.urlretrieve(url, filename=\"../enron_mail_20150507.tar.gz\") \n",
    "print (\"download complete!)\"\n",
    "\n",
    "\n",
    "@@ -41,7 +41,7 @@\n",
    "import tarfile\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "tfile = tarfile.open(\"enron_mail_20150507.tgz\", \"r:gz\")\n",
    "tfile = tarfile.open(\"enron_mail_20150507.tar.gz\", \"r:gz\")\n",
    "tfile.extractall(\".\")\n",
    "\n",
    "print (\"you're ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'email_preprocess'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c6a4b2f07cc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../tools/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0memail_preprocess\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'email_preprocess'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GaussianNB Deployment on Terrain Data\n",
    "\n",
    "---\n",
    "\n",
    "studentMain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Complete the code in ClassifyNB.py with the sklearn\n",
    "    Naive Bayes classifier to classify the terrain data.\n",
    "    \n",
    "    The objective of this exercise is to recreate the decision \n",
    "    boundary found in the lesson video, and make a plot that\n",
    "    visually shows the decision boundary \"\"\"\n",
    "\n",
    "from prep_terrain_data import makeTerrainData\n",
    "from class_vis import prettyPicture, output_image\n",
    "from ClassifyNB import classify\n",
    "\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "### the training data (features_train, labels_train) have both \"fast\" and \"slow\" points mixed\n",
    "### in together--separate them so we can give them different colors in the scatterplot,\n",
    "### and visually identify them\n",
    "grade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "\n",
    "# You will need to complete this function imported from the ClassifyNB script.\n",
    "# Be sure to change to that code tab to complete this quiz.\n",
    "clf = classify(features_train, labels_train)\n",
    "\n",
    "### draw the decision boundary with the text points overlaid\n",
    "prettyPicture(clf, features_test, labels_test)\n",
    "output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_vis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Complete the code in ClassifyNB.py with the sklearn\n",
    "    Naive Bayes classifier to classify the terrain data.\n",
    "    \n",
    "    The objective of this exercise is to recreate the decision \n",
    "    boundary found in the lesson video, and make a plot that\n",
    "    visually shows the decision boundary \"\"\"\n",
    "\n",
    "from prep_terrain_data import makeTerrainData\n",
    "from class_vis import prettyPicture, output_image\n",
    "from ClassifyNB import classify\n",
    "\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "### the training data (features_train, labels_train) have both \"fast\" and \"slow\" points mixed\n",
    "### in together--separate them so we can give them different colors in the scatterplot,\n",
    "### and visually identify them\n",
    "grade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "\n",
    "# You will need to complete this function imported from the ClassifyNB script.\n",
    "# Be sure to change to that code tab to complete this quiz.\n",
    "clf = classify(features_train, labels_train)\n",
    "\n",
    "### draw the decision boundary with the text points overlaid\n",
    "prettyPicture(clf, features_test, labels_test)\n",
    "output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prep_terrain_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import random\n",
    "\n",
    "def makeTerrainData(n_points=1000):\n",
    "###############################################################################\n",
    "### make the toy dataset\n",
    "    random.seed(42)\n",
    "    grade = [random.random() for ii in range(0,n_points)]\n",
    "    bumpy = [random.random() for ii in range(0,n_points)]\n",
    "    error = [random.random() for ii in range(0,n_points)]\n",
    "    y = [round(grade[ii]*bumpy[ii]+0.3+0.1*error[ii]) for ii in range(0,n_points)]\n",
    "    for ii in range(0, len(y)):\n",
    "        if grade[ii]>0.8 or bumpy[ii]>0.8:\n",
    "            y[ii] = 1.0\n",
    "\n",
    "### split into train/test sets\n",
    "    X = [[gg, ss] for gg, ss in zip(grade, bumpy)]\n",
    "    split = int(0.75*n_points)\n",
    "    X_train = X[0:split]\n",
    "    X_test  = X[split:]\n",
    "    y_train = y[0:split]\n",
    "    y_test  = y[split:]\n",
    "\n",
    "    grade_sig = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==0]\n",
    "    bumpy_sig = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==0]\n",
    "    grade_bkg = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==1]\n",
    "    bumpy_bkg = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==1]\n",
    "\n",
    "#    training_data = {\"fast\":{\"grade\":grade_sig, \"bumpiness\":bumpy_sig}\n",
    "#            , \"slow\":{\"grade\":grade_bkg, \"bumpiness\":bumpy_bkg}}\n",
    "\n",
    "    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "\n",
    "    test_data = {\"fast\":{\"grade\":grade_sig, \"bumpiness\":bumpy_sig}\n",
    "            , \"slow\":{\"grade\":grade_bkg, \"bumpiness\":bumpy_bkg}}\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "#    return training_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ClassifyNB.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(features_train, labels_train):   \n",
    "  from ssklearn.naive_bayes import GaussianNB ### import the sklearn module for GaussianNB\n",
    "  clf = GaussianNB() ### create classifier\n",
    "  return clf.fit(features_train, labels_train) ### fit the classifier on the training features and label\n",
    "                                               ### return the fit classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the ClassifyNB.py script that you need to update for the quiz, you can click on the dropdown in the classroom code editor to get a list of files that will be used\n",
    "\n",
    "In the quiz that follows, the line that reads \n",
    "\n",
    "    pred = clf.predict(features_test) \n",
    "\n",
    "is not necessary for drawing the decision boundary, at least as we've written the code.\n",
    "\n",
    "However, the whole point of making a classifier is that you can make predictions with it, so be sure to keep it in mind since you'll be using it in the quiz after this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating NB Accuracy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NBAccuracy(features_train, labels_train, features_test, labels_test):\n",
    "    \"\"\" compute the accuracy of your Naive Bayes classifier \"\"\"\n",
    "    ### import the sklearn module for GaussianNB\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    # print accuracy_score(pred, labels_test)\n",
    "\n",
    "    ### create classifier\n",
    "    clf = GaussianNB()\n",
    "\n",
    "    ### fit the classifier on the training features and labels\n",
    "    fit = clf.fit(features_train, labels_train)\n",
    "\n",
    "    ### use the trained classifier to predict labels for the test features\n",
    "    pred = clf.predict(features_test)\n",
    "\n",
    "    ### calculate and return the accuracy on the test data\n",
    "    ### this is slightly different than the example, \n",
    "    ### where we just print the accuracy\n",
    "    ### you might need to import an sklearn module\n",
    "    accuracy = accuracy_score(pred, labels_test) #pega a predição, pega elemento por elemento para fazer a comparação\n",
    "    return accuracy\n",
    "    \n",
    "clf = GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "studentCode.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from class_vis import prettyPicture\n",
    "from prep_terrain_data import makeTerrainData\n",
    "from classify import NBAccuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "def submitAccuracy():\n",
    "    accuracy = NBAccuracy(features_train, labels_train, features_test, labels_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we print the accuracy. However, the quiz is slightly different in that you return the accuracy, rather than printing it directly\n",
    "\n",
    "Remember, accuracy is defined as the number of test points that are classified correctly divided by the total number of test points\n",
    "\n",
    "There's another way you can do this, too:\n",
    "\n",
    "    print clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em Machine Learning é normal se **treinar** em um conjunto de dados e se **testar** em outro conjunto de dados!\n",
    "\n",
    "*isso traz **generalização** ao nosso modelo!*\n",
    "\n",
    "*normalmente eu separo 10% dos meus dados para fazer o teste posteriormente*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regra de Bayes\n",
    "\n",
    "---\n",
    "**Sensibilidade** ocorre quando o alarme dispara, quando deveria disparar (ex: 90% dos casos) - P(pos|c) = 0,9 (90%)\n",
    "\n",
    "**Especificidade** ocorre quando o alarme não soa, quando não deveria soar (ex: 90% dos casos) - P(neg|^c) = 0,9 (90%)\n",
    "\n",
    "**Prior probability** é a probabilidade que eu tenho antes de rodar o teste\n",
    "\n",
    "    + alguma evidência (que é o teste em si mesmo)\n",
    "    \n",
    "nos leva a uma **Posterior probability**\n",
    "\n",
    "    Prip x ev -> Posp\n",
    "    \n",
    "    Prior P(c) = 0,01 (1%)\n",
    "          P(pos|c) = 0,9 (90%)\n",
    "          P(neg|^c) = 0,9 (90%)\n",
    "    \n",
    "    \n",
    "    Joint P(c|pos) = P(c) x P(pos|c) = 0,01 x 0,9 = 0,009\n",
    "          P(^c|pos) = P(^c) x P(pos|^c) = 0,99 x 0,1 = 0,099\n",
    "              \n",
    "    Normalizes A soma das duas áreas dá 0,108 (mas eu quero isso em escala estatística, ou seja, 1)\n",
    "    Isso também significa que a P(pos) = 0,108 (ou seja, esta área é o domínio dos testes positivos!)\n",
    "    \n",
    "    Posterior P(c|pos) = 0.0833 (perto de 8%)\n",
    "    \n",
    "    Em diagrama:\n",
    "    \n",
    "    P(c) -> mult P(pos|c) -> (ramo do câncer) P(pos,c) -> normalizado r1 P(c|pos)\n",
    "    \n",
    "         -> mult P(pos|^c) -> (ramo do não câncer) P(pos,^c) -> normalizado r2 P(pos|^c)\n",
    "         \n",
    "    Agora eu simplesmente somo os dois ramos e divido, para normalizar a estatística para 1, e resolvido!\n",
    "    \n",
    "**Prior probability** x **Test evidence** -> **Posterior probability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08333333333333333"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.009/0.108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666667"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.099/0.108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text learning\n",
    "\n",
    "A probabilidade de Cris ou de Sara enviarem um e-mail é de 50% - P(Cris) = 0.5\n",
    "\n",
    "Duas pessoas:\n",
    "\n",
    "- Cris é mais reservada e voltada a negócios\n",
    "\n",
    " - P(Love|Cris) = 0.1\n",
    " \n",
    " - P(Deal|Cris) = 0.8\n",
    " \n",
    " - P(Life|Cris) = 0.1\n",
    " \n",
    "- Sara é extrovertida e adora curtir a vida\n",
    "\n",
    " - P(Love|Sara) = 0.5\n",
    " \n",
    " - P(Deal|Sara) = 0.2\n",
    " \n",
    " - P(Life|Sara) = 0.3\n",
    " \n",
    " Recebemos um e-mail com as seguintes palavras: \"Life Deal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrar que uma das regras básicas da Estatística é que probabilidades de eventos independentes se **multiplicam** e não se **somam**!\n",
    "\n",
    "---\n",
    "\n",
    "Assim, o exemplo clássico: existe a chance de 20% de chover num dia de verão. Estamos indo para o quinto dia sem chuva e o sujeito anuncia na rádio: \"hoje cidadãos, tem 100% de probabilidade de chover!\n",
    "\n",
    "Nossa intuição já nos mostra que isso é uma falácia. A segunda vez que eu lançasse uma moeda não viciada e já tivesse dado Cara, um deus desceria do Olimpo e faria dar Coroa!\n",
    "\n",
    "O que eu tenho que ir na rádio e dizer é: \"hoje, cidadãos, a probabilidade de chover é de 20%!\".\n",
    "\n",
    "E se os eventos se repetirem. Ou seja, qual a probabilidade de chover ao menos um dia, nestes quatro de verão?\n",
    "\n",
    "Neste caso eu vou fazendo as combinações dos fenômenos acontecerem, dia a dia. Eu também poderia fazer a pergunta: \"qual a probabilidade chover exatamente um dia, nesses quatro de verão?\"\n",
    "\n",
    "Se tiver dificuldade com esses conceitos, é legal passar na Academia Khan. Há muitos vídeos explicando essa e muitas outras coisas de Estatística!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Love deal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jcris = 0.1*0.8*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jsara = 0.5*0.2*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444445"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pcris = Jcris / (Jcris+Jsara)\n",
    "Pcris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555555555555555"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Psara = Jsara / (Jcris+Jsara)\n",
    "Psara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Um caso não naïve\n",
    "\n",
    "Duas fontes:\n",
    "\n",
    "- Emissor oculto 1:\n",
    "\n",
    " - P(Love|E1) = e1love\n",
    " \n",
    " - P(Deal|E1) = e1deal\n",
    " \n",
    " - P(Life|E1) = e1life\n",
    " \n",
    "- Emissor oculto 2:\n",
    "\n",
    " - P(Love|E2) = e2love\n",
    " \n",
    " - P(Deal|E2) = e2deal\n",
    " \n",
    " - P(Life|E2) = e2life\n",
    " \n",
    " Cada palavra me dá evidências de que se trata do E1 ou de E2\n",
    " \n",
    " OK, mas de fato ele não está **interpretando** o texto! Então a **ordem** das palavras não conta para nada! É como se pegássemos um e-mail, quebrássemos em palavras e colocássemos no saco. Tirando uma a uma, elas vão remontando mais a E1 ou E2. Um interpretador de textos exige realmente muito mais esforço computacional. Mas Naïve Bayes é usado para um bocado de coisas, então já pode ser usado como uma ferramenta útil!\n",
    " \n",
    " Por exemplo se o mecanismo Google fosse baseado em Naïve Bayes e eu entrasse as palavras **Chicago Bulls** (um time de futebol americano) ele iria me resultar em dezenas de respostas para a cidade, para bois e talvez para alguns eventos envolvendo gado e a cidade!\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini projeto [aqui](https://github.com/udacity/ud120-projects)\n",
    "\n",
    "Eu recebo strings de textos, resultado de um pré processamento sobre e-mails de dois funcionários. Eu os corto em palavras e as armazeno em listas. A partir da frequência das palavras recebidas, eu quero saber se eu atribuo o e-mail ao funcionário A ou ao funcionário B\n",
    "\n",
    "Uma parte dos dados eu uso para treinar minha máquina virtual e outra parte, para testá-la\n",
    "\n",
    "- Use Python 2.7\n",
    "\n",
    "- install sklearn: pip install scikit-learn\n",
    "\n",
    "- install natural language toolkit: pip install nltk\n",
    "\n",
    "- Get the Intro to Machine Learning source code. You will need git to clone the repository: git clone https://github.com/udacity/ud120-projects.git\n",
    "\n",
    "You only have to do this once, the code base contains the starter code for all the mini-projects. Go into the tools/ directory, and run startup.py. It will first check for the python modules, then download and unzip a large dataset that we will use heavily later. The download/unzipping can take a while, but you don’t have to wait for it to finish in order to get started on Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 1 (Naive Bayes) mini-project. \n",
    "\n",
    "    Use a Naive Bayes Classifier to identify emails by their authors\n",
    "\n",
    "    authors and labels:\n",
    "\n",
    "    Sara has label 0\n",
    "\n",
    "    Chris has label 1\n",
    "\n",
    "\"\"\"\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and train a Naive Bayes classifier in naive_bayes/nb_author_id.py. Use it to make predictions for the test set. What is the accuracy?\n",
    "\n",
    "When training you may see the following error: UserWarning: Duplicate scores. Result may depend on feature ordering.There are probably duplicate features, or you used a classification score for a regression task. warn(\"Duplicate scores. Result may depend on feature ordering.\")\n",
    "\n",
    "This is a warning that two or more words happen to have the same usage patterns in the emails--as far as the algorithm is concerned, this means that two features are the same. Some algorithms will actually break (mathematically won’t work) or give multiple different answers (depending on feature ordering) when there are duplicate features and sklearn is giving us a warning. Good information, but not something we have to worry about\n",
    "\n",
    "*Some students have encountered memory problems when executing the code for this problem. To reduce the chance of seeing a memory error while running the code, we recommend that you use a computer with at least 2GB of RAM. If you find that the code is causing memory errors, you can also try setting test_size = 0.5 in the email_preprocess.py file*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important topic that we didn’t explicitly talk about is the time to train and test our algorithms. Put in two lines of code, above and below the line fitting your classifier, like this:\n",
    "\n",
    "t0 = time()\n",
    "< your clf.fit() line of code >\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "Put similar lines of code around the clf.predict() line of code, so you can compare the time to train the classifier and to make predictions with it. What is faster, training or prediction?\n",
    "\n",
    "We will compare the Naive Bayes timing to a couple other algorithms, so note down the speed and accuracy you get and we’ll revisit this in the next mini-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "---\n",
    "\n",
    "*encontra um hiperplano entre dados de duas classes*\n",
    "\n",
    "*o que a \"linha\" (em 2D o hiperplano se torna uma linha!) faz é maximizar a distância (margin) a seus pontos mais próximos (em ambos os lados)*\n",
    "\n",
    "*com isso ele maximiza a **robustez** do meu modelo*\n",
    "\n",
    "Passos:\n",
    "\n",
    "1- lidar com **outliers** \n",
    "\n",
    "2- classificar **corretamente** o máximo de pontos possíveis\n",
    "\n",
    "3- maximizar a **margem**\n",
    "\n",
    "---\n",
    "\n",
    "#### Decision boundary (draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from class_vis import prettyPicture\n",
    "from prep_terrain_data import makeTerrainData\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "########################## SVM #################################\n",
    "### we handle the import statement and SVC creation for you here\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel=\"linear\")\n",
    "clf.fit( features_train, labels_train )\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(pred, labels_test)\n",
    "\n",
    "def submitAccuracy():\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado foi: 0.92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear SVMs\n",
    "\n",
    "Nova feição:\n",
    "    \n",
    "- antes - $X$ e $Y$ entram na caixa preta do SVM e ela nos retorna um rótulo (label)\n",
    "    \n",
    "- agora - $X$, $Y$ e $Z = X^2 + Y^2$ (sempre positivo) entram na caixa preta (agora 3D)\n",
    "\n",
    "E isso **sim** é linearmente separável!\n",
    "\n",
    "- medidas de distância no terceiro plano **polar** começam a separar objetos próximos e distantes em referência ao centro\n",
    "\n",
    "- então os objetos, no plano **X-Z** ou no plano **Y-Z** se tornaram linearmente separáveis!\n",
    "\n",
    "Outra feição:\n",
    "\n",
    "- $|X|$ - às vezes criando uma dimensão espelhada, as coisas se tornam separáveis!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Trick [aqui](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html)\n",
    "\n",
    "- $X , Y$ (non-separable) -kernels-> $X1 X2 X3 X4 X5$ (separable)\n",
    "\n",
    "- (non-linear separation) <---------  (solution)\n",
    "\n",
    "Parâmetros:\n",
    "\n",
    "- É o que eu passo à máquina **antes** de criar o classificador. Em SVM:\n",
    "\n",
    " - Kernel\n",
    " \n",
    " - C (penalidade para o erro ) - controls tradeoff between **smooth decision boundary** and **classifying training points correctly**\n",
    " \n",
    " - Gamma (quantos dos pontos mais próximos são levado em consideração) - defines how far the influence of a single training example reaches \n",
    " \n",
    "#### Overfitting\n",
    "\n",
    "- don´t get your data **too literary**\n",
    "\n",
    "- controle seus três fatores:\n",
    "\n",
    " - Kernel (a não ser que **realmente** seja necessário, produza resultados com o Kernel **mais simples**)\n",
    " \n",
    " - C (a não ser que seja estritamente necessário **não cometer erros**, deixe a penalidade **reduzida**)\n",
    " \n",
    " - Gamma (não **exagere** no número de pontos envolvidos na otimização!)\n",
    " \n",
    "*o que normalmente compromete a viabilidade do SVM é o **esforço computacional** - especialmente adicionando núcleos polinomiais, o esforço evolui geometricamente!* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-projeto 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:    \n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees [aqui](https://scikit-learn.org/stable/modules/tree.html)\n",
    "\n",
    "---\n",
    "\n",
    "- truque simples para transformar problemas **não lineares** de decisão em **lineares**\n",
    "\n",
    "Windsurf:\n",
    "\n",
    "- eu só pratico quando tem **muito** sol e quando está ventando **muito**\n",
    "\n",
    "- esses dados **brutos** não são separáveis linearmente no gráfico!\n",
    "\n",
    "- os separo em uma **árvore de decisões**\n",
    "\n",
    " - está ventando?\n",
    " \n",
    "   - **não** e meu problema acabou (não tenho para onde seguir)\n",
    "   \n",
    "   - **sim** e então... está fazendo sol?\n",
    "   \n",
    "     - **não** e meu problema acabou\n",
    "    \n",
    "     - **sim** e então eu vou praticar windsurf!\n",
    "\n",
    "Cortes com uma tesoura:\n",
    "\n",
    "- o primeiro corte ($X<3$) pode não purificar totalmente meus As de Bs\n",
    "\n",
    " - o pedaço 1 contém agora mais As do que Bs, mas cortando em ($Y<2$)\n",
    " \n",
    "   - a fatia 11 contém apenas As\n",
    "   \n",
    "   - a fatia 12 contém apenas Bs\n",
    " \n",
    " - o pedaço 2 contém mais As do que Bs, mas em outra configuração, mas cortando em ($Y<4$)\n",
    " \n",
    "  - a fatia 21 contém apenas As\n",
    "  \n",
    "  - a fatia 22 contém apenas Bs\n",
    "  \n",
    "*observe que eu poderia cortar em qualquer direção $X$ ou $Y$ e isso poderia resultar num algoritmo super rápido de classificação*\n",
    "\n",
    "*eu ainda poderia cortar em **planos inclinados** e com um pouco mais de esforço computacional, o meu algoritmo ainda poderia ser bastante eficiente* \n",
    " \n",
    " \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from class_vis import prettyPicture\n",
    "from prep_terrain_data import makeTerrainData\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classify() function in classifyDT is where the magic happens-fill in this function in the file 'classifyDT.py'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = classify(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyPicture(clf, features_test, labels_test)\n",
    "output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(features_train, labels_train):\n",
    "    ### your code goes here--should return a trained decision tree classifer\n",
    "    from sklearn import tree\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(features_train, labels_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "acc= clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submitAccuracies():\n",
    "  return {\"acc\":round(acc,3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método **.min_samples_split()**:\n",
    "    \n",
    "- a árvore vai criando ramos a cada iteração:\n",
    "\n",
    "        100 -> 60 -> 25\n",
    "\n",
    "                  -> 15\n",
    "\n",
    "            -> 40 -> 35\n",
    "          \n",
    "                  ->  5 ...\n",
    "        \n",
    "*até onde eu quero continuar **ramificando**?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracies diferentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = tree.DecisionTreeClassifier(min_samples_split=2)\n",
    "clf2 = clf2.fit(features_train, labels_train)\n",
    "\n",
    "clf50 = tree.DecisionTreeClassifier(min_samples_split=50)\n",
    "clf50 = clf50.fit(features_train, labels_train)\n",
    "\n",
    "acc_min_samples_split_2 = clf2.score(features_test, labels_test)\n",
    "acc_min_samples_split_50 = clf50.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eu refino minha árvore e o resultado da acuidade **piorou**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submitAccuracies():\n",
    "  return {\"acc_min_samples_split_2\":round(acc_min_samples_split_2,3),\n",
    "          \"acc_min_samples_split_50\":round(acc_min_samples_split_50,3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Impureza dos dados e Entropia\n",
    "\n",
    "$H = -\\sum_{i} p_{i}\\log_{2} (p_{i})$\n",
    "\n",
    "$(p_{i})$ $\\rightarrow$ a fração de eventos na classe $i$\n",
    "\n",
    "- how a decision tree (DT) decides where to split the data\n",
    "\n",
    "- é a medida de impureza em um conjunto de eventos\n",
    "\n",
    "Minimizando a impureza - caso do limite de velocidade:\n",
    "\n",
    "$H = \\frac{1}{Impureza}$\n",
    "\n",
    "- dados produzidos pelo limite tendem a ser **puros**\n",
    "\n",
    "- minimizando a **Entropia** eu garanto que minhas divisões foram as mais precisas possíveis\n",
    "\n",
    "Quatro variáveis:\n",
    "\n",
    "- grade\n",
    "\n",
    " - steep\n",
    " \n",
    " - flat\n",
    " \n",
    "- bumpiness\n",
    "\n",
    " - bumpy\n",
    "\n",
    " - smooth\n",
    " \n",
    "- speed limit\n",
    "\n",
    " - yes\n",
    " \n",
    " - no\n",
    " \n",
    "- speed\n",
    "\n",
    " - slow\n",
    " \n",
    " - fast\n",
    " \n",
    "Condição:\n",
    "\n",
    " - ssff\n",
    " \n",
    "  - 2 de slow, 4 no total $\\rightarrow$ 0.5\n",
    "  \n",
    "  - 2 de fast $\\rightarrow$ 0.5\n",
    "  \n",
    "  - H=1 (estado mais impuro possível)\n",
    "  \n",
    "#### Information Gain\n",
    "\n",
    "$Information gain = entropy(parent) - [weighted average] entropy(children)$\n",
    "\n",
    "$I(parent,children)= H(parent) - H(parent|children)$\n",
    "\n",
    "*objetivo: maximizar o ganho de informação*\n",
    " \n",
    "Faço uma árvore de decisão sobre **inclinação do terreno**, mas colho dados sobre **velocidade**\n",
    "\n",
    "- então eu saí de slow/fast ssff (H=1) e orientado por slope/flat, obtenho ssf e f (H=0)\n",
    "\n",
    " - em ssf $\\rightarrow$\n",
    "\n",
    "   - P(s) = 2/3\n",
    " \n",
    "   - P(f) = 1/3\n",
    "   \n",
    " - resulto em $Igain = 0.3112$\n",
    " \n",
    "- a mesma coisa para bumpy/smooth, saindo de bsbs (H=1) e dividindo por slow/fast, eu obtenho bs (H=1) e bs (H=1)\n",
    "\n",
    " - jogo de soma zero, eu resulto em $Igain = 0$\n",
    " \n",
    "- em speed limit, saindo de ssff (H=1), dividindo por slow/fast eu obtenho yy (H=0) e nn (H=1)\n",
    "\n",
    " - agora eu resulto em $Igain = 1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropia de um elemento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "p = -0.5 * math.log2(0.5)\n",
    "Hparent = 2 * p\n",
    "Hparent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = -(2/3) * math.log2(2/3) -(1/3) * math.log2(1/3)\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6887218755408672"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hchild = (3/4)*H + (1/4)*0.\n",
    "Hchild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31127812445913283"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Igain = Hparent - Hchild\n",
    "Igain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn.tree.DecisionTreeClassifier() method\n",
    "\n",
    "- criterion='gini' (default) $\\leftarrow$ Gini information impurity and Entropy for the information gain\n",
    "\n",
    "*os critérios do sklearn são ligeiramente diferentes, mas o resultado obtido é muito similar*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Bias/Variance Dilemma\n",
    "\n",
    "- **extremo 1** - alto Bias $\\leftarrow$ praticamente irá **ignorar** seus dados (incapaz de aprender qualquer coisa)\n",
    "\n",
    "- **extremo 2** - extremamente receptivo aos dados irá apenas **replicar** o que já foi visto (terá reações muito ruins frente a novas situações)\n",
    "\n",
    "- devemos calibrar entre maior ou menor **bias** e **variância**\n",
    "\n",
    "*cuidado com **overfitting** em Decision Trees!*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini projeto em Decision Trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 3 (decision tree) mini-project.\n",
    "    Use a Decision Tree to identify emails from the Enron corpus by author:    \n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "\n",
    "#########################################################\n",
    "### your code goes here ###\n",
    "\n",
    "\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Mini Project\n",
    "\n",
    "In this project, we will again try to classify emails, this time using a decision tree.   The starter code is in decision_tree/dt_author_id.py.\n",
    "\n",
    "#### Part 1: Get the Decision Tree Running\n",
    "\n",
    "Get the decision tree up and running as a classifier, setting min_samples_split=40.  It will probably take a while to train.  What’s the accuracy?\n",
    "\n",
    "#### Part 2: Speed It Up\n",
    "\n",
    "You found in the SVM mini-project that the parameter tune can significantly speed up the training time of a machine learning algorithm.  A general rule is that the parameters can tune the complexity of the algorithm, with more complex algorithms generally running more slowly\n",
    "\n",
    "Another way to control the complexity of an algorithm is via the number of features that you use in training/testing.  The more features the algorithm has available, the more potential there is for a complex fit.  We will explore this in detail in the “Feature Selection” lesson, but you’ll get a sneak preview now\n",
    "\n",
    "find the number of features in your data.  The data is organized into a numpy array where the number of rows is the number of data points and the number of columns is the number of features; so to extract this number, use a line of code like \n",
    "\n",
    "    len(features_train[0])\n",
    "\n",
    "go into tools/email_preprocess.py, and find the line of code that looks like this:\n",
    "\n",
    "    selector = SelectPercentile(f_classif, percentile=1)  Change percentile from 10 to 1.\n",
    "\n",
    "- What’s the number of features now?\n",
    "\n",
    "- What do you think SelectPercentile is doing?  Would a large value for percentile lead to a more complex or less complex \n",
    "decision tree, all other things being equal?\n",
    "\n",
    "- Note the difference in training time depending on the number of features.  \n",
    "\n",
    "- What’s the accuracy when percentile = 1?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose your algorithm\n",
    "\n",
    "CYOA - Choose Your Own Adventure \n",
    "\n",
    "---\n",
    "\n",
    "Supervised classification\n",
    "\n",
    "- classic, simple, easy to understand:\n",
    "\n",
    " - k nearest neighbors\n",
    " \n",
    "  1. clusterize seus dados (training data - usei PCA)\n",
    "  \n",
    "  2. adicione o elemento ao seu gráfico (new data/unknown classification - ou heatmaps)\n",
    "  \n",
    "  3. classifique o novo elemento, segundo a proximidade com os elementos dos grupos\n",
    "  \n",
    "  4. se $k_{n} = 1$, então nós usamos apenas aquele grupo para definir o elemento (valores pequenos de k podem conter ruídos/oultiers)\n",
    "  \n",
    "  5. se $k_{n} = 11$, então usaremos 11 elementos de grupos diferentes e classificaremos segundo o mais votado (não exagere, ou haverão votos de outras categorias!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees:\n",
    "\n",
    "- ensemble methods - **meta classifiers**, built from (usually) decision trees (uma espécie de **votação**, no qual um sai presidente)\n",
    "\n",
    "- (\"Elements of Statistical Learning\" - DT prevents them from being the ideal tool for predictive learning $\\rightarrow$ **innacuracy**\n",
    " \n",
    "- são ótimas para os dados que as criaram, mas não funcionam bem para classificar **novas amostras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest** (simplicidade das DTs + flexibilidade $\\rightarrow$ ganhos em **accuracy**)\n",
    " \n",
    "  1. crie um dataset **bootstrap** (using the aggregate to make a decision is called **b´agging**)\n",
    "  \n",
    "  2. crie uma **decision tree** a partir do bootstrap dataset. Mas use apenas um subconjunto aleatório de variáveis para cada passo (mais tarde saberemos como saber qual o número de variáveis escolher)\n",
    "  \n",
    "  3. por exemplo, de 4 colunas, escolhemos aleatoriamente apenas 2. Como na raiz, escolhemos mais 2 variáveis aleatoriamente como a segunda bifurcação, até esgotarem as colunas. Árvore criada!\n",
    "  \n",
    "  4. agora crie uma **floresta** dessas árvores (normalmente **centenas** delas), totalmente aleatórias\n",
    "  \n",
    "  5. com um novo dado, eu rodo as árvores individualmente. O resultado que obtiver maior número de votos será o selecionado!\n",
    "  \n",
    "  6. tipicamente 1/3 dos dados acabam não entrando no dataset **bootstrap** (chamado de **out of bag dataset**). Eu tenho um número chamado **out of bag error** (os classificados errado). Estima a **accuracy** da minha **random forest**\n",
    "  \n",
    "  7. depois nós mudanos o número de variáveis por passo (ponto inicial: a raiz quadrada do número de variáveis)\n",
    "  \n",
    "Random forests - missing data:\n",
    "\n",
    " 1. se o dado estiver no grupo de **origem**, preencha-o provisoriamente com o valor da **mediana** para aquela coluna\n",
    " \n",
    " 2. determinamos quais **amostras** são similares ao dado ausente. Rodando a **random forest**, aquelas árvores que terminarem na mesma **folha** podem ser consideradas **similares** \n",
    " \n",
    " 3. nós podemos continuar rastreando **amostras similares** usando uma **Proximity Matrix**. Quando duas amostras terminam na mesma folha, eu coloco um **1** na conjunção. Isso irá somando números inteiros. Ao final, eu divido os valores da matriz de proximidades pelo número de árvores\n",
    "\n",
    "*um número igual a 1 significa que esses dados são o mais próximos possíveis!*\n",
    "\n",
    "*com 1-... e esses valores e você cria a **matriz de distâncias** e com isso eu posso desenhar um **heatmap**!\n",
    "\n",
    "*e eu também posso usar a **matriz de distâncias** para plodar um **MDS**!*\n",
    "\n",
    " 4. depois eu volto a 1 e multiplico o valor pelo **peso** deste valor, que agora eu descobri na minha matriz de proximidades. Com medias **numéricas** eu posso fazer os mesmos cálculos, ponderando os valores para suas respectivas **proximidades** e os somando\n",
    " \n",
    " 5. uma vez que eu tenha os valores preenchidos, eu vou para 2 e recalculo tudo de novo, gerando uma nova matriz de proximidades e refazendo as medidas de acordo com as novas ponderações\n",
    " \n",
    " 6. eu faço isso por 6-7 vezes, até que os valores convirjam (ou seja, que não se alterem mais)\n",
    " \n",
    "Missing data no **novo dado**:\n",
    "\n",
    " 1. agora eu tenho dado faltante no dado que eu quero **categorizar**. E eu tiro **duas cópias** deste dado. Uma para o cenário do resultado ser **Sim** e outra para o cenário do resultado ser **Não**\n",
    " \n",
    " 2. e então eu uso a **iteração de dados**, como na anterior, para preencher os dados ausentes. E agora eu rodo as duas amostras na minha **floresta aleatória** e verifico qual das duas é nomeada corretamente, na maioria das vezes\n",
    " \n",
    "*assim nós ajustamos o dado faltante e conseguimos classificar a amostra com dados faltantes!*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adaboost** (sometimes also called boosted decision tree)\n",
    "\n",
    "*foca nas áreas em que o sistema não está desempenhando bem*\n",
    "\n",
    "1. pego os meus dados de **treinamento** com reposição e coloco numa sacola (**bagging**), treino e ajusto um modelo\n",
    "\n",
    "2. uso todos os dados de treinamento para verificar so meu modelo. E descubro que alguns pontos nos meus dados de treinamento não foram bem preditos\n",
    "\n",
    "3. numa segunda sacola eu coloco dados, tendo aqueles que **não foram** bem preditos no primeiro modelo, escolhidos com **maior chance**\n",
    "\n",
    "4. eu faço medições combinadas entre os dados de treinamento + dados do modelo1 e dados desse modelo para verificar aqueles que não foram bem preditos\n",
    "\n",
    "5. e eu faço a sacola3, 4, 5... e modelos3, 4, 5...\n",
    "\n",
    "---\n",
    "\n",
    "**Adaboost** em relação a **Decision Trees**\n",
    "\n",
    "- em **Random Forests**, cada **Decision Tree** é montada até esgotar os elementos (algumas ficam mais profundas e outras mais largas)*\n",
    "\n",
    "- em **Random Forests**, cada **Decision Tree** tem o mesmo peso no voto nas classificações*\n",
    "\n",
    "- em **Random Forests**, cada **Decision Tree** é construída independentemente das demais, o tempo em que cada árvore foi criada não é levado em consideração*\n",
    "\n",
    "- em **Adaboost**, cada árvore terá **apenas** um tronco e duas folhas (uma **Stump**). Então essa será uma **floresta de tocos**!*\n",
    "\n",
    "- classificações com **tocos** se mostram muito imprecisas! Tocos são **Weak Learners**...*\n",
    "\n",
    "- em **Adabost**, algons **tocos** ganham mais poder no voto do que outros nas classificações*\n",
    "\n",
    "- em **Adaboost**, os erros que um **toco** cometeu na primeira **floresta de tocos** influencia o erro que ele cometerá na segunda **floresta de tocos** we assim na sua sequência de **florestas de tocos**...*\n",
    "\n",
    "---\n",
    "\n",
    "**Adaboost**\n",
    "\n",
    "1. combina um bocado de **weak learners** para produzir classificações\n",
    "\n",
    "2. alguns tocos ganham mais **voz** na classificação do que outros\n",
    "\n",
    "3. cada toco é feito levando em conta os **erros de toco** cometidos anteriormente\n",
    "\n",
    "---\n",
    "\n",
    "**Algoritmo Adaboost** [aqui](https://www.youtube.com/watch?v=LsK-xG1cLYA)\n",
    "\n",
    "0. eu crio uma nova coluna no meu dataset de **peso da amostra**. Todos começam com o mesmo peso\n",
    "\n",
    "1. eu crio meus **tocos** baseados em **uma coluna**. E de linha por linha, avalio como os meus tocos classificaram o resultado\n",
    "\n",
    "2. eu faço o mesmo com **cada uma** das outras colunas\n",
    "\n",
    "3. e agora eu calculo o **Índice Gini** para cada um dos tocos. O com **menor** Índice Gini será o primeiro da minha nova classificação\n",
    "\n",
    "4. o **erro total** de um toco é a soma dos pesos associados às amostras classificadas incorretamente. Um toco **ruim** apresenta peso zero e um toco **perfeito**, o peso 1. Assim os tocos que predizem tanto quanto o lançar de uma moeda passam a perder a voz, com voz **zero**. E os tocos que classificam mal, ganham voz **negativa** até peso -1\n",
    "\n",
    "$AmountofSay = \\frac{1}{2} log \\frac{(1 - Total Error)}{Total Error}$\n",
    "\n",
    "*isso forma uma **curva logísitca** inversa!*\n",
    "\n",
    "5. eu somo para o **erro total**. E eu atribuo novos pesos **mais elevados**, a todos os tocos que classificaram **mal**. A equação que faz isso é gradativa e o que aumenta a cada iteração não é algo tão elevado\n",
    "\n",
    "*o **erro total** não pode ser nem zero, nem um, ou a equação não funcionará! Na prática, um pequeno erro é introduzido no sistema para prevenir este problema*\n",
    "\n",
    "$NewSampleWeight = SampleWeight \\cdot e^{AmounttoSay}$\n",
    "\n",
    "6. e agora a mesma coisa, **diminuindo** o peso dos tocos que classificaram **bem**\n",
    "\n",
    "$NewSampleWeight = SampleWeight \\cdot e^{-AmounttoSay}$\n",
    "\n",
    "7. e eu crio uma **nova coluna** de pesos amostrais e a normalizo. Eu posso fazer isso por algumas vezes para esta floresta de tocos, até que os resultados dos pesos não variem significativamente\n",
    "\n",
    "---\n",
    "\n",
    "Estratégia no **bootstrapping**\n",
    "\n",
    "0. cada floresta de tocos é produzida a partir de uma amostragem **com reposição**. Então a segunda floresta... sétima floresta podem variar ligeiramente entre si. Mas eu produzo uma variação **maior** a partir da **segunda floresta**, dando preferência aos tocos **especialmente ruins**\n",
    "\n",
    "1. a partir da **primeira floresta de tocos**, eu os separo em **dois grupos**, os que deram respostas **corretas** e os que deram respostas **erradas** e somo os totais para cada grupo\n",
    "\n",
    "2. eu gero um número aleatório entre 0 e 1 e eu vejo onde o número cai, usando os pesos amostrais como uma **distribuição normal**\n",
    "\n",
    "*existem vários métodos para se fazer essa coleta. O importante é que eu quero que os tocos que deram respostas **erradas** tenham mais chance de entrar na nova coleção*\n",
    "\n",
    "3. eu faço o processo até gerar uma coleção do mesmo tamanho que o original, usando seleção **com reposição**\n",
    "\n",
    "*por fim: aqui estão os passos para o algoritmo realizar uma varredura pelos **piores tocos**. Agora, eu posso usar esse mesmo algoritmo para encontrar os **melhores tocos**? Claro que sim! É que na prática ele se mostrou uma ótima ferramenta para buscar casos problemáticos e não tão eficiente assim para o caso da otimização. Mas algoritmos são **ferramentas** - e martelar com uma chave de fenda não é minha melhor escolha!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Process:\n",
    "\n",
    "1. Do some research\n",
    "\n",
    " - get a general understanding\n",
    "\n",
    "2. Find SkLearn documentation\n",
    "\n",
    "3. Deploy it!\n",
    "\n",
    " - get your hands dirty\n",
    " \n",
    "4. Use it to make predictions\n",
    "\n",
    "5. Evaluate it\n",
    "\n",
    " - what is the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and questions\n",
    "\n",
    "---\n",
    "\n",
    "***POI - Person of Interest*** - 35 people - 2005-12-28-enron-participants_x.htm\n",
    "\n",
    "- indicted\n",
    "\n",
    "- settled without admitting guilt\n",
    "\n",
    "- testified in exchange for immunity\n",
    "\n",
    "Partimos de uma lista pequena, descoberta a partir de artigos de jornal. Ela pode ter muitos erros.\n",
    "\n",
    "#### Accuracy $\\leftrightarrow$ Training Set Size\n",
    "\n",
    "- 1.000 amostras\n",
    "\n",
    " - 800 para treino\n",
    " \n",
    "   - 4 grupos de 200, fatias recombinadas\n",
    " \n",
    " - 200 para teste\n",
    "\n",
    "100% eu nunca atinjo\n",
    "\n",
    "Um excelente modelo pega tipo uns 96%\n",
    "\n",
    "Uma accuracy inicial, com 200 amostras, gira em torno de 55%\n",
    "\n",
    "Para 400 amostras, isso sobe para uns 70%... para 600 amostras, em torno de 80%... 800 amostras, em torno de 82%, delimitando uma **curva de aprendizado**\n",
    "\n",
    "O grande problema é tipo, abaixo de 200 amostras (os padrões não aparecem com nenhuma clareza!)\n",
    "\n",
    "- no mundo real, nestes casos eu corro atrás de mais amostras\n",
    "\n",
    "- verifique **sempre** se quando eu adiciono mais dados, como a minha curva de aprendizado se comporta!\n",
    "\n",
    "*mais **dados** normalmente dão melhor resultado do que um algoritmo **super tunado**!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enron dataset\n",
    "\n",
    "Procure pelo Google um arquivo de quase meio Gigabytes...\n",
    "\n",
    "tar -xvzf enron_mail_20110402.tgz\n",
    "\n",
    "tipos de dados:\n",
    "\n",
    "- texto (palavras)\n",
    "\n",
    "- numérico (valores numéricos)\n",
    "\n",
    "- time series (date, timestamp)\n",
    "\n",
    "- categorical - um número limitado de valores limitados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions\n",
    "\n",
    "---\n",
    "\n",
    "#### Continous supervised learning\n",
    "\n",
    "- duas dimensões: input e output\n",
    "\n",
    "- minha **saída** será preferencialmente **binária** (discreta)\n",
    "\n",
    "- em casos especiais, minha **saída** será o produto de alguma curva, **contínua** (muitas coisas que consideramos discretas, como dia ensolarado ou chuvoso, na verdade são um colapso de algo contínuo!) - existe um conceito de **ordering** implicado!\n",
    "\n",
    "- um bom caso **discreto** é quando um número inteiro não tem nenhuma relação com o seguinte (ex: número de telefone) e o espaço intermediário simplesmente **não existe**\n",
    "\n",
    "---\n",
    "\n",
    "Classificação de terrenos:\n",
    "\n",
    "- eu recebo uma informação **discreta** (rápido/devagar), mas na verdade eu tenho a velocidade do veículo em uma escala **contínua**! Eu posso dizer que a velocidade é uma **generalização/especialização** da informação discreta (rápido/devagar) - depende de quem eu colho primeiro!\n",
    "\n",
    "- se eu seleciono um conjunto de **pontos discretos** sobre um gráfico e eu vejo um certo padrão na dispersão, eu posso **generalizar** essa relação ajustando uma reta:\n",
    "\n",
    "$PesoTotal = (\\frac{catop}{hip})*Densidade + val(Y|X_{o})$\n",
    "\n",
    "- $Y = m x + b$\n",
    "\n",
    " - $Y$ é meu **target**, a variável que eu quero prever\n",
    " \n",
    " - $m$ é minha **slope** \n",
    " \n",
    " - $x$ é meu **input**\n",
    " \n",
    " - $b$ é minha **intercept** (o valor de $Y$, quando o ponto na reta está em $X=0$)\n",
    " \n",
    "- se u usasse um classificador **discreto**, meu gráfico iria se assemelhar a uma **escada de degraus**, o que também não estaria errado. Tudo isso depende de onde quero chegar!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:\n",
    "\n",
    "- eu construo minha linha de regressão\n",
    "\n",
    "Testing data:\n",
    "    \n",
    "- eu verifico a qualidade do que eu fiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from studentRegression import studentReg\n",
    "from class_vis import prettyPicture, output_image\n",
    "from ages_net_worths import ageNetWorthData\n",
    "\n",
    "ages_train, ages_test, net_worths_train, net_worths_test = ageNetWorthData()\n",
    "reg = studentReg(ages_train, net_worths_train)\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(ages_train, net_worths_train, color=\"b\", label=\"train data\")\n",
    "plt.scatter(ages_test, net_worths_test, color=\"r\", label=\"test data\")\n",
    "plt.plot(ages_test, reg.predict(ages_test), color=\"black\")\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel(\"ages\")\n",
    "plt.ylabel(\"net worths\")\n",
    "\n",
    "plt.savefig(\"test.png\")\n",
    "output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando minha regressão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def studentReg(ages_train, net_worths_train):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(ages_train,net_worths_train)\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-quadrado para predições\n",
    "\n",
    "*observação: as outras ferramentas eram de **Machine Learning** e as coisas naquele mundo funcionam diferentes! Então observe que agora estamos novamente no mundo da estatística clássica!*\n",
    "\n",
    "*o valor máximo é um e quanto maior este valor, melhor será a justeza do modelo*\n",
    "\n",
    "*note também que eu terei um r-quadrado para o meu dataset de treinamento e outro para meu dataset de teste! Isso é normal, pois os dados não são exatamente iguais!*\n",
    "\n",
    "---\n",
    "\n",
    "How to **evaluate** your linear regression?\n",
    "\n",
    "Uso **linear regression errors** para isso\n",
    "\n",
    "$Error = ActualNetWorth - PredictedNetWorth$ \n",
    "\n",
    "- $ActualNetWorth$ - um ponto/uma amostra dos dados preditos\n",
    "\n",
    "- $PredictedNetWorth$ - predito pela linha de regressão\n",
    "\n",
    "*o erro é a distância em $Y$, do ponto real à linha de regressão, para um mesmo $X$*\n",
    "\n",
    "A ideia é **minimizar o erro** em todo o meu conjunto de pontos usados para treinar o modelo\n",
    "\n",
    "- você tem que encontrar valores para $m$ e $b$ de maneira a atender à condição acima\n",
    "\n",
    "$Y = m x + b$\n",
    "\n",
    "- ao usar o $erro^{2}$, eu penalizo **geometricamente** os pontos mais distantes. Isso funciona como uma **desambiguação** de linhas paralelas, forçando o modelo a pegar a mais central. E ainda me corrige o **sinal**, caso ele seja negativo!\n",
    "\n",
    "- aparentemente em **SSE**, o esforço computacional **diminui** sensivelmente ao se trabalhar com $erro^{2}$\n",
    "\n",
    "*SSE - Sum of Square Errors, muitas vezes contraposto a SEE - Standard Error of Estimate [aqui](https://www.analystforum.com/forums/cfa-forums/cfa-level-ii-forum/91342844)*\n",
    "\n",
    "*O SSE é meio **traiçoeiro**, pois a cada ponto que eu adiciono ao treinamento do modelo, este número não para de **crescer**, dando a ideia de que meu modelo está degradando. E isso não é verdadeiro e é um **péssimo** sinal!*\n",
    "\n",
    "---\n",
    "\n",
    "Outra métrica é o $r^{2}$:\n",
    "\n",
    "- \"How much of my change in the output $Y$ is explained by the change in my input $X$?\"\n",
    "\n",
    "$0. \\leq r^{2} \\leq 1.$\n",
    "\n",
    "*quando eu obtenho um $r^{2} no meu modelo linear computacional, eu sempre posso fazer um esforço para melhorar este número incorporando outras **feições** que me tragam algo de novo, ou alguma **dica** do que pode estar acontecendo. Por exemplo, um valor alto de **temperatura** pode estar indicando que o motor do automóvel está sendo forçado, tanto por **velocidade excessiva**, como por **esforço de torque em baixa rotação**. Ou mesmo que algo vai mal com meu sistema de arrefecimento... mar por que não tentar incorporar este indicador secundário ao meu modelo?*\n",
    "\n",
    "*às vezes o que eu tento prever é algo tão complexo que é difícil de extrapolar através de um modelo matemático. Por exemplo, se aquele país está realmente indo para a **declaração de guerra** ou se é apenas mais um blefe do seu ditador... não confie 100% em **modelos**, essa é a lição!*\n",
    "\n",
    "---\n",
    "\n",
    "Algoritmos mais usados:\n",
    "\n",
    "- Ordinary Least Squares (OLS) - o usado no **sklearn**\n",
    "\n",
    "- Gradient Descent\n",
    "\n",
    "*existem **macetes** em todos esses algoritmos, por exemplo, eu realizar um **Gradient Descent** guardando a **memória** do termo anterior (ou formando um **padrão** com termos anteriores), ou usando a **derivada primeira** para facilitar encontrar o caminho mais rápido. Isso pode acelerar imensamente processos complexos de otimização usando sistemas lineares. Mas por enquanto, o que é colocado aqui é apenas o **básico** e ficamos por aqui*\n",
    "\n",
    "*as an advanced note, for the parabola shape, it is possible to fit non-linear relationships through use of **feature transformations**. For instance adding the $x^{2}$ term as a feature gets us **polynomial regression**, which can be considered a special case of multiple linear regression*\n",
    "\n",
    "*às vezes você pode querer **quebrar** o seu conjunto de dados em dois ou mais conjuntos e atribuir a cada um deles uma nova regressão linear. Essa é uma estratégia bastante usual para este tipo de trabalho!*\n",
    "\n",
    "*eliminar **outliers** também pode ser uma boa escolha! O fator que causa sua distribuição anômala pode não ter nenhuma origem nos fatores que estamos tentando modelizar. Outro fator é aplicar **filtros** e eliminar **ruído**. Às vezes aquele ponto porta ruído que o afasta do lugar onde ele deveria estar! Antes de olhar para sua matemática, olhe para a **realidade** à sua volta em busca de uma causa provável para aquele fenômeno estranho!*\n",
    "\n",
    "*números **muito grandes** para $m$ demonstram que de fato, $X$ não causa alterações em $Y$! Eu não tenho valores a atribuir para $X$ no caso de uma reta vertical!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Classification Vs Regression\n",
    "\n",
    "Property\n",
    "\n",
    "- output type:\n",
    "\n",
    " - supervised learning: **Discrete** (class labels)\n",
    " \n",
    " - regression: **Continous** (number)\n",
    " \n",
    "- what are you trying to find?\n",
    "\n",
    " - supervised learning: **Decision boundary** (may assign a class label)\n",
    " \n",
    " - regression: **Best fit line**\n",
    " \n",
    "- how you evaluate?\n",
    "\n",
    " - supervised learning: **Accuracy**\n",
    " \n",
    " - regression: **Sum of square error** / **$r^{2}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Multivariate Regression\n",
    "\n",
    "- várias entradas $\\rightarrow$ uma reta\n",
    "\n",
    " - potência do motor - $X1$ $\\rightarrow$ preço: $Y = \\alpha X1  + \\beta X2 + \\gamma X3 + \\delta$ \n",
    " \n",
    " - linha - $X2$\n",
    " \n",
    " - ano - $X3$\n",
    " \n",
    "- uma dimensão: $X1$; duas dimensões: $X1$, $X2$; três dimensões: $X1$, $X2$, $X3$; ... + o **intercept**!\n",
    "\n",
    "*se os números não encaixarem perfeitamente, existem métodos de ajuste que olham em **todos** os dados do conjunto* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Mini project**: os bônus anuais por trabalhar na ENRON\n",
    "\n",
    "    #!/usr/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Starter code for the regression mini-project.\n",
    "    \n",
    "    Loads up/formats a modified version of the dataset\n",
    "    (why modified?  we've removed some trouble points\n",
    "    that you'll find yourself in the outliers mini-project).\n",
    "    Draws a little scatterplot of the training/testing data\n",
    "    You fill in the regression code where indicated:\n",
    "\"\"\"    \n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "dictionary = pickle.load( open(\"../final_project/final_project_dataset_modified.pkl\", \"r\") )\n",
    "\n",
    "### list the features you want to look at--first item in the \n",
    "### list will be the \"target\" feature\n",
    "features_list = [\"bonus\", \"salary\"]\n",
    "data = featureFormat( dictionary, features_list, remove_any_zeroes=True)\n",
    "target, features = targetFeatureSplit( data )\n",
    "\n",
    "### training-testing split needed in regression, just like classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(features, target, test_size=0.5, random_state=42)\n",
    "train_color = \"b\"\n",
    "test_color = \"b\"\n",
    "\n",
    "### Your regression goes here!\n",
    "### Please name it reg, so that the plotting code below picks it up and \n",
    "### plots it correctly. Don't forget to change the test_color above from \"b\" to\n",
    "### \"r\" to differentiate training points from test points.\n",
    "\n",
    "### draw the scatterplot, with color-coded training and testing points\n",
    "import matplotlib.pyplot as plt\n",
    "for feature, target in zip(feature_test, target_test):\n",
    "    plt.scatter( feature, target, color=test_color ) \n",
    "for feature, target in zip(feature_train, target_train):\n",
    "    plt.scatter( feature, target, color=train_color ) \n",
    "\n",
    "### labels for the legend\n",
    "plt.scatter(feature_test[0], target_test[0], color=test_color, label=\"test\")\n",
    "plt.scatter(feature_test[0], target_test[0], color=train_color, label=\"train\")\n",
    "\n",
    "### draw the regression line, once it's coded\n",
    "try:\n",
    "    plt.plot( feature_test, reg.predict(feature_test) )\n",
    "except NameError:\n",
    "    pass\n",
    "plt.xlabel(features_list[1])\n",
    "plt.ylabel(features_list[0])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "---\n",
    "\n",
    "Causados por:\n",
    "\n",
    "- um sensor funcionou mal (ignorar)\n",
    "\n",
    "- entrou ruído na aquisição dos dados (ignorar)\n",
    "\n",
    "- um número foi digitado errado / erro de OCRização (reinterpretar)\n",
    "\n",
    "- uma entrada realmente **anômala** e **perigosa**! (ex: **fraud detection** $\\rightarrow$ prestar muita atenção!)\n",
    "\n",
    "#### Outliers detection & removal (Outliers-rejection)\n",
    "\n",
    "Algoritmo:\n",
    "\n",
    "1. treinar (com os dados disponíveis)\n",
    "\n",
    "2. remover (depois do treinamento - em torno de 10% dos seus pontos de dados)\n",
    "\n",
    "3. treinar novamente\n",
    "\n",
    "*isso pode ser repetido inúmeras vezes, ou quando um **trigger** for acionado - é como remover um vírus!*\n",
    "\n",
    "*o outlier terá sempre o maior **residual error** do conjunto!*\n",
    "\n",
    "#### Effect of outlier removal on Regression\n",
    "\n",
    "- a **inclinação** e a **posição** da linha de regressão se alteram\n",
    "\n",
    "- a linha se ajustará **melhor** aos pontos remanescentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini project - outliers [aqui](https://github.com/udacity/ud120-projects/commit/82958a765c7632c5fa39ec6beb5b3ab8fa96892b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "### read in data dictionary, convert to numpy array\n",
    "data_dict = pickle.load( open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "features = [\"salary\", \"bonus\"]\n",
    "data = featureFormat(data_dict, features)\n",
    "\n",
    "### your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlierCleaner(predictions, ages, net_worths):\n",
    "    \"\"\"\n",
    "        clean away the 10% of points that have the largest\n",
    "        residual errors (different between the prediction\n",
    "        and the actual net worth)\n",
    "        return a list of tuples named cleaned_data where \n",
    "        each tuple is of the form (age, net_worth, error)\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_data = []\n",
    "\n",
    "    ### your code goes here\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from outlier_cleaner import outlierCleaner\n",
    "\n",
    "### load up some practice data with outliers in it\n",
    "ages = pickle.load( open(\"practice_outliers_ages.pkl\", \"r\") )\n",
    "net_worths = pickle.load( open(\"practice_outliers_net_worths.pkl\", \"r\") )\n",
    "\n",
    "### ages and net_worths need to be reshaped into 2D numpy arrays\n",
    "### second argument of reshape command is a tuple of integers: (n_rows, n_columns)\n",
    "### by convention, n_rows is the number of data points\n",
    "### and n_columns is the number of features\n",
    "ages       = numpy.reshape( numpy.array(ages), (len(ages), 1))\n",
    "net_worths = numpy.reshape( numpy.array(net_worths), (len(net_worths), 1))\n",
    "\n",
    "### fill in a regression here!  Name the regression object reg so that\n",
    "### the plotting code below works, and you can see what your regression looks like\n",
    "\n",
    "try:\n",
    "    plt.plot(ages, reg.predict(ages), color=\"blue\")\n",
    "except NameError:\n",
    "    pass\n",
    "plt.scatter(ages, net_worths)\n",
    "plt.show()\n",
    "\n",
    "### identify and remove the most outlier-y points\n",
    "cleaned_data = []\n",
    "try:\n",
    "    predictions = reg.predict(ages)\n",
    "    cleaned_data = outlierCleaner( predictions, ages, net_worths )\n",
    "except NameError:\n",
    "    print \"your regression object doesn't exist, or isn't name reg\"\n",
    "    print \"can't make predictions to use in identifying outliers\"\n",
    "\n",
    "### only run this code if cleaned_data is returning data\n",
    "if len(cleaned_data) > 0:\n",
    "    ages, net_worths, errors = zip(*cleaned_data)\n",
    "    ages       = numpy.reshape( numpy.array(ages), (len(ages), 1))\n",
    "    net_worths = numpy.reshape( numpy.array(net_worths), (len(net_worths), 1))\n",
    "\n",
    "    ### refit your cleaned data!\n",
    "    try:\n",
    "        reg.fit(ages, net_worths)\n",
    "        plt.plot(ages, reg.predict(ages), color=\"blue\")\n",
    "    except NameError:\n",
    "        print \"you don't seem to have regression imported/created,\"\n",
    "        print \"   or else your regression object isn't named reg\"\n",
    "        print \"   either way, only draw the scatter plot of the cleaned data\"\n",
    "    plt.scatter(ages, net_worths)\n",
    "    plt.xlabel(\"ages\")\n",
    "    plt.ylabel(\"net worths\")\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print \"outlierCleaner() is returning an empty list, no refitting to be done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "sklearn [aqui](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "\n",
    "---\n",
    "\n",
    "#### Unsupervised learning\n",
    "\n",
    "- partimos de um dataset sem rótulos\n",
    "\n",
    "- **clustering** - os dados se aglomeram em uma região do espaço\n",
    "\n",
    "- **dimensionality reduction** - os dados se reduzem a alguma espécie de função\n",
    "\n",
    "Algoritmo mais usado - **k-Means** [simulador](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)\n",
    "\n",
    "sklearn [aqui](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "1. **assign** - determino quais pontos **pertencem** a cada centro\n",
    "\n",
    "2. **optimize** - ajusto a **localização** do centro para o ponto de menor soma dos quadrados\n",
    "\n",
    "*eu tenho uma linha de separação perpendicular á reta que une os pontos de centro de clusters*\n",
    "\n",
    "*ao otimizar, eu mudei o local do centro. ao fazer novo **assign**, alguns pontos mudarão de domínio. Eu repito esse processo até os centros não se moverem mais ou até pontos pararem de mudar de domínio*\n",
    "\n",
    "Quantas classes eu crio? Quais as limitações de k-Means?\n",
    "\n",
    " - cada classe tem uma espécie de **centro** e uma **fronteira**. As fronteiras do k-Means me parecem um pouco **quadradas**. Ele tenta abarcar **todos** os pontos do modelo e dividí-los. No mundo real, acabam sobrando zonas **selvagens** nas periferias de todos os centros\n",
    " \n",
    " - a posição inicial do centróide parece influenciar **sim**, como será a configuração final do problema. Especialmente em casos meio difusos com núcleos pouco definidos. Isso é um problema que parece afetar todos os algoritmos da classe **hill-climbing**. O sistemam às vezes cai naquilo que se chama **local minimum** (estaciona sobre uma pedra mais alta/um buraco e para por lá!)\n",
    " \n",
    "*eu posso usar **feature scaling** para preparar meus dados antes de fazer a clusterização*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini projeto** [aqui](https://github.com/udacity/ud120-projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Skeleton code for k-means clustering mini-project.\n",
    "\"\"\"\n",
    "import pickle\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "def Draw(pred, features, poi, mark_poi=False, name=\"image.png\", f1_name=\"feature 1\", f2_name=\"feature 2\"):\n",
    "    \"\"\" some plotting code designed to help you visualize your clusters \"\"\"\n",
    "\n",
    "    ### plot each cluster with a different color--add more colors for\n",
    "    ### drawing more than five clusters\n",
    "    colors = [\"b\", \"c\", \"k\", \"m\", \"g\"]\n",
    "    for ii, pp in enumerate(pred):\n",
    "        plt.scatter(features[ii][0], features[ii][1], color = colors[pred[ii]])\n",
    "\n",
    "    ### if you like, place red stars over points that are POIs (just for funsies)\n",
    "    if mark_poi:\n",
    "        for ii, pp in enumerate(pred):\n",
    "            if poi[ii]:\n",
    "                plt.scatter(features[ii][0], features[ii][1], color=\"r\", marker=\"*\")\n",
    "    plt.xlabel(f1_name)\n",
    "    plt.ylabel(f2_name)\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "\n",
    "### load in the dict of dicts containing all the data on each person in the dataset\n",
    "data_dict = pickle.load( open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "### there's an outlier--remove it! \n",
    "data_dict.pop(\"TOTAL\", 0)\n",
    "\n",
    "### the input features we want to use \n",
    "### can be any key in the person-level dictionary (salary, director_fees, etc.) \n",
    "feature_1 = \"salary\"\n",
    "feature_2 = \"exercised_stock_options\"\n",
    "poi  = \"poi\"\n",
    "features_list = [poi, feature_1, feature_2]\n",
    "data = featureFormat(data_dict, features_list )\n",
    "poi, finance_features = targetFeatureSplit( data )\n",
    "\n",
    "### in the \"clustering with 3 features\" part of the mini-project,\n",
    "### you'll want to change this line to \n",
    "### for f1, f2, _ in finance_features:\n",
    "### (as it's currently written, the line below assumes 2 features)\n",
    "for f1, f2 in finance_features:\n",
    "    plt.scatter( f1, f2 )\n",
    "plt.show()\n",
    "\n",
    "### cluster here; create predictions of the cluster labels\n",
    "### for the data and store them to a list called pred\n",
    "\n",
    "### rename the \"name\" parameter when you change the number of features\n",
    "### so that the figure gets saved to a different file\n",
    "try:\n",
    "    Draw(pred, finance_features, poi, mark_poi=False, name=\"clusters.pdf\", f1_name=feature_1, f2_name=feature_2)\n",
    "except NameError:\n",
    "    print \"no predictions object named pred found, no clusters to plot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "---\n",
    "\n",
    "*isso é um tópico de **preprocessing data**!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso de Cris, Sara e Cameron\n",
    "\n",
    "Cris quer comprar uma camiseta na praia, mas não tem certeza do seu tamanho. Seus termos de comparação são seus amigos:\n",
    "    \n",
    "- Sara é mignon e bastante baixa e usa P (Cris é mais encorpado e um pouco mais alto do que Sara)\n",
    "    \n",
    "- Cameron é do tipo alto e forte e usa G (Cris não é tão forte assim e também não tão alto como Cameron)\n",
    "\n",
    "*supondo que não exista tamanho M, sugerimos que Cris compre G*\n",
    "\n",
    "- Cris pesa 71kg e mede 1.70m - somando os dois, chegamos a 72.7\n",
    "\n",
    "- Cameron pesa 89kg e mede 1.80 - somando os dois, temos 90.8\n",
    "\n",
    "- Sara pesa 63kg e mede 1.62m - somando, temos 64.62\n",
    "\n",
    "*apenas **somando** as duas métricas, o resultado nos aproximou Cris de Sara e não de Cameron, como imaginávamos! O problema é que esse nosso novo índice possui duas feições muito **desbalanceadas**: massa corpórea predomina e altura serve mais bem... como um critério de desempate!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Quando adianta e quando não adianta reescalonar\n",
    "\n",
    "- **Decision Trees** - **não** existe **Tradeoff**, apenas cortes em uma ou outra dimensão\n",
    "\n",
    "- **Linear Regression** - **não**, pois os coeficientes **acompanham** a feição\n",
    "\n",
    "- **SVM** - **sim**, pois as distâncias se tornam mais nítidas\n",
    "\n",
    "- **k-Means Clustering** - **sim**, pela mesma razão do anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Feature Scaling Formula\n",
    "\n",
    "${X}' = \\frac{X - X_{min}}{X_{max} - X_{min}}$\n",
    "\n",
    "Em outras palavras:\n",
    "\n",
    "$0 \\leqslant {X}' \\leqslant 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4166666666666667"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" quiz materials for feature scaling clustering \"\"\"\n",
    "### FYI, the most straightforward implementation might \n",
    "### throw a divide-by-zero error, if the min and max\n",
    "### values are the same\n",
    "### but think about this for a second--that means that every\n",
    "### data point has the same value for that feature!  \n",
    "### why would you rescale it?  Or even use it at all?\n",
    "def featureScaling(arr):\n",
    "    nmax = max(data)\n",
    "    nmin = min(data)\n",
    "    if (nmax == nmin):\n",
    "        return None\n",
    "    normalize = nmax - nmin\n",
    "    return [float(e-nmin)/normalize for e in data ]\n",
    "\n",
    "# tests of your feature scaler--line below is input data\n",
    "data = [115, 140, 175]\n",
    "print featureScaling(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Learning\n",
    "\n",
    "---\n",
    "\n",
    "#### Learning from text\n",
    "\n",
    "What is the input feature?\n",
    "\n",
    "- a bag of words (contamos a **frequência** de cada palavra):\n",
    "\n",
    " - (**regative**) nice day\n",
    " \n",
    " - (**positive**) a very nice day $\\rightarrow SVM \\rightarrow \\{0,x\\}$\n",
    " \n",
    "*o tamanho de um e-mail não é padronizado. Então não podemos simplesmente usar uma dimensão para cada palavra no SVM*\n",
    "\n",
    "Método:\n",
    "\n",
    "1. quebre o texto em palavras e o coloque no saco\n",
    "\n",
    "2. crie um dicionário\n",
    "\n",
    "3. conte a frequência com que cada palavra aparece em cada um de seus textos\n",
    "\n",
    "Observe que:\n",
    "\n",
    "- **ordem** das palavras não conta (para contar, é necessário usar métodos mais sofisticados)\n",
    "\n",
    "- **frases longas** fornecem entradas diferentes (sim, o problema são as frases muito **curtas**!)\n",
    "\n",
    "- **frases complexas** não são interpretadas corretamente (ex: \"Chicago Bulls\" não trará um bom resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Count Verctorized (Bag of Words) [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "É parte da biblioteca **sklearn.feature_extraction.text**\n",
    "\n",
    "Formato de saída:\n",
    "\n",
    "    (n documento, n palavra) = vezes que aparece\n",
    "    \n",
    "#### Low-Information Words\n",
    "\n",
    "- algumas palavras não nos dizem **nada** (**stopwords**) - podemos excluir normalmente expressões diárias, termos de ligação, etc..\n",
    "\n",
    "- estratégia: **remover**\n",
    "\n",
    "*o julgamento disso depente do que queremos filtrar*\n",
    "\n",
    "---\n",
    "\n",
    "**Pacote NLTK** (Natural Language Toolkit)\n",
    "\n",
    "        conda install -c anaconda nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso abre uma janela para um download monstruoso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = stopwords.words(\"english\")\n",
    "len(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Vocabulary**: not all unique words are different!\n",
    "\n",
    "- unresponsive $\\rightarrow$ **Stammer** $\\rightarrow$ u' respons' (você pega a **haste** de uma palavra)\n",
    "\n",
    "- response\n",
    "\n",
    "- responsivity\n",
    "\n",
    "- responsiveness\n",
    "\n",
    "- respond\n",
    "\n",
    "*um problema de busca 5D se torna um problema de busca 1D sem perda substancial do objetivo!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Order of informations in Text Processing\n",
    "\n",
    "1. remove **stopwords**\n",
    "\n",
    "2. **stamming**\n",
    "\n",
    "3. create your **bag of words**\n",
    "\n",
    "#### Tf Idf Representation\n",
    "\n",
    "- **Term frequency** (tipo: **Bag of Words**)\n",
    "\n",
    "- **Inverse document frequency** (weighting by how often word occurs in corpus) - ordem **invera**\n",
    "\n",
    " - eu coloco alto peso em palavras **raras**\n",
    " \n",
    " - a ideia é como essas palavras tornam um determinado conjunto de textos **distintos** dos demais\n",
    " \n",
    " - sao palavras que de certa maneira colocam sua **assinatura** em um texto que desconhecemos o autor\n",
    " \n",
    " - tem a ver com suas proficiências, suas áreas específicas de interesse (ex: **escotilhas**, **escultura**, **africano**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In the beginning of this class, you identified emails by their authors using a number of supervised classification algorithms. In those projects, we handled the preprocessing for you, transforming the input emails into a TfIdf so they could be fed into the algorithms. Now you will construct your own version of that preprocessing step, so that you are going directly from raw data to processed features\n",
    "\n",
    "You will be given two text files: \n",
    "\n",
    "- one contains the locations of all the emails from Sara\n",
    "\n",
    "- the other has emails from Chris\n",
    "\n",
    "You will also have access to the **parseOutText()** function, which accepts an opened email as an argument and returns a string containing all the (stemmed) words in the email.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "---\n",
    "\n",
    "Why Feature Selection?\n",
    "\n",
    "- as coisas devem ser feitas o mais **simples** possível, mas não **o mais simples** (A Einstein)\n",
    "\n",
    "- muitas variáveis ocultas são capturadas razoavelmente por apenas **uma** feição - dor no peito $\\rightarrow$ problema cardíaco\n",
    "\n",
    "- mas se verificarmos o ajuste deste método, ele nos dará um resultado apenas **medíocre** (algo como 58%)\n",
    "\n",
    "- então muitas vezes precisamos de outras feições para nos apoiar - sobrepeso, consumo de gorduras, diabetes...\n",
    "\n",
    "- adicione **novas feições** e adicione as **melhores feições** que tiver para o apoiar na escolha\n",
    "\n",
    "- uma feição pode ser **estratégica**, ela pode não ser completa e nem aplicável a todos os casos, mas pode ajudar a **definir padrões**. O segredo todo por trás de mineração de dados é a **identifiação de padrões essenciais**\n",
    "\n",
    "#### Nova feição no caso Enron:\n",
    "\n",
    "- use da intuição humana\n",
    "\n",
    "- codifique sua nova feição\n",
    "\n",
    "- visualize\n",
    "\n",
    "- corrija, aprimore, volte ao passo 1\n",
    "\n",
    "POIs enviam e-mails com mais frequência a outros POIs que à população em geral\n",
    "\n",
    "        from_poi_to_this_person\n",
    "        \n",
    "- número de mensagens\n",
    "\n",
    "- do POI\n",
    "\n",
    "- para esta pessoa\n",
    "\n",
    "- procedimento:\n",
    "\n",
    " - pegue o e-mail do autor\n",
    " \n",
    " - compare com uma lista de e-mails POIs conhecidos\n",
    " \n",
    " - retorne um booleano se o autor for um POI\n",
    " \n",
    " - agregue a contagem a todos os e-mails da pessoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### in poiFlagEmail() below, write code that returns a boolean\n",
    "### indicating if a given email is from a POI\n",
    "import sys\n",
    "import reader\n",
    "import poi_emails\n",
    "\n",
    "def getToFromStrings(f):\n",
    "    '''\n",
    "    The imported reader.py file contains functions that we've created to help\n",
    "    parse e-mails from the corpus. .getAddresses() reads in the opening lines\n",
    "    of an e-mail to find the To: From: and CC: strings, while the\n",
    "    .parseAddresses() line takes each string and extracts the e-mail addresses\n",
    "    as a list.\n",
    "    '''\n",
    "    f.seek(0)\n",
    "    to_string, from_string, cc_string   = reader.getAddresses(f)\n",
    "    to_emails   = reader.parseAddresses( to_string )\n",
    "    from_emails = reader.parseAddresses( from_string )\n",
    "    cc_emails   = reader.parseAddresses( cc_string )\n",
    "\n",
    "    return to_emails, from_emails, cc_emails\n",
    "### POI flag an email\n",
    "\n",
    "def poiFlagEmail(f):\n",
    "    \"\"\" given an email file f,\n",
    "        return a trio of booleans for whether that email is\n",
    "        to, from, or cc'ing a poi \"\"\"\n",
    "\n",
    "    to_emails, from_emails, cc_emails = getToFromStrings(f)\n",
    "    ### poi_emails.poiEmails() returns a list of all POIs' email addresses.\n",
    "    poi_email_list = poi_emails.poiEmails()\n",
    "\n",
    "    to_poi = False\n",
    "    from_poi = False\n",
    "    cc_poi   = False\n",
    "    ### to_poi and cc_poi are boolean variables which flag whether the email\n",
    "    ### under inspection is addressed to a POI, or if a POI is in cc,\n",
    "    ### respectively. You don't have to change this code at all.\n",
    "\n",
    "    ### There can be many \"to\" emails, but only one \"from\", so the\n",
    "    ### \"to\" processing needs to be a little more complicated\n",
    "    if to_emails:\n",
    "        ctr = 0\n",
    "        while not to_poi and ctr < len(to_emails):\n",
    "            if to_emails[ctr] in poi_email_list:\n",
    "                to_poi = True\n",
    "            ctr += 1\n",
    "    if cc_emails:\n",
    "        ctr = 0\n",
    "        while not cc_poi and ctr < len(cc_emails):\n",
    "            if cc_emails[ctr] in poi_email_list:\n",
    "                cc_poi = True\n",
    "            ctr += 1\n",
    "    #################################\n",
    "    ######## your code below ########\n",
    "    ### set from_poi to True if #####\n",
    "    ### the email is from a POI #####\n",
    "    #################################\n",
    "    if from_emails and from_emails[0] in poi_email_list:\n",
    "        from_poi = True    \n",
    "    #################################\n",
    "    return to_poi, from_poi, cc_poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "\n",
    "with zipfile.ZipFile('emails.zip', \"r\") as z:\n",
    "    z.extractall()\n",
    "\n",
    "for email_message in os.listdir(\"emails\"):\n",
    "    if email_message == \".DS_Store\":\n",
    "        continue\n",
    "    message = open(os.getcwd()+\"/emails/\"+email_message, \"r\")\n",
    "    to_addresses, from_addresses, cc_addresses = getToFromStrings(message) \n",
    "    \n",
    "    to_poi, from_poi, cc_poi = poiFlagEmail(message)\n",
    "    \n",
    "    for recipient in to_addresses:\n",
    "        # initialize counter\n",
    "        if recipient not in data_dict:\n",
    "            data_dict[recipient] = {\"from_poi_to_this_person\":0}\n",
    "        # add to count\n",
    "        if from_poi:\n",
    "                data_dict[recipient][\"from_poi_to_this_person\"] += 1\n",
    "    message.close()\n",
    "\n",
    "for item in data_dict:\n",
    "    print item, data_dict[item]\n",
    "    \n",
    "#######################################################    \n",
    "def submitData():\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from get_data import getData\n",
    "\n",
    "def computeFraction( poi_messages, all_messages ):\n",
    "    \"\"\" given a number messages to/from POI (numerator) \n",
    "        and number of all messages to/from a person (denominator),\n",
    "        return the fraction of messages to/from that person\n",
    "        that are from/to a POI\n",
    "   \"\"\"\n",
    "    ### you fill in this code, so that it returns either\n",
    "    ###     the fraction of all messages to this person that come from POIs\n",
    "    ###     or\n",
    "    ###     the fraction of all messages from this person that are sent to POIs\n",
    "    ### the same code can be used to compute either quantity\n",
    "\n",
    "    ### beware of \"NaN\" when there is no known email address (and so\n",
    "    ### no filled email features), and integer division!\n",
    "    ### in case of poi_messages or all_messages having \"NaN\" value, return 0.\n",
    "    fraction = 0.\n",
    "    if all_messages == 'NaN':\n",
    "        return fraction\n",
    "    if poi_messages == 'NaN':\n",
    "        poi_messages = 0\n",
    "    fraction = float(poi_messages)/float(all_messages)\n",
    "    return fraction\n",
    "\n",
    "data_dict = getData() \n",
    "\n",
    "submit_dict = {}\n",
    "for name in data_dict:\n",
    "\n",
    "    data_point = data_dict[name]\n",
    "\n",
    "    print\n",
    "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
    "    to_messages = data_point[\"to_messages\"]\n",
    "    fraction_from_poi = computeFraction( from_poi_to_this_person, to_messages )\n",
    "    print fraction_from_poi\n",
    "    data_point[\"fraction_from_poi\"] = fraction_from_poi\n",
    "\n",
    "\n",
    "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
    "    from_messages = data_point[\"from_messages\"]\n",
    "    fraction_to_poi = computeFraction( from_this_person_to_poi, from_messages )\n",
    "    print fraction_to_poi\n",
    "    submit_dict[name]={\"from_poi_to_this_person\":fraction_from_poi,\n",
    "                       \"from_this_person_to_poi\":fraction_to_poi}\n",
    "    data_point[\"fraction_to_poi\"] = fraction_to_poi\n",
    "#####################\n",
    "def submitDict():\n",
    "    return submit_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beware of feature bugs!\n",
    "\n",
    "Uma **feição** com 100% de **accuracy** pode significar duas coisas:\n",
    "\n",
    "- sua métrica dos resultados e sua ferramenta de atribuição usam exatamente os mesmos atributos (ou seja, você não está lendo a coisa correta)\n",
    "\n",
    "- sua ferramenta de atribuição não precisaria existir (se você tem 100% de certeza de que determinada caraterística deve ser atribuída a determinado grupo, basta rodar uma consulta de adição numa tabela!)\n",
    "\n",
    "*o caso mostrado é de uma ferramenta para descobrir outros POIs. Ela rodava sobre e-mails de um determinado POI (tínhamos certeza deste!) e verificava se havia muitos e-mails deste POI compartilhados com outras pessoas. Quando havia, isso ia sendo adicionado. O problema: se a outra pessoa **já havia** sido atribuída como sendo um POI, não fazia sentido continuar a adicionar! Assim a ferramenta foi modificada para dar o sinal apenas quando um **novo POI** havia sido descoberto!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Por que excluir uma feição\n",
    "\n",
    "- se ela causa **ruído** (noisy)\n",
    "\n",
    "- se ela já está correlacionada ou implicada em outra feição\n",
    "\n",
    "- se ela causa **overfitting**\n",
    "\n",
    "- feições demais causam **lentidão**/excesso de esforço computacional\n",
    "\n",
    "#### Feições $\\neq$ Informação\n",
    "\n",
    "- o que eu **realmente** quero extrair do meu dado é informação!\n",
    "\n",
    "- feições são a nossa tentativa de **acessar** a informação\n",
    "\n",
    "- a **qualidade** das feições é a **qualidade** da informação (supondo que eu tenho muita **quantidade** de informação)\n",
    "\n",
    "- você deseja um modelo **enxuto**, ou seja, um espremedor de laranja que seja relativamente simples, mas que extraia grande quantidade de suco! Modelos grandes demais e complexos demais costumam quebrar!\n",
    "\n",
    "#### Univariate feature selection\n",
    "\n",
    "- ferramentas do sklearn: \n",
    "\n",
    " - **SelectPercentile** - o percentual das feições que são as mais poderosas para prever os dados\n",
    " \n",
    " - **SelectKBest** - as feições K que são as mais poderosas\n",
    " \n",
    "*a clear candidate for feature reduction is text learning, since the data has such high dimension. We actually did feature selection in the Sara/Chris email classification problem during the first few mini-projects; you can see it in the code in tools/email_preprocess.py*\n",
    "\n",
    "*o que quer dizer o comentário é: há muitos termos inúteis na linguagem natural e que podem ser eliminados/comprimidos*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Métodos para selecionar automaticamente feições:\n",
    "\n",
    "- There are several go-to methods of automatically selecting your features in sklearn. Many of them fall under the umbrella of **univariate feature selection**, which treats each feature independently and asks how much power it gives you in classifying or regressing\n",
    "\n",
    "Métodos que atribuem um único número:\n",
    "\n",
    "- There are two big univariate feature selection tools in sklearn: SelectPercentile and SelectKBest. The difference is pretty apparent by the names:\n",
    "\n",
    " - SelectPercentile selects the **X% of features that are most powerful** (where X is a parameter)\n",
    " \n",
    " - SelectKBest selects the **K features that are most powerful** (where K is a parameter).\n",
    "\n",
    "A clear candidate for feature reduction is text learning, since the data has such high dimension. We actually did feature selection in the Sara/Chris email classification problem during the first few mini-projects; you can see it in the code in tools/email_preprocess.py .\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passos para lidar com palavas:\n",
    "\n",
    "- eu carrego meus dados\n",
    "\n",
    "- eu alimento com eles o meu **.TfidVectorizer()** \n",
    "\n",
    " - observe os argumentos que eu passo para este método:\n",
    " \n",
    "  - **max_df=0.5** ignora na minha entrada palavras que aparecem em 50% dos documentos (**max document frequency** - filtra frequentes demais!)\n",
    "\n",
    "- eu crio um **seletor** em **.SelectPercentile()** (eu jogo fora um bocado de feições, a hora que eu escolho tipo 10% das **melhores** feições que me dizem quem escreveu aquele e-mail)\n",
    "\n",
    "- \n",
    "\n",
    "*isso é parte de **Supervised Learning**!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Bias-Variance Dilemma & no of features\n",
    "\n",
    "High **Bias** - poucas feições usadas, eu me coloco em uma situação de:\n",
    "\n",
    "- pouca atenção aos dados estarem simplificados demais (**Oversimplified**)\n",
    "\n",
    "- alta margem de **erro** no conjunto de treinamento (training set)\n",
    "\n",
    "- baixo $r^{2}$\n",
    "\n",
    "- grande SSE (Sum of the Squares Residual Errors)\n",
    "\n",
    "Alta **Variância** muitas feições, cuidadosamente otimizadas nos meus dados de treinamento, tento reduzir ao máximo meu SSE, eu crio uma situação de:\n",
    "\n",
    "- muita atenção aos **dados** (não generaliza muito bem - novas situações não são contempladas de forma alguma! Ele está simplesmente **memorizando dados**)\n",
    "\n",
    "- **overfits** (não está **generalizando** muito bem)\n",
    "\n",
    "- muito mais erros no conjunto de dados de **teste** do que no de treinamento\n",
    "\n",
    "*tente fazer o **fit** de um algoritmo com **poucas** feições, alto $r^{2}$ e baixo SSE!*\n",
    "\n",
    "*vá adicionando feições aos poucos (como numa poção de bruxaria) e não otimize extremamente!*\n",
    "\n",
    "*um exemplo de **overfitting** - pense um conjunto de pontos, que ao invés de eu tentar explicar a coisa com por exemplo, um arco de círculo ou mesmo uma reta, eu force uma **spline** (polinômio com uma ordem de grandeza n, sendo n igual ao número de pontos - 1). Eu na verdade estou criando um modelo ultracomplexo que na verdade tenta explicar **tudo** e de fato não explica **nada**. Eu forço meu modelo a tentar ajustar **erros** como sendo componentes **essenciais** dos meus dados! Resultado: novos pontos, mesmo condizentes com a realidade, são considerados como **fora do modelo** e não se encaixam mais*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "**Lasso Regression**\n",
    "\n",
    "$minimize SSE + \\lambda|\\beta|$\n",
    "\n",
    "- $\\lambda$ é o meu **parâmetro**\n",
    "\n",
    "- $\\beta$ é o meu **coeficiente de regressão**\n",
    "\n",
    "- $\\lambda|\\beta|$ é a penalização para feições extras\n",
    "\n",
    "*feições que não ajudam muito a explicar o modelo são setadas com um **coeficiente de feição** muito baixo, a fim de serem eliminadas do meu modelo*\n",
    "\n",
    "$Y = m1x1 + m2x2 + m3x3 + m4x4 + b$\n",
    "\n",
    "- $x1-x4 \\rightarrow feições$\n",
    "\n",
    "- $m1-m4 \\rightarrow coeficientesdeRegressão$\n",
    "\n",
    "Passos:\n",
    "\n",
    "0. começo com um modelo hiper-simplificado\n",
    "\n",
    "1. adiciono uma nova feição\n",
    "\n",
    "2. testo a penalidade\n",
    "\n",
    "3. se paguei caro demais pela feição, a descarto ($m_{n} = 0$)\n",
    "\n",
    "4. volto a 1\n",
    "\n",
    "\n",
    "\n",
    "*no fundo, nós queremos maximizar a **qualidade do modelo** que estamos criando. Normalmente 5-6 feições já são mais do que o suficiente para explicar o que está acontecendo!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "    import SKlearn.linearmodel.Lasso\n",
    "\n",
    "    features, labels = GetMyData()\n",
    "    \n",
    "    regression = Lasso()\n",
    "    \n",
    "    regression.fit(features, labels) #isso é Supervised Learning e eu devo incluir os rótulos!\n",
    "    \n",
    "    regression.predict([2,  4]) #prediz um rótulo para o ponto [2, 4]\n",
    "    \n",
    "    print(regression.coef_)\n",
    "    \n",
    "Tenho como resposta:\n",
    "\n",
    "    [0.7, 0.0] #o meu primeiro coeficiente importa na regressão, o segundo não\n",
    "\n",
    "*the documentation requests an array of shape = (n_samples, n_features). It happens in this case that sklearn is lenient and will still work if you give it a one dimensional array of features and you want a single prediction, but the answer regression.predict([[2,4]]) is a little more correct*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Mini project - Feature Selection\n",
    "\n",
    "Uma feição quando fica poderosa demais, age como se fosse uma **assinatura**, o que é ruim. Neste processo de identicação dos e-mails de Katie e Cris ocorreu um caso assim. Você irá identificar e resolver o problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - Principal Component Analysis\n",
    "\n",
    "---\n",
    "\n",
    "#### Data Dimensionality (Feature Setting Compression)\n",
    "\n",
    "- dados  $X-Y$ $\\rightarrow$ duas dimensões para expressar minha informação\n",
    "\n",
    "- às vezes o **ruído** nos dá a ilusão de estarmos trabalhando com mais dimensões\n",
    "\n",
    "- pontos formando uma **seinoidal** apresentam uma estrutura 1D, mas:\n",
    "\n",
    " - PCA pode lidar apenas com **retas**\n",
    " \n",
    " - as operações podem ser basicamente **translação** e **rotação**\n",
    "\n",
    "- PCA cria um novo conjunto de referências cartesianas mais adequado para o seu sistema!\n",
    "\n",
    "Notação:\n",
    "\n",
    "- o **centro de massa** $C$ dos pontos passa a receber $(0,0)$\n",
    " \n",
    "- os vetores dos **eixos** são dados por dois parâmetros cada um:\n",
    "\n",
    " - $X'$ por $\\Delta X$ e $\\Delta Y$\n",
    " \n",
    " - $Y'$ por $\\Delta X$ e $\\Delta Y$\n",
    " \n",
    "*essa notação não envolve $Seno$ e $Cosseno$. Basta somar os números de projeção em $X$ e em $Y$, a partir do centro já definido e estas são as **componentes vetoriais** de cada um dos novos eixos!*\n",
    "\n",
    "\n",
    "#### Spread value\n",
    "\n",
    "- o PCA lhe devolve dois **spread values**:\n",
    "\n",
    " - um **maior** para o **eixo principal** $X$\n",
    " \n",
    " - um **menor** para o **eixo secundário** $Y$\n",
    " \n",
    "*lembrando que com a rotação perde o sentido quem era o eixo $X$ e o $Y$ originalmente. O novo $X'$ receberá os dados onde a **amplitude** for maior!*\n",
    "\n",
    "*uma das graças do PCA é que os dados não precisam estar muito alinhados, ou em perfeitas condições de simetria, para receberem uma resposta! Como PCA trabalha com dois **vetores**, formular um eixo $X$ na **vertical** não será nenhum problema - diferente de tentar ajustar uma curva vertical*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurable vc Latent features\n",
    "\n",
    "- dadas as feições de uma casa, qual será o seu preço? \n",
    "\n",
    " - isso é uma questão boa para ser respondida com **regressão linear**!\n",
    "\n",
    " - **decision trees** são boas **classificadoras** e suas respostas não serão contínuas, mas **discretas**\n",
    " \n",
    "Voltando à questão...\n",
    "\n",
    "- é **mensurável**\n",
    "\n",
    " - área do imóvel\n",
    " \n",
    " - número de quartos\n",
    " \n",
    " - distância da escola mais próxima\n",
    " \n",
    " - valor do condomínio/do IPTU\n",
    " \n",
    "- é **latente**\n",
    "\n",
    " - tamanho (minha família vai caber nesse imóvel?)\n",
    " \n",
    " - vizinhança (vou conseguir morar tranquilo?)\n",
    " \n",
    "*normalmente eu quero responder perguntas **latentes**, mas para sondar a respeito delas, eu preciso de sinalizações vindas de fatores **mensuráveis**. Então minha grande pergunta é: quais os fatores **mensuráveis** que poderiam me dar dicas para responder minha questão **latente**? Não estou colocando meu cliente em risco ao lhe oferecer uma mansão no bairro do riscafaca?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Preservando a informação\n",
    "\n",
    "A ferramenta de **select k-best** neste caso será mais útil do que a **select percentile**, pois quero excluir feicões inúteis, quero podar a árvore! No entanto, se eu tiver ainda muitas feições e quiser um panorama geral, então eu posso primeiro usar a **select percentile** para aquecer e depois por a mão na massa com **seleck k-best**\n",
    "\n",
    "Caso usual:\n",
    "\n",
    "- eu tenho diversas feições **mensuráveis** que eu posso usar para responder a uma questão **latente**. Mas eu sinto que eu poderia dar uma **podada** nas minhas feições, de modo a deixar o modelo mais magro. \n",
    "\n",
    "- eu posso tentar criar uma **feição composta** que sonde mais diretamente o meu **fenômeno de base** (underlying phenomenon)\n",
    "\n",
    "- essa feição composta chama-se **principle component** (PCA) $\\rightarrow$ isso é ligado a **dimensionallity regression**, uma das matérias tratadas em **unsupervised learning**\n",
    "\n",
    "Supondo que eu queira usar PCA para criar um **principle component** = área do imóvel + número de quartos:\n",
    "\n",
    "1. eu posso traçar um gráfico área do imóvel vs número de quartos. Eu verei este gráfico tendo pontos, com áreas cada vez maiores, e números de quartos indo de 1... 2... 3... 4... (e por que não 5 quartos num apartamento de 320m2?)\n",
    "\n",
    "2. bom, agora eu quero a **componente principal**. Isso **não é** uma regressão! Eu não vou usar isso para **prever** nada! Eu quero tipo, determinar a direção do meu vetor principal que acompanha a evolução desta dispersão\n",
    "\n",
    "3. e sobre esta **componente principal**, cada ponto de dado real do meu gráfico irá produzir uma **sombra**, uma projeção de si naquela componente. Com isso eu crio uma **representação unidimensional** desses meus dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como determinar a componente principal\n",
    "\n",
    " - **variância** - a **willingness/flexibility** de um algoritmo em aprender **X** $\\leftarrow$ Não quero esta definição!\n",
    " \n",
    " - **variância** - em **Estatística** é o **espalhamento** (Spread) de uma distribuição de dados (similar ao desvio padrão)\n",
    " \n",
    "1. eu jogo fora 5% dos meus dados (outliers)\n",
    "\n",
    "2. se as variáveis fazem algum sentido, eu normalmente obtenho uma nuvem meio ovóide (uma direção maior $X'$ e uma direção menor $Y'$). A direção da minha **máxima variância** irá determinar o eixo $X$\n",
    "\n",
    "*essa direção, a do eixo $X$ é a que retém a **máxima quantidade de informação** dos dados originais. Ou seja, eu **comprimo** meus dados em uma dimensão, perdendo o **mínimo possível** de informação útil!*\n",
    "\n",
    "*e o que será do eixo $Y$? Ele guardará a memória do **ruído** que aconteceu no meu processo de compressão. Quanto menor $Y$, melhor o resultado obtido!*\n",
    "\n",
    "*e seu eu tiver um $Y$ elevado? Bom, neste caso, tente plotar os dados. Às vezes existe um fator um **pouco mais complexo** que poderia explicar melhor esses dados. Por exemplo, um processo cíclico envolvido, ou uma tendência exponencial. Então a ferramenta de análise terá que ser um pouco mais complexa do que a tratada aqui*\n",
    "\n",
    "*a demonstração de que proceder dessa forma minimiza a perda de informação envolve um pouco de Álgebra Linear, mas ela existe. A direção máxima da variância é a que **minimiza** a soma das projeções dos pontos individuais*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### PCA as a general algorithm for Feature Transformation\n",
    "\n",
    "O problema que ainda reside é que fizemos a separação **manual** das feições que poderiam interessar para serem candidatas ao agrupamento. Isso em Big Data não é factível!\n",
    "\n",
    "Uma estratégia:\n",
    "\n",
    "- colocar todas essas feições juntas em um PCA\n",
    "\n",
    "- combinar essas feições par a par\n",
    "\n",
    "- puxar uma gradação de suas importâncias relativas\n",
    "\n",
    "*para descobrir respostas para suas **latent features**, esta é uma ferramenta muito poderosa!*\n",
    "\n",
    "*com **quatro** feições, eu precisarei de **quatro** PCAs para obter esta resposta. Consulte a documentação do sklearn sobre PCAs [aqui](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "Em suma:\n",
    "\n",
    "- é uma maneira sistematizada de transformar **feições de entrada** em **componentes principais**\n",
    "\n",
    "- e então use os **componentes principais** como novas feições\n",
    "\n",
    "- os **componentes principais** são direções nos dados que maximizam a variância (minimizam a perda de informação) quando você projeta/comprime neles\n",
    "\n",
    "- quanto mais **variância** de dados você tiver no seu componente principal, mais longo ele será\n",
    "\n",
    "- as duas componentes principais serão perpendicuares entre si:\n",
    "\n",
    " - a primeira modela a maior variância/informação\n",
    " \n",
    " - a segunda modela o erro/ruído\n",
    " \n",
    "- existe um **número máximo** de componentes principais que você pode encontrar e ele será igual ao número de feições do seu Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Quando usar PCA\n",
    "\n",
    "- quando eu acreditar que existem feições latentes que estão guiando todos os meus dados (big shots @ Enron!)\n",
    "\n",
    "- redução de dimensionalidade\n",
    "\n",
    " - visualizar dados de dimensionalidade mais alta (**k-means** se torna bem mais fácil de visualizar!)\n",
    "\n",
    " - redução de ruído (se de fato os compnentes principais estiverem capturando relações, então o segundo componente principal realmente irá servir para limpar ruídos!) \n",
    "\n",
    " - fazer seus algoritmos (regressões, classificações) rodarem **melhor** com menos entradas (eigenfaces)\n",
    "\n",
    "*observe que ao visualizar os gráficos dos **componentes principais**, eles podem não nos parecer **ortogonais**. Isso se dá apenas pelo fato de geralmente usarmos escalas diferentes para $X$ e $Y$, na hora de plotar os gráficos*\n",
    "\n",
    "---\n",
    "\n",
    "#### PCA para reconhecimento facial\n",
    "\n",
    "- fotos de faces humanas possuem **alta dimensionalidade** (muitos pixels)\n",
    "\n",
    "- existem **padrões gerais** que poderiam ser capturados em uma **menor dimensionalidade** (dois olhos no topo, uma boca, dentes na boca, etc..) $\\rightarrow$ alimentar uma SVM\n",
    "\n",
    "- **eigenfaces** são basicamente o componente principal do PCA: 800 feições foram compactadas em 150\n",
    "\n",
    "- **SVC** - Support Vector Classifier é um componente da **SVM** - Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Validation\n",
    "\n",
    "---\n",
    "\n",
    "#### Cross-validation for run & profit [aqui](https://scikit-learn.org/0.17/modules/cross_validation.html)\n",
    "\n",
    "Para validar algoritmos:\n",
    "\n",
    "- separo meus dados em de **trenamento** e de **teste**\n",
    "\n",
    " - nos dá uma estimativa de performance e acuidade em um dataset diferente\n",
    " \n",
    " - serve para checar se não houve **overfitting**\n",
    "\n",
    "*eu não quero entregar a meu cliente apenas a resposta, mas também **como** eu cheguei até ela e por qual razão eu creio que ela é válida!*\n",
    "\n",
    "*observe que eu não estou trabalhando com modelos **mecânicos**, mas com algoritmos que tentam **inferir algo**. Não é muito fácil fazer isso e nem sempre minha operaçao é bem sucedida. Mas em alguns casos este é o **único caminho** conhecido para se identificar algumas anomalias*\n",
    "\n",
    "        x_train, x_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = iris.target\n",
    "###############################################################\n",
    "### import the relevant code and make your train/test split\n",
    "### name the output datasets features_train, features_test,\n",
    "### labels_train, and labels_test\n",
    "# PLEASE NOTE: The import here changes depending on your version of sklearn\n",
    "from sklearn import cross_validation # for version 0.17\n",
    "# For version 0.18\n",
    "# from sklearn.model_selection import train_test_split\n",
    "### set the random_state to 0 and the test_size to 0.4 so\n",
    "### we can exactly check your resultiris.data, \n",
    "                                                                    \n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(iris.data, \n",
    "                                                                    iris.target, test_size=0.4, random_state=0)\n",
    "###############################################################\n",
    "# DONT CHANGE ANYTHING HERE\n",
    "clf = SVC(kernel=\"linear\", C=1.)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "print clf.score(features_test, labels_test)\n",
    "##############################################################\n",
    "def submitAcc():\n",
    "    return clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submitAcc():\n",
    "    return clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training, transforms, predicting\n",
    "\n",
    "Quando usar **Training** vs **Testing** data\n",
    "\n",
    "train/test/split $\\rightarrow$ PCA $\\rightarrow$ SVM\n",
    "\n",
    "- train/test/split (training-features, test-features)\n",
    "\n",
    "- PCA (fit, transform)\n",
    "\n",
    " - primeiro \n",
    "\n",
    "- SVM (fit, predict)\n",
    "\n",
    "Primeiro eu faço todas minas operações com os dados de **treinamento**:\n",
    "\n",
    "        pca.fit(training-features) #aqui eu ajusto o modelo do PCA\n",
    "        \n",
    "        pca.transform(training-features) #aqui eu transformo os dados para o modelo ajustado\n",
    "        \n",
    "        svc.train(training-features) #aqui eu treino minha SVC\n",
    "        \n",
    "E agora com os dados de **teste**:\n",
    "\n",
    "        pca.transform(test-features)\n",
    "        \n",
    "*observe que eu não dou um novo fit! eu quero usar o exato mesmo fit do treinamento no teste!*\n",
    "\n",
    "        svc.predict(test-features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### k-Fold Cross Validation\n",
    "\n",
    "Problemas ao quebrar o dataset em treinamento e teste:\n",
    "\n",
    "- muito treinamento $\\rightarrow$ ótimo treinamento do modelo (mas validaçao precária)\n",
    "\n",
    "- muito teste $\\rightarrow$ ótimos testes (mas treinamento prejudicado)\n",
    "\n",
    "Uma saída:\n",
    "\n",
    "1. compartimentamos nossos dados (ex: 200 amostras) em $k=10$ compartimentos, contendo 20 amostras cada\n",
    "\n",
    "* $k$ é o número de **Bins**\n",
    "\n",
    "2. usamos 1 compartimento para treinamento e 1 compartimento para teste\n",
    "\n",
    "3. rodamos o experimento completo $k$ vezes e guardamos o resultado\n",
    "\n",
    "Depois disso, tiramos a média dos resultados\n",
    "\n",
    "Isso nos dá:\n",
    "\n",
    "- máxima **acuidade** (accuracy), pois eu usei **todos** os dados tanto para treinamento, como para teste!\n",
    "\n",
    "- menor tempo de processamento, pois \n",
    "\n",
    "Ao custo de:\n",
    "\n",
    "- maior tempo de treinamento (o que é bom!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Uma recomendação prática\n",
    "\n",
    "Eu cortei meu dataset em dois e obtive resultados horríveis de **accuracy**! Algo deu errado, mas o quê?\n",
    "\n",
    "- às vezes meus dados são **biased** (isso é bastante usual), então todos os e-mails de Cris vieram primeiro e os de Sara vieram depois. Isso distorceu todo o meu aprendizado!\n",
    "\n",
    "- então **antes** de proceder os cortes, lembre-se de **sempre** embaralhar seus dados! O fatiador apenas fatia, ele não fará isso para você!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Cross validation for parameter tuning\n",
    "\n",
    "GridSearchCV is a way of systematically working through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. The beauty is that it can work through many combinations in only a couple extra lines of code\n",
    "\n",
    "        parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "        \n",
    "A dictionary of the parameters, and the possible values they may take. In this case, they're playing around with the kernel (possible choices are 'linear' and 'rbf'), and C (possible choices are 1 and 10). Each is used to train an SVM, and the performance is then assessed using cross-validation.\n",
    "\n",
    "        ('rbf', 1) $\\leftarrow$ ('linear', 1)\t\n",
    "\n",
    "        ('rbf', 10) $\\leftarrow$ ('linear', 10)\n",
    "        \n",
    "Each is used to train an SVM, and the performance is then assessed using cross-validation\n",
    "\n",
    "        svr = svm.SVC()\n",
    "    \n",
    "This looks kind of like creating a classifier, just like we've been doing since the first lesson. But note that the \"clf\" isn't made until the next line--this is just saying what kind of algorithm to use. Another way to think about this is that the \"classifier\" isn't just the algorithm in this case, it's algorithm plus parameter values. Note that there's no monkeying around with the kernel or C; all that is handled in the next line\n",
    "\n",
    "        clf = grid_search.GridSearchCV(svr, parameters)\n",
    "        \n",
    "This is where the first bit of magic happens; the classifier is being created. We pass the algorithm (svr) and the dictionary of parameters to try (parameters) and it generates a grid of parameter combinations to try\n",
    "\n",
    "        clf.fit(iris.data, iris.target)\n",
    "\n",
    "And the second bit of magic. The fit function now tries all the parameter combinations, and returns a fitted classifier that's automatically tuned to the optimal parameter combination. You can now access the parameter values via\n",
    "\n",
    "        clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Os códigos para reconhecimento facial [aqui](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html)\n",
    "\n",
    "Quais parâmetros do SVM foram tunados por CV?\n",
    "\n",
    "        GridSearchCV\n",
    "        \n",
    "- C e gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Mini project - Validation\n",
    "\n",
    "Identificar o POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Starter code for the validation mini-project.\n",
    "    The first step toward building your POI identifier!\n",
    "    Start by loading/formatting the data\n",
    "    After that, it's not our code anymore--it's yours!\n",
    "\"\"\"\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "data_dict = pickle.load(open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### first element is our labels, any added elements are predictor\n",
    "### features. Keep this the same for the mini-project, but you'll\n",
    "### have a different feature list when you do the final project.\n",
    "features_list = [\"poi\", \"salary\"]\n",
    "\n",
    "data = featureFormat(data_dict, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### it's all yours from here forward!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "---\n",
    "\n",
    "accuracy = no of items in a class labelled correctly / all items in that class\n",
    "\n",
    "É deficiente nas seguntes áreas:\n",
    "\n",
    "- não é bom para classes enviesadas (**skewed**). Por exemplo, no caso Enron, as pessoas que realmente nos interessam são uma **minoria** na empresa! Como estou **pescando** elementos específicos, o número de **all items in that class** pode se tornar realmente pequeno!\n",
    "\n",
    "- pode ser que eu esteja propenso a realmente cometer erros do **Tipo I** - eu aceito tomar por culpado um funcionário inocente da Enron só porque, como o CEO, o filho dele também curte Pateta... (afinal, estou **exporando dados** e não julgando!)\n",
    "\n",
    "- pode ser que eu esteja propens o cometer erros do **Tipo II** - eu aceito inocentar pontos \"culpados\" no meu mapa, pois a suspensão do meu carro pode suportar algum maltrato, para não comprometer a velocidade de cruzeiro do meu veículo...\n",
    "\n",
    "*conceitos como **precision** e **recall** podem ser bem mais úteis para o meu caso!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "*este assunto já está dissecado no módulo **Advanced Statistics**!*\n",
    "\n",
    "Em uma **Decision Tree**:\n",
    "\n",
    "- o mapa de uma Decision Tree parece uma folha de papel, recortada\n",
    "\n",
    "- um quadrado divide **Sim** e **Não** e assim em diante\n",
    "\n",
    "- cada quadrado apresenta seu próprio **domínio** (sim/não)\n",
    "\n",
    "- minha Confusion Matrix assim pode ser remontada com algum esforço computacional, sem maiores problemas!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "todos os projetos se encontram [aqui](https://github.com/udacity/ud120-projects)\n",
    "\n",
    "o projeto final se encontra [aqui](https://github.com/udacity/ud120-projects/tree/master/final_project)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
